{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38e62a0e",
   "metadata": {},
   "source": [
    "Poppy humanoid robot is a humanoid robot that can be used with its hardware platform or with the CoppeliaSim simulator. It can be programmed with python, using the pypot library.\n",
    "\n",
    "Download and install the simulator CoppeliaSim from https://www.coppeliarobotics.com/downloads "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930d7746",
   "metadata": {},
   "source": [
    "# Setup the software\n",
    "\n",
    "Poppy uses pypot for control. It is a python library : http://poppy-project.github.io/pypot/. \n",
    "On the top of pypot are libraries for Poppy creatures : https://github.com/poppy-project.\n",
    "The quick install consists in:\n",
    "- install pypot :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38894b80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install pypot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2189f627",
   "metadata": {},
   "source": [
    "- install your poppy creature with its geometry :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ec7925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install poppy_torso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08251879",
   "metadata": {},
   "source": [
    "- install the library **ikpy** that proposes the inverse kinematics of the robot. The source code is https://github.com/Phylliade/ikpy. See tutorial on https://notebook.community/Phylliade/ikpy/tutorials/Moving%20the%20Poppy%20Torso%20using%20Inverse%20Kinematics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df445384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install 'ikpy[plot]'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f4c7cf",
   "metadata": {},
   "source": [
    "We use another algorithm for human pose estimation : Blazepose :\n",
    "- the article describing the algorithm is in https://arxiv.org/abs/2006.10204\n",
    "- the source code is still available at https://github.com/google/mediapipe\n",
    "\n",
    "To install, use the command : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d213243c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install mediapipe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d3ef68",
   "metadata": {},
   "source": [
    "# Start your code\n",
    "Now you are done with the installation phase. You can start your project by importing the different libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfe5f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils.skeleton import *\n",
    "from utils.quaternion import *\n",
    "from utils.blazepose import blazepose_skeletons\n",
    "import os\n",
    "from pypot.creatures import PoppyTorso\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "from pypot.creatures.ik import IKChain\n",
    "from pypot.primitive.move import Move\n",
    "from pypot.primitive.move import MovePlayer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94a4033",
   "metadata": {},
   "source": [
    "## Capture a video with a camera\n",
    "\n",
    "For this section only, you need to install first opencv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f98919e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "\n",
    "#time.sleep(5)\n",
    "\n",
    "# Open a connection to the cameras\n",
    "# cap1 = cv2.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0985282",
   "metadata": {},
   "source": [
    "Capture the frames of the camera for 10 seconds. Save as file cam1.avi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3ab6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# frame_width_1 = int(cap1.get(3))\n",
    "# frame_height_1 = int(cap1.get(4))\n",
    "# # if you want to change the resolution of the camera\n",
    "# #cap1.set(3,frame_width_1)\n",
    "# #cap1.set(4,frame_height_1)\n",
    "\n",
    "   \n",
    "# size_1 = (frame_width_1, frame_height_1)\n",
    "\n",
    "# video_nom_1 = \"cam1.avi\"\n",
    "\n",
    "# # Compression\n",
    "# lossless = cv2.VideoWriter_fourcc(* 'FFV1')\n",
    "\n",
    "# # Save the frame as an video file\n",
    "# video_1 = cv2.VideoWriter(video_nom_1, lossless, 30, size_1)\n",
    "\n",
    "# start = time.time()\n",
    "\n",
    "# while  time.time() < start + 10:\n",
    "\n",
    "#     # Capture a frame from the cameras\n",
    "#     ret1, frame1 = cap1.read()\n",
    "\n",
    "#     # Check if the user pressed the enter key\n",
    "#     cv2.waitKey(1)\n",
    "\n",
    "#     if ret1:\n",
    "#         #cv2.imshow(\"Webcam 1\", frame1) #to display the camera images\n",
    "#         video_1.write(frame1)\n",
    "    \n",
    "\n",
    "# video_1.release()\n",
    "            \n",
    "# # Release the camera and close the window\n",
    "# cap1.release()\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3c18aa",
   "metadata": {},
   "source": [
    "# Instantiate the robot\n",
    "\n",
    "Now open the simulator CoppeliaSim. Click yes when prompted if you accept all incoming communications.\n",
    "For MacOs, type in the Sandbox script terminal\n",
    "\n",
    "simExtRemoteApiStart(19997)\n",
    "\n",
    "If instead it does now appear and you get an error message like\n",
    "\n",
    "\n",
    "> pypot.vrep.io.VrepConnectionError: Could not connect to V-REP server on 127.0.0.1:19997. This could also means that you still have a previously opened connection running! (try pypot.vrep.close_all_connections())\n",
    ">\n",
    "> During handling of the above exception, another exception occurred:\n",
    ">\n",
    "> pypot.vrep.io.VrepIOErrors: No value\n",
    "\n",
    "type in the Sandbox script terminal\n",
    "\n",
    "simExtRemoteApiStart(19997)\n",
    "    \n",
    "\n",
    "Instantiate the poppy robot in the simulator with the code below, this is necessary in order to add the kinematic chains attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7db6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypot import vrep\n",
    "vrep.close_all_connections()\n",
    "poppy = PoppyTorso(simulator='vrep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777a8627",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in poppy.motors:\n",
    "    print(m.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f0ca28",
   "metadata": {},
   "source": [
    "Poppy should now appear on the CoppeliaSim simulation screen, and a popup appeared in CoppeliaSim to inform you that the simulation use custom parameters. This popup block the communication to the Python API of CoppeliaSim. You have to check the check-box “Do not show this message again” and press “Ok”. Do this 3 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea7065e",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = None\n",
    "targets = None\n",
    "smoothed_targets = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f94a45",
   "metadata": {},
   "source": [
    "For each kinematic chain we have built an urdf file. We create an IKChain object for each kinematic chain, making it possible to compute the inverse kinematics, i.e. motor angles from desired end-effector position.\n",
    "\n",
    "The constructor takes as input the poppy robot instance, the motors that are part of the kinematic chain, the motors that remain passive during the inverse kinematics, the distance of the tip of the last bone of the chain, and finally, the list of motors for which the urdf file give reversed orientation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0605a3ec",
   "metadata": {},
   "source": [
    "We can plot some of these kinematic chains in a figure. If the position of the robot in the simulator is changed, these changes should be reflected when reexecuting this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96bd6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline\n",
    "zero = [0] * 7\n",
    "\n",
    "ax = matplotlib.pyplot.figure().add_subplot(111, projection='3d')\n",
    "ax.scatter([0], [0],[0])\n",
    "\n",
    "poppy.l_arm_chain.plot(poppy.l_arm_chain.convert_to_ik_angles(poppy.l_arm_chain.joints_position), ax, target = (0.2, -0.2, 0.2))\n",
    "poppy.r_arm_chain.plot(poppy.r_arm_chain.convert_to_ik_angles(poppy.r_arm_chain.joints_position), ax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c77950c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9ae3c7c8",
   "metadata": {},
   "source": [
    "The robot is now ready to use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2c58fd",
   "metadata": {},
   "source": [
    "# Using the robot Poppy\n",
    "\n",
    "\n",
    "\n",
    "You can directly access the chains:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de32f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"chaine  \" , poppy.l_arm_chain)\n",
    "print(\"nom     \" , poppy.l_arm_chain.name)\n",
    "print(\"links   \" , poppy.l_arm_chain.links)\n",
    "print(\"1er link\" , poppy.l_arm_chain.links[1].name)\n",
    "#print(poppy.torso_chain)\n",
    "#print(poppy.l_elbow_chain   )\n",
    "#print(poppy.l_elbow_chain   )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b3f9d0",
   "metadata": {},
   "source": [
    "You can access their respective motors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f641450d",
   "metadata": {},
   "outputs": [],
   "source": [
    "[m.name for m in poppy.l_arm_chain.motors]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770ec8f0",
   "metadata": {},
   "source": [
    "You can access the state of the robot.\n",
    "\n",
    "joints_position returns the joint angles for all the motors of the chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb758c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(poppy.l_arm_chain.joints_position)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d25452",
   "metadata": {},
   "source": [
    "position returns the cartesian position of the end effector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1fb74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(poppy.l_arm_chain.position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2455079f",
   "metadata": {},
   "outputs": [],
   "source": [
    "poppy.l_shoulder_y.goto_position(-30,3)\n",
    "poppy.l_shoulder_x.goto_position(30,3)\n",
    "poppy.abs_z.goto_position(-20,3)\n",
    "print(poppy.l_arm_chain.position)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6d0fe6",
   "metadata": {},
   "source": [
    "Reset the robot to an initial position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1ed3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def poppy_reset():\n",
    "    joint_pos = { 'l_elbow_y':0.0,\n",
    "                 'head_y': 0.0,\n",
    "                 'r_arm_z': 0.0, \n",
    "                 'head_z': 0.0,\n",
    "                 'r_shoulder_x': 0.0, \n",
    "                 'r_shoulder_y': 0.0,\n",
    "                 'r_elbow_y': 0.0, \n",
    "                 'l_arm_z': 0.0,\n",
    "                 'abs_z': 0.0,\n",
    "                 'bust_y': 0.0, \n",
    "                 'bust_x':0.0,\n",
    "                 'l_shoulder_x': 0.0,\n",
    "                 'l_shoulder_y': 0.0\n",
    "                }\n",
    "    for m in poppy.motors:\n",
    "        m.goto_position(joint_pos[m.name],5)\n",
    "\n",
    "poppy_reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cebbebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# poppy.abs_z.goto_position(-180,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0adfe13",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pypot.primitive.move import Move\n",
    "fps =10\n",
    "\n",
    "move = Move(freq=fps)\n",
    " \n",
    "\n",
    "print(\"list of all motors of poppy\", [m.name for m in poppy.motors])\n",
    "move_motors = [m.name for m in poppy.motors]\n",
    "\n",
    "\n",
    "for t in np.linspace(0.02,3,int(3*fps)):\n",
    "        new_positions = {}\n",
    "        for motor in move_motors:\n",
    "            # decide for each timestep and each motor a joint angle and a velocity\n",
    "            new_positions[motor] = [20*np.sin(t), 0.0]\n",
    "\n",
    "        move.add_position(new_positions, t)\n",
    "        \n",
    "#print(\"joint positions of the move \",(move._timed_positions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f17ab4",
   "metadata": {},
   "source": [
    "Before sending the motor commands to the robot, reset the robot to an initial position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caaf51cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "poppy_reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef7fa1a",
   "metadata": {},
   "source": [
    "Send the motor commands to the robot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f195e2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp = MovePlayer(poppy, move,play_speed=1)\n",
    "mp.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c051f1",
   "metadata": {},
   "source": [
    "Record the movement in an file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bd59f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "move.save(open('new_movement.record', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d242c3d",
   "metadata": {},
   "source": [
    "# Imitation by inverse kinematics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00343adc",
   "metadata": {},
   "source": [
    "This function is a wrapper for the inverse kinematics methods of the IKChain objects.\n",
    "\n",
    "If no initial position is provided, the method will use the current position of the robot in the simulator, and will automatically control the simulated robot towards the provided target.\n",
    "\n",
    "It returns the new joints angle coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdcb6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ik(kinematic_chain, target_position, initial_position=None):\n",
    "    \n",
    "    kwargs = {}\n",
    "    kwargs['max_iter'] = 3\n",
    "    if initial_position is not None:\n",
    "        kwargs['initial_position'] = kinematic_chain.convert_to_ik_angles(initial_position)\n",
    "    else:\n",
    "        kwargs['initial_position'] = kinematic_chain.convert_to_ik_angles(kinematic_chain.joints_position)\n",
    "\n",
    "    q = kinematic_chain.inverse_kinematics(\n",
    "        target_position=target_position,\n",
    "        orientation_mode=None,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    joints = kinematic_chain.convert_from_ik_angles(q)\n",
    "\n",
    "    last = kinematic_chain.motors[-1]\n",
    "    \n",
    "    if initial_position is None:\n",
    "        for i, (m, pos) in enumerate(list(zip(kinematic_chain.motors, joints))):\n",
    "            if kinematic_chain.active_links_mask[i+1]:\n",
    "                m.goal_position = pos\n",
    "        \n",
    "    return joints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084607c4",
   "metadata": {},
   "source": [
    "## Pose estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7e078c",
   "metadata": {},
   "source": [
    "This function uses blazepose to compute the skeleton based on a video file.\n",
    "Blazepose/Mediapipe is a 3D human pose estimation algorithm that can run in realtime on a CPU computer. \n",
    "\n",
    "This function processes a video and returns a list of positions (x,y,z) for each joint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5339289b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "skeletons = blazepose_skeletons('mai1.mov')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2a3a0a",
   "metadata": {},
   "source": [
    "Let's examine the skeletons variable. It is a tensor of size :\n",
    "- the number of frames in the video\n",
    "- 17 joints in the human figure model\n",
    "- 3 for the (x,y,z) positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e84378",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(skeletons.shape)\n",
    "print(skeletons)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67026fd6",
   "metadata": {},
   "source": [
    "Normalize the skeleton, change the reference frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165744af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_frame(skeletons, frame_name, alpha, topology):\n",
    "\n",
    "    rota_skeletons_A = skeletons.clone()\n",
    "    rota_skeletons_A[:, :, 2] = -skeletons[:, :, 1]\n",
    "    rota_skeletons_A[:, :, 1] = skeletons[:, :, 2]\n",
    "    center_A = rota_skeletons_A[:, 0,:].unsqueeze(1).repeat(1, len(topology), 1)\n",
    "    rota_skeletons_A = rota_skeletons_A - center_A\n",
    "\n",
    "    batch_size, n_joints, _ = rota_skeletons_A.shape\n",
    "        \n",
    "\n",
    "    # Measure skeleton bone lengths\n",
    "    lengths = torch.Tensor(batch_size, n_joints)\n",
    "    for child, parent in enumerate(topology):\n",
    "            lengths[:, child] = torch.sqrt(\n",
    "                torch.sum(\n",
    "                    (rota_skeletons_A[:, child] - rota_skeletons_A[:, parent])**2,\n",
    "                    axis=-1\n",
    "                )\n",
    "            )\n",
    "\n",
    "    # Find the corresponding angles\n",
    "    offsets = torch.zeros(batch_size, n_joints, 3)\n",
    "    offsets[:, :, -1] = lengths\n",
    "    quaternions = find_quaternions(topology, offsets, rota_skeletons_A)\n",
    "        \n",
    "    # Rotate of alpha\n",
    "    #define the rotation by its quaternion \n",
    "    rotation = torch.Tensor([np.cos(alpha/2),  np.sin(alpha/2),0,0]).unsqueeze(0).repeat(batch_size*n_joints, 1)\n",
    "    quaternions = quaternions.reshape(batch_size*n_joints, 4)\n",
    "    quaternions = batch_quat_left_multiply(\n",
    "            batch_quat_inverse(rotation),\n",
    "            quaternions\n",
    "        )\n",
    "    quaternions = quaternions.reshape(batch_size, n_joints, 4)\n",
    "\n",
    "    # Use these quaternions in the forward kinematics with the Poppy skeleton\n",
    "    skeleton = forward_kinematics(\n",
    "            topology,\n",
    "            torch.zeros(batch_size, 3),\n",
    "            offsets,\n",
    "            quaternions\n",
    "        )[0]\n",
    "        \n",
    "    outputs= skeleton.clone()\n",
    "        \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69536f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skeleton topology\n",
    "topology = [0, 0, 1, 2, 0, 4, 5, 0, 7, 8, 9, 8, 11, 12, 8, 14, 15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ee986c",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = np.pi/4. \n",
    "\n",
    "rota_skeletons_B = change_frame(skeletons, 'general', alpha, topology)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a287448d",
   "metadata": {},
   "source": [
    "Plot the skeleton in the new reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20821d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "print(rota_skeletons_B.shape)\n",
    "t=10\n",
    "\n",
    "ax = pyplot_skeleton(topology, rota_skeletons_B[t], show=True, color='blue') #output by blazepose\n",
    "# ax=pyplot_skeleton(topology, rota_skeletons_B[t], ax=ax, show=True, color='red') #in the new reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7163237",
   "metadata": {},
   "source": [
    "Then, this function computes target XYZ positions for Poppy's kinematic chains' end effectors based on the skeleton obtained from blazepose. It proceeds as follows:\n",
    "- it estimates the source (i.e. video) bone lengths\n",
    "- it estimates the source (i.e. video) joint orientations using the \"find_quaternions\" util function\n",
    "- it reorients all the joint angles using as base orientation the pelvis -> chest axis\n",
    "- it computes the XYZ joint positions based on the found orientations and the poppy bone lengths\n",
    "\n",
    "A little trick is then applied, but could be removed. To decrease the depth of the movement estimated by blazepose, the y-axis values are divided by 10. To ensure that the target positions still correspond to achievable positions by the robot, we do another round of XYZ positions -> joint orientations -> XYZ positions.\n",
    "\n",
    "Finally, this function returns the joint XYZ positions only for kinematic chaine end effectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ac268d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def targets_from_skeleton(source_positions, topology):\n",
    "    # Works in batched\n",
    "    batch_size, n_joints, _ = source_positions.shape\n",
    "    \n",
    "    # Measure skeleton bone lengths\n",
    "    source_lengths = torch.Tensor(batch_size, n_joints)\n",
    "    for child, parent in enumerate(topology):\n",
    "        source_lengths[:, child] = torch.sqrt(\n",
    "            torch.sum(\n",
    "                (source_positions[:, child] - source_positions[:, parent])**2,\n",
    "                axis=-1\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    # Find the corresponding angles\n",
    "    source_offsets = torch.zeros(batch_size, n_joints, 3)\n",
    "    source_offsets[:, :, -1] = source_lengths\n",
    "    quaternions = find_quaternions(topology, source_offsets, source_positions)\n",
    "    \n",
    "    # Re-orient according to the pelvis->chest orientation\n",
    "    base_orientation = quaternions[:, 7:8].repeat(1, n_joints, 1).reshape(batch_size*n_joints, 4)\n",
    "    base_orientation += 1e-3 * torch.randn_like(base_orientation)\n",
    "    quaternions = quaternions.reshape(batch_size*n_joints, 4)\n",
    "    quaternions = batch_quat_left_multiply(\n",
    "        batch_quat_inverse(base_orientation),\n",
    "        quaternions\n",
    "    )\n",
    "    quaternions = quaternions.reshape(batch_size, n_joints, 4)\n",
    "    \n",
    "    # Use these quaternions in the forward kinematics with the Poppy skeleton\n",
    "    target_offsets = torch.zeros(batch_size, n_joints, 3)\n",
    "    target_offsets[:, :, -1] = poppy_lengths.unsqueeze(0).repeat(batch_size, 1)\n",
    "    target_positions = forward_kinematics(\n",
    "        topology,\n",
    "        torch.zeros(batch_size, 3),\n",
    "        target_offsets,\n",
    "        quaternions\n",
    "    )[0]\n",
    "\n",
    "    # Measure the hip orientation\n",
    "    alpha = np.arctan2(\n",
    "        target_positions[0, 1, 1] - target_positions[0, 0, 1],\n",
    "        target_positions[0, 1, 0] - target_positions[0, 0, 0]\n",
    "    )\n",
    "    \n",
    "    # Rotate by alpha around z\n",
    "    alpha = alpha\n",
    "    rotation = torch.Tensor([np.cos(alpha/2), 0, 0, np.sin(alpha/2)]).unsqueeze(0).repeat(batch_size*n_joints, 1)\n",
    "    quaternions = quaternions.reshape(batch_size*n_joints, 4)\n",
    "    quaternions = batch_quat_left_multiply(\n",
    "        batch_quat_inverse(rotation),\n",
    "        quaternions\n",
    "    )\n",
    "    quaternions = quaternions.reshape(batch_size, n_joints, 4)\n",
    "    \n",
    "    # Use these quaternions in the forward kinematics with the Poppy skeleton\n",
    "    target_positions = forward_kinematics(\n",
    "        topology,\n",
    "        torch.zeros(batch_size, 3),\n",
    "        target_offsets,\n",
    "        quaternions\n",
    "    )[0]\n",
    "    \n",
    "\n",
    "    \n",
    "    # Return only target positions for the end-effector of the 6 kinematic chains:\n",
    "    # Chest, head, left hand, left elbow, left shoulder, right hand, right elbow\n",
    "    # end_effector_indices = [8, 10, 13, 12, 11, 16, 15]\n",
    "    end_effector_indices = [13, 16]\n",
    "    # end_effector_indices = [13, 12, 16, 15]\n",
    "\n",
    "    return target_positions[:, end_effector_indices], target_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b9654e",
   "metadata": {},
   "outputs": [],
   "source": [
    "poppy_lengths = torch.Tensor([\n",
    "    0.0,\n",
    "    0.07,\n",
    "    0.18,\n",
    "    0.19,\n",
    "    0.07,\n",
    "    0.18,\n",
    "    0.19,\n",
    "    0.12,\n",
    "    0.08,\n",
    "    0.07,\n",
    "    0.05,\n",
    "    0.1, \n",
    "    0.15,\n",
    "    0.13,\n",
    "    0.1,\n",
    "    0.15,\n",
    "    0.13\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc05faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets, all_positions = targets_from_skeleton(skeletons, topology)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676678b2",
   "metadata": {},
   "source": [
    "We can display the target skeleton with stars representing the end-effector target positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966744b4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "t = t if t is not None else 0\n",
    "t = -100\n",
    "\n",
    "ax = pyplot_skeleton(topology, all_positions[t], show=False)\n",
    "ax.scatter(targets[t, :, 0], targets[t, :, 1], targets[t, :, 2], c='red', s=500, marker='*')\n",
    "\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "set_axes_equal(ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8964897",
   "metadata": {},
   "source": [
    "For smoother movements, we compute a moving average of the target positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc72992",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(a, n=3) :\n",
    "    repeat_shape = list(a.shape)\n",
    "    repeat_shape[1:] = [1 for _ in range(len(repeat_shape)-1)]\n",
    "    repeat_shape[0] = n//2\n",
    "    a = torch.cat([a[:1].repeat(*repeat_shape), a, a[-2:].repeat(*repeat_shape)])\n",
    "    ret = torch.cumsum(a, axis=0)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret[n - 1:] / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84f3cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can be used to have more keypoints for the inverse kinematics. It could be useful for fast movements.\n",
    "# With factor=1, it does nothing\n",
    "def interpolate_targets(targets, factor=1):\n",
    "    \n",
    "    length, joints, _ = targets.shape\n",
    "    \n",
    "    new_targets = torch.zeros((length-1) * factor + 1, joints, 3)\n",
    "    \n",
    "    for i in range(new_targets.shape[0]):\n",
    "            \n",
    "        target_id = float(i/factor)\n",
    "        before_id = int(np.floor(target_id))\n",
    "        after_id = int(np.floor(target_id + 1))\n",
    "        \n",
    "        before_coef = 1 - (target_id - before_id)\n",
    "        after_coef = 1 - (after_id - target_id)\n",
    "        \n",
    "        if after_id > length - 1:\n",
    "            after_id = length - 1\n",
    "        \n",
    "        new_targets[i] = before_coef * targets[before_id] + after_coef * targets[after_id]\n",
    "        \n",
    "    return new_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d093ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03d847c",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolated_targets = interpolate_targets(targets)\n",
    "smoothed_targets = moving_average(interpolated_targets, n=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674c12fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(targets[:, :, 0])\n",
    "plt.suptitle('Raw Targets x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d35ed3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(targets[:, :, 1])\n",
    "plt.suptitle('Smoothed Targets x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d55b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(smoothed_targets[:, :, 0])\n",
    "plt.suptitle('Smoothed Targets y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13254637",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(smoothed_targets[:, :, 1])\n",
    "plt.suptitle('Smoothed Targets z')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d2f1ef",
   "metadata": {},
   "source": [
    "These two functions perform inverse kinematics combining multiple kinematic chains at the same time. The upper_body_imitation uses torso, head, elbows and hands targets, while the arms_imitation only focuses on the two hands.\n",
    "\n",
    "If positions are provided, its uses it as initial positions, otherwise, it uses the current simulated robot configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7394e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cb8e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_from_position(kinematic_chains, positions):\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for kc, joint_positions in zip(kinematic_chains, positions):       \n",
    "\n",
    "        for i, (motor, motor_position) in enumerate(zip(kc.motors, joint_positions)):\n",
    "                        \n",
    "            if motor.name not in enumerate(results.keys()) and kc.active_links_mask[i+1]:\n",
    "                results[motor.name] = [motor_position, 0.]\n",
    "                \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d252a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upper_body_imitation(poppy, targets, positions=None):\n",
    "    \n",
    "    # Targets should be a tensor of shape (6, 3)\n",
    "    # In order: chest, head, r_hand, l_hand, l_elbow, r_elbow\n",
    "    kinematic_chains = [\n",
    "        #poppy.torso_chain,\n",
    "        #poppy.head_chain,\n",
    "        #poppy.l_shoulder_chain,\n",
    "        poppy.l_arm_chain,\n",
    "        #poppy.l_hand_chain,\n",
    "        poppy.r_arm_chain,\n",
    "    ]\n",
    "    \n",
    "    next_positions = []\n",
    "    \n",
    "    for i, kinematic_chain in enumerate(kinematic_chains):\n",
    "        \n",
    "        if positions is not None:\n",
    "            next_positions.append(ik(kinematic_chain, targets[i], positions[i]))\n",
    "        else:\n",
    "            next_positions.append(ik(kinematic_chain, targets[i]))\n",
    "\n",
    "    return next_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d461c0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from pypot.primitive.move import Move"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daffdc91",
   "metadata": {},
   "source": [
    "This cell loops along the different frames of the input video and performs frame by frame imitation. It registers the found motor angles in a Move object that stores the whole Poppy movement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f05832",
   "metadata": {},
   "outputs": [],
   "source": [
    "poppy_reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7666c98e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fps = 10\n",
    "\n",
    "for motor in poppy.motors:\n",
    "    motor.compliant = False\n",
    "\n",
    "# Upper body imitation seems to work better\n",
    "# Otherwise, the elbow might go to wrong positions that can block the motion later\n",
    "\n",
    "kinematic_chains = [\n",
    "    # poppy.torso_chain,\n",
    "    #poppy.head_chain,\n",
    "    #poppy.l_shoulder_chain,\n",
    "    poppy.l_arm_chain,\n",
    "    #poppy.l_elbow_chain,\n",
    "    # poppy.l_hand_chain,\n",
    "    poppy.r_arm_chain,\n",
    "    #poppy.r_elbow_chain,\n",
    "    # poppy.r_hand_chain,\n",
    "]\n",
    "\n",
    "#kinematic_chains = [\n",
    "#    poppy.l_arm_chain,\n",
    "#    poppy.r_arm_chain,\n",
    "#]\n",
    "\n",
    "positions = [k.joints_position for k in kinematic_chains]\n",
    "\n",
    "move = Move(freq=fps)\n",
    "\n",
    "for t in range(smoothed_targets.shape[0]):\n",
    "    time.sleep(1./fps)\n",
    "    #positions = upper_body_imitation(\n",
    "    #    poppy, \n",
    "    #    smoothed_targets[t],\n",
    "    #    # positions = positions\n",
    "    #)\n",
    "\n",
    "    positions = upper_body_imitation(\n",
    "        poppy,\n",
    "        smoothed_targets[t],\n",
    "    )\n",
    "        \n",
    "    move.add_position(\n",
    "        dict_from_position(kinematic_chains, positions), \n",
    "        float(t)/fps\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc59b81d",
   "metadata": {},
   "source": [
    "We can plot the evolution of the motor angles during the trajectory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dab7f2f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "ax = plt.axes()\n",
    "move.plot(ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab09cb4f",
   "metadata": {},
   "source": [
    "save this movement to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb9cbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "move.save(open('move.record', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96e4833",
   "metadata": {},
   "source": [
    "## Play the saved move in the simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebb05db",
   "metadata": {},
   "outputs": [],
   "source": [
    "poppy_reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33b275c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypot.primitive.move import MovePlayer, MoveRecorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bba2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothen_move(move):\n",
    "    \n",
    "    # Function to smoothen the Poppy movement\n",
    "    n = 10\n",
    "    \n",
    "    # Create a tensor from the dictionary\n",
    "    motors = move.positions()[0].keys()\n",
    "    move_tensor = torch.Tensor([\n",
    "        [move.positions()[t][motor]  for motor in motors] for t in move.positions().keys()\n",
    "    ])\n",
    "    \n",
    "    # Control the motor range\n",
    "    move_tensor = torch.minimum(move_tensor, torch.full(move_tensor.shape, 180.))\n",
    "    move_tensor = torch.maximum(move_tensor, torch.full(move_tensor.shape, -180.))\n",
    "    \n",
    "    # Moving average to smoothen the positions\n",
    "    move_tensor = moving_average(move_tensor, n=n)\n",
    "    \n",
    "    # Compute velocity as the (next position - previous positions) * fps / 2\n",
    "    move_tensor[1:-1, :, 1] = (move_tensor[2:, :, 0] - move_tensor[:-2, :, 0]) * 0.5 * move.framerate\n",
    "    \n",
    "    # Rebuild the dictionary from the tensor\n",
    "    new_move = Move(freq=move.framerate)\n",
    "        \n",
    "    for i in range(move_tensor.shape[0]):\n",
    "        dictionary = {}\n",
    "        for j, motor in enumerate(motors):\n",
    "            dictionary[motor] = move_tensor[i, j].tolist()\n",
    "        new_move.add_position(\n",
    "            dictionary,\n",
    "            float(i)/fps\n",
    "        )\n",
    "\n",
    "    return new_move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122f521e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_move = smoothen_move(move)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233a2c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp = MovePlayer(poppy, new_move)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abf8a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2866c533",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "ax = plt.axes()\n",
    "new_move.plot(ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927fb1bf",
   "metadata": {},
   "source": [
    "Finally, we save the move in a file, copy the file through ssh to the poppy robot, and send an API request to play the move. Note that the computer executing the notebook should be on the same network than poppy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b91484",
   "metadata": {},
   "source": [
    "# Your project\n",
    "\n",
    "Your task is to use the reinforcement learning and inverse reinforcement learning algorithms to learn how Poppy can imitate the movement, without using the inverse kinematics library.\n",
    "\n",
    "You can use stable baselines or imitation libraries. You can also use your own implementation of the RL or iRL algorithm.\n",
    "For using the libraries, it is recommended that you create your own gymnamsium environment (NB gymnasium is the new version of gym, it can be used in the previous code by replacing the import line by 'import gymnasium as gym').\n",
    "\n",
    "- You can refer to the documentation by gym on https://gymnasium.farama.org/tutorials/gymnasium_basics/environment_creation/ or https://www.gymlibrary.dev/content/environment_creation/. They propose a tutorial python notebook\n",
    "- You can also look at an example project with a similar robot, ErgoJr : https://github.com/fgolemo/gym-ergojr with the gym environment implementation : https://github.com/fgolemo/gym-ergojr/blob/master/gym_ergojr/envs/ergo_reacher_env.py\n",
    "\n",
    "The action is the joint positions given to each of the motors.\n",
    "The observation are the cartesian positions that can be accessed by commands like poppy.l_arm_chain.position.\n",
    "\n",
    "\n",
    "You also need to decide on the reward function to be used.\n",
    "\n",
    "Two interesting articles to get your inspiration can be read : \n",
    "\n",
    "- https://arxiv.org/abs/2209.05135\n",
    "- https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8956326&casa_token=10GKHz8KJR8AAAAA:OZNWV-X7RxXJqLlRNqMBEtBg7jbH4fyy8pjDiMf5cOT65USEECinEMOiEVj0VW5sUDETHjGVgA&tag=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32301a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gym stable-baselines3 torch\n",
    "# !pip install tqdm shimmy\n",
    "# !pip install tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a738b7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils.skeleton import *\n",
    "from utils.quaternion import *\n",
    "from utils.blazepose import blazepose_skeletons\n",
    "import os\n",
    "from pypot.creatures import PoppyTorso\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "from pypot.creatures.ik import IKChain\n",
    "from pypot.primitive.move import Move\n",
    "from pypot.primitive.move import MovePlayer\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import os\n",
    "# from PoppyEnv import PoppyEnv\n",
    "# from Poppy_Env import PoppyEnv\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "# from imitation.algorithms import bc\n",
    "import numpy as np\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from Poppy_Env_edouard import PoppyEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d73d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env = PoppyEnv()\n",
    "vec_env = DummyVecEnv([lambda: env])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913751ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Chemin vers les fichiers de données\n",
    "path = './resources/sample_poppy_skeletons/'\n",
    "file = 'Edouard_0_poppy_skeletons.pt'\n",
    "data_path = path + file\n",
    "\n",
    "# Charger les données\n",
    "poppy_skeletons = torch.load(data_path)\n",
    "\n",
    "# Vous devez déterminer comment ces données se mappent à vos observations et actions\n",
    "# Supposons que vos observations sont les positions des joints et les actions sont des différences entre les positions actuelles et les cibles\n",
    "observations = poppy_skeletons[:-1]  # toutes les frames sauf la dernière\n",
    "actions = poppy_skeletons[1:] - poppy_skeletons[:-1]  # différences entre les frames consécutives\n",
    "\n",
    "# Sauvegarder les données formatées pour l'apprentissage par imitation\n",
    "np.savez(path+'formatted_training_data.npz', obs=observations, acts=actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d0077e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import load\n",
    "\n",
    "data = load(path+'formatted_training_data.npz')\n",
    "lst = data.files\n",
    "# for item in lst:\n",
    "#     print(item)\n",
    "#     print(data[item])\n",
    "    \n",
    "print('observations: ', data['obs'].shape)\n",
    "print('actions: ', data['acts'].shape)\n",
    "\n",
    "for i in range(200):\n",
    "    print(data['acts'][i][16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afcb784",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3d_hand_trajectory(targets, period=20, title='Targets -'):\n",
    "    \"\"\"\n",
    "    Plot 3D trajectories from a tensor and annotate specific points.\n",
    "    \n",
    "    Parameters:\n",
    "    - targets (torch.Tensor): A tensor with shape [N, 2, 3] where N is the number of time steps.\n",
    "    - period (int): The interval at which to place annotations and markers.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a 3D plot\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    # Extracting the coordinates for both lines\n",
    "    x1, y1, z1 = targets[:, 0, 0], targets[:, 0, 1], targets[:, 0, 2]\n",
    "    x2, y2, z2 = targets[:, 1, 0], targets[:, 1, 1], targets[:, 1, 2]\n",
    "    \n",
    "    # Plotting line 1\n",
    "    ax.plot(x1.numpy(), y1.numpy(), z1.numpy(), label='Line 1')\n",
    "    \n",
    "    # Plotting line 2\n",
    "    ax.plot(x2.numpy(), y2.numpy(), z2.numpy(), label='Line 2')\n",
    "    \n",
    "    # Plot a dot every 'period' frames, annotate them, and adjust dot size and text position\n",
    "    for t in range(0, len(targets), period):\n",
    "        ax.scatter(x1[t].numpy(), y1[t].numpy(), z1[t].numpy(), color='black', s=10)  # Smaller dot size\n",
    "        ax.scatter(x2[t].numpy(), y2[t].numpy(), z2[t].numpy(), color='black', s=10)  # Smaller dot size\n",
    "        ax.text(x1[t].numpy() + 0.02, y1[t].numpy(), z1[t].numpy(), t, color='black', ha='left',fontsize=6)\n",
    "        ax.text(x2[t].numpy() + 0.02, y2[t].numpy(), z2[t].numpy(), t, color='black', ha='left',fontsize=6)\n",
    "    \n",
    "    # Labels and title\n",
    "    ax.set_xlabel('X Coordinate')\n",
    "    ax.set_ylabel('Y Coordinate')\n",
    "    ax.set_zlabel('Z Coordinate')\n",
    "    ax.set_title(title + '3D Line Plot of Targets with Time Steps')\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "plot_3d_hand_trajectory(torch.from_numpy(data['obs'][:121, [13, 16]]), period=10, title='Targets (all) -')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba52f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a8f8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = PoppyEnv()\n",
    "observation = env.reset()\n",
    "print(\"Observation:\", observation)\n",
    "print(\"Shape:\", observation.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e83c3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_env = DummyVecEnv([lambda: PoppyEnv()])\n",
    "observations = vec_env.reset()\n",
    "print(\"Vectorized Observations:\", observations)\n",
    "print(\"Shape:\", observations.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3a22ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from utils.skeleton import *\n",
    "from utils.quaternion import *\n",
    "from utils.blazepose import blazepose_skeletons\n",
    "\n",
    "from pypot.creatures import PoppyTorso\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "import gymnasium as gym\n",
    "\n",
    "from Poppy_Env_edouard import PoppyEnv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b758c0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = PoppyEnv(observations, actions)\n",
    "env = PoppyEnv(gym.Env)\n",
    "\n",
    "# Envelopper l'environnement dans un DummyVecEnv\n",
    "vec_env = DummyVecEnv([lambda: env])\n",
    "\n",
    "vec_env.reset()\n",
    "\n",
    "targets_obs, l_joints_obs, r_joints_obs = vec_env.get_attr(observation)\n",
    "print(targets_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bedb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Fonction pour évaluer le modèle et obtenir la récompense moyenne\n",
    "def evaluate_model(model, env, num_episodes=10):\n",
    "    episode_rewards = []\n",
    "    for _ in range(num_episodes):\n",
    "        obs = env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, reward, done, _ = env.step(action)\n",
    "            episode_reward += reward\n",
    "        episode_rewards.append(episode_reward)\n",
    "    return episode_rewards\n",
    "\n",
    "# Charger les données d'entraînement\n",
    "# training_data = np.load(path+'formatted_training_data.npz')\n",
    "# observations = training_data[\"obs\"]\n",
    "# actions = training_data[\"acts\"]\n",
    "\n",
    "# Créer une instance de l'environnement Poppy en passant les données d'entraînement\n",
    "\n",
    "\n",
    "# Créer le modèle SAC\n",
    "model = SAC(\"MlpPolicy\", vec_env, verbose=1)\n",
    "\n",
    "# Entraîner le modèle sur 10000 steps\n",
    "model.learn(total_timesteps=10)\n",
    "\n",
    "# Sauvegarder le modèle\n",
    "model.save(\"sac_poppy_model\")\n",
    "\n",
    "# Évaluer le modèle après l'entraînement\n",
    "mean_rewards = evaluate_model(model, vec_env)\n",
    "\n",
    "# Tracer l'évolution de la récompense moyenne\n",
    "plt.plot(mean_rewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Mean Reward')\n",
    "plt.title('Evolution of Mean Reward')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a01eb1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from utils.skeleton import *\n",
    "from utils.quaternion import *\n",
    "from utils.blazepose import blazepose_skeletons\n",
    "\n",
    "from pypot.creatures import PoppyTorso\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "import gymnasium as gym\n",
    "\n",
    "from Poppy_Env_edouard import PoppyEnv\n",
    "from ddpg_edouard import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e7e3246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am Poppy!\n"
     ]
    }
   ],
   "source": [
    "# Initialiser l'environnement et l'agent\n",
    "env = PoppyEnv()\n",
    "agent = DDPGAgent(env, actor_lr=1e-2, critic_lr=1e-2, gamma=0.1, tau=0.005)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cdf9832",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(env.action_space.shape[0])\n",
    "# env.get_target()\n",
    "# print('target main gauche', env.targets[::5, 13])\n",
    "# print('target main droite', env.targets[::5, 16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "298e53b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading targets from :  anaele_bent_arms_0_poppy_skeletons.pt\n",
      "episode :  1\n",
      "current step :  0\n",
      "reward :  0.3896371481942537\n",
      "episode :  1\n",
      "current step :  5\n",
      "reward :  0.47186282879136\n",
      "episode :  1\n",
      "current step :  10\n",
      "reward :  0.4140712045480077\n",
      "episode :  1\n",
      "current step :  15\n",
      "reward :  0.4561944839007084\n",
      "episode :  1\n",
      "current step :  20\n",
      "reward :  0.4068702151693704\n",
      "episode :  1\n",
      "current step :  25\n",
      "reward :  0.47457944369976846\n",
      "episode :  1\n",
      "current step :  30\n",
      "reward :  0.4424319357375332\n",
      "episode :  1\n",
      "current step :  35\n",
      "reward :  0.49754912382645045\n",
      "episode :  1\n",
      "current step :  40\n",
      "reward :  0.45783962321161437\n",
      "episode :  1\n",
      "current step :  45\n",
      "reward :  0.47981817676693117\n",
      "episode :  1\n",
      "current step :  50\n",
      "reward :  0.5514265377934827\n",
      "episode :  1\n",
      "current step :  55\n",
      "reward :  0.539352265595441\n",
      "episode :  1\n",
      "current step :  60\n",
      "reward :  0.47541313499552185\n",
      "episode :  1\n",
      "current step :  65\n",
      "reward :  0.43620803033580835\n",
      "episode :  1\n",
      "current step :  70\n",
      "reward :  0.5232919326140179\n",
      "episode :  1\n",
      "current step :  75\n",
      "reward :  0.38496132023994123\n",
      "episode :  1\n",
      "current step :  80\n",
      "reward :  0.40399548702779975\n",
      "episode :  1\n",
      "current step :  85\n",
      "reward :  0.4682374462044525\n",
      "episode :  1\n",
      "current step :  90\n",
      "reward :  0.28730138548598777\n",
      "episode :  1\n",
      "current step :  95\n",
      "reward :  0.27196371918345047\n",
      "episode :  1\n",
      "current step :  100\n",
      "reward :  0.4320029045083349\n",
      "episode :  1\n",
      "current step :  105\n",
      "reward :  0.21942934212932602\n",
      "episode :  1\n",
      "current step :  110\n",
      "reward :  0.40377238670089677\n",
      "episode :  1\n",
      "current step :  115\n",
      "reward :  0.40082762519585813\n",
      "episode :  1\n",
      "current step :  120\n",
      "reward :  0.35620129251405186\n",
      "episode :  1\n",
      "current step :  125\n",
      "reward :  0.4702070822627147\n",
      "episode :  1\n",
      "current step :  130\n",
      "reward :  0.4247196241018806\n",
      "episode :  1\n",
      "current step :  135\n",
      "reward :  0.49800277627451683\n",
      "episode :  1\n",
      "current step :  140\n",
      "reward :  0.386037466478119\n",
      "episode :  1\n",
      "current step :  145\n",
      "reward :  0.6082565427416915\n",
      "Episode 1, Total Reward: 13.032462486229289\n",
      "episode :  2\n",
      "current step :  0\n",
      "reward :  0.45716028095754035\n",
      "episode :  2\n",
      "current step :  5\n",
      "reward :  0.5857785509249939\n",
      "episode :  2\n",
      "current step :  10\n",
      "reward :  0.6305718268708971\n",
      "episode :  2\n",
      "current step :  15\n",
      "reward :  0.4818356068694486\n",
      "episode :  2\n",
      "current step :  20\n",
      "reward :  0.3054506270137343\n",
      "episode :  2\n",
      "current step :  25\n",
      "reward :  0.44570425375582695\n",
      "episode :  2\n",
      "current step :  30\n",
      "reward :  0.27759174486423044\n",
      "episode :  2\n",
      "current step :  35\n",
      "reward :  0.35227740570179333\n",
      "episode :  2\n",
      "current step :  40\n",
      "reward :  0.5213276950807113\n",
      "episode :  2\n",
      "current step :  45\n",
      "reward :  0.5188685586378927\n",
      "episode :  2\n",
      "current step :  50\n",
      "reward :  0.6540032855721396\n",
      "episode :  2\n",
      "current step :  55\n",
      "reward :  0.5626396461313552\n",
      "episode :  2\n",
      "current step :  60\n",
      "reward :  0.4866944039800365\n",
      "episode :  2\n",
      "current step :  65\n",
      "reward :  0.49741723716151953\n",
      "episode :  2\n",
      "current step :  70\n",
      "reward :  0.5892652186762757\n",
      "episode :  2\n",
      "current step :  75\n",
      "reward :  0.36906612031734554\n",
      "episode :  2\n",
      "current step :  80\n",
      "reward :  0.5365308807453874\n",
      "episode :  2\n",
      "current step :  85\n",
      "reward :  0.46632161259993354\n",
      "episode :  2\n",
      "current step :  90\n",
      "reward :  0.2566255578572591\n",
      "episode :  2\n",
      "current step :  95\n",
      "reward :  0.29051463920365356\n",
      "episode :  2\n",
      "current step :  100\n",
      "reward :  0.4065839704671188\n",
      "episode :  2\n",
      "current step :  105\n",
      "reward :  0.3573617047791161\n",
      "episode :  2\n",
      "current step :  110\n",
      "reward :  0.4750745184406134\n",
      "episode :  2\n",
      "current step :  115\n",
      "reward :  0.505427044643511\n",
      "episode :  2\n",
      "current step :  120\n",
      "reward :  0.2366255459369593\n",
      "episode :  2\n",
      "current step :  125\n",
      "reward :  0.4200295988492748\n",
      "episode :  2\n",
      "current step :  130\n",
      "reward :  0.388312341240985\n",
      "episode :  2\n",
      "current step :  135\n",
      "reward :  0.39260455816324635\n",
      "episode :  2\n",
      "current step :  140\n",
      "reward :  0.23147631780145775\n",
      "episode :  2\n",
      "current step :  145\n",
      "reward :  0.45303341562629484\n",
      "Episode 2, Total Reward: 13.152174168870554\n",
      "episode :  3\n",
      "current step :  0\n",
      "reward :  0.5632824938039253\n",
      "episode :  3\n",
      "current step :  5\n",
      "reward :  0.48904752777048155\n",
      "episode :  3\n",
      "current step :  10\n",
      "reward :  0.4895003839310644\n",
      "episode :  3\n",
      "current step :  15\n",
      "reward :  0.3722430846347745\n",
      "episode :  3\n",
      "current step :  20\n",
      "reward :  0.6229945233454908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\edoua\\_Travail\\IA705\\Projet2023M\\Poppy\\ddpg_edouard.py:77: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:278.)\n",
      "  states, actions, rewards, next_states, dones = map(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode :  3\n",
      "current step :  25\n",
      "reward :  0.3960897904903194\n",
      "episode :  3\n",
      "current step :  30\n",
      "reward :  0.46276178206306473\n",
      "episode :  3\n",
      "current step :  35\n",
      "reward :  0.4654127426209539\n",
      "episode :  3\n",
      "current step :  40\n",
      "reward :  0.3924250552780984\n",
      "episode :  3\n",
      "current step :  45\n",
      "reward :  0.24536806622773646\n",
      "episode :  3\n",
      "current step :  50\n",
      "reward :  0.30035456607902766\n",
      "episode :  3\n",
      "current step :  55\n",
      "reward :  0.25897994622193504\n",
      "episode :  3\n",
      "current step :  60\n",
      "reward :  0.0\n",
      "episode :  3\n",
      "current step :  65\n",
      "reward :  0.18043525078934092\n",
      "episode :  3\n",
      "current step :  70\n",
      "reward :  0.5170803655187448\n",
      "episode :  3\n",
      "current step :  75\n",
      "reward :  0.38424334293185053\n",
      "episode :  3\n",
      "current step :  80\n",
      "reward :  0.34108877603554905\n",
      "episode :  3\n",
      "current step :  85\n",
      "reward :  0.4965308851508211\n",
      "episode :  3\n",
      "current step :  90\n",
      "reward :  0.16735518444018602\n",
      "episode :  3\n",
      "current step :  95\n",
      "reward :  0.15967820298253296\n",
      "episode :  3\n",
      "current step :  100\n",
      "reward :  0.5421585179439568\n",
      "episode :  3\n",
      "current step :  105\n",
      "reward :  0.3915987599963182\n",
      "episode :  3\n",
      "current step :  110\n",
      "reward :  0.42570858492583413\n",
      "episode :  3\n",
      "current step :  115\n",
      "reward :  0.389808501377621\n",
      "episode :  3\n",
      "current step :  120\n",
      "reward :  0.45756500842718684\n",
      "episode :  3\n",
      "current step :  125\n",
      "reward :  0.4244525543884363\n",
      "episode :  3\n",
      "current step :  130\n",
      "reward :  0.5940088568727562\n",
      "episode :  3\n",
      "current step :  135\n",
      "reward :  0.5634702381379911\n",
      "episode :  3\n",
      "current step :  140\n",
      "reward :  0.4475153238846862\n",
      "episode :  3\n",
      "current step :  145\n",
      "reward :  0.3593729400290757\n",
      "Episode 3, Total Reward: 11.900531256299757\n",
      "episode :  4\n",
      "current step :  0\n",
      "reward :  0.36827972745780135\n",
      "episode :  4\n",
      "current step :  5\n",
      "reward :  0.4362470686001463\n",
      "episode :  4\n",
      "current step :  10\n",
      "reward :  0.43025573240768944\n",
      "episode :  4\n",
      "current step :  15\n",
      "reward :  0.42943890670297\n",
      "episode :  4\n",
      "current step :  20\n",
      "reward :  0.5693851152220791\n",
      "episode :  4\n",
      "current step :  25\n",
      "reward :  0.37006097429813883\n",
      "episode :  4\n",
      "current step :  30\n",
      "reward :  0.43014439546796\n",
      "episode :  4\n",
      "current step :  35\n",
      "reward :  0.3792023681514201\n",
      "episode :  4\n",
      "current step :  40\n",
      "reward :  0.4815445162387798\n",
      "episode :  4\n",
      "current step :  45\n",
      "reward :  0.30415056400462537\n",
      "episode :  4\n",
      "current step :  50\n",
      "reward :  0.47251763526494117\n",
      "episode :  4\n",
      "current step :  55\n",
      "reward :  0.45792999717964844\n",
      "episode :  4\n",
      "current step :  60\n",
      "reward :  0.2717471251726674\n",
      "episode :  4\n",
      "current step :  65\n",
      "reward :  0.25302699865234307\n",
      "episode :  4\n",
      "current step :  70\n",
      "reward :  0.5562518999050493\n",
      "episode :  4\n",
      "current step :  75\n",
      "reward :  0.37878175353355414\n",
      "episode :  4\n",
      "current step :  80\n",
      "reward :  0.39981152057617925\n",
      "episode :  4\n",
      "current step :  85\n",
      "reward :  0.5126541786098128\n",
      "episode :  4\n",
      "current step :  90\n",
      "reward :  0.4287723093520208\n",
      "episode :  4\n",
      "current step :  95\n",
      "reward :  0.3964259000456104\n",
      "episode :  4\n",
      "current step :  100\n",
      "reward :  0.3873485392396415\n",
      "episode :  4\n",
      "current step :  105\n",
      "reward :  0.4831681615873625\n",
      "episode :  4\n",
      "current step :  110\n",
      "reward :  0.5291547673873211\n",
      "episode :  4\n",
      "current step :  115\n",
      "reward :  0.47788816559843894\n",
      "episode :  4\n",
      "current step :  120\n",
      "reward :  0.38403024035416533\n",
      "episode :  4\n",
      "current step :  125\n",
      "reward :  0.42618791905086917\n",
      "episode :  4\n",
      "current step :  130\n",
      "reward :  0.4617257483350254\n",
      "episode :  4\n",
      "current step :  135\n",
      "reward :  0.48225892874961174\n",
      "episode :  4\n",
      "current step :  140\n",
      "reward :  0.4025507888206093\n",
      "episode :  4\n",
      "current step :  145\n",
      "reward :  0.4455501763595777\n",
      "Episode 4, Total Reward: 12.80649212232606\n",
      "episode :  5\n",
      "current step :  0\n",
      "reward :  0.24632612264261716\n",
      "episode :  5\n",
      "current step :  5\n",
      "reward :  0.522205349134707\n",
      "episode :  5\n",
      "current step :  10\n",
      "reward :  0.38111341311613295\n",
      "episode :  5\n",
      "current step :  15\n",
      "reward :  0.45104033641840774\n",
      "episode :  5\n",
      "current step :  20\n",
      "reward :  0.2081077830488932\n",
      "episode :  5\n",
      "current step :  25\n",
      "reward :  0.3195464438596768\n",
      "episode :  5\n",
      "current step :  30\n",
      "reward :  0.4327806889787733\n",
      "episode :  5\n",
      "current step :  35\n",
      "reward :  0.2898031262831716\n",
      "episode :  5\n",
      "current step :  40\n",
      "reward :  0.5085208795407439\n",
      "episode :  5\n",
      "current step :  45\n",
      "reward :  0.43021502320146043\n",
      "episode :  5\n",
      "current step :  50\n",
      "reward :  0.2343532284613035\n",
      "episode :  5\n",
      "current step :  55\n",
      "reward :  0.19657527330326283\n",
      "episode :  5\n",
      "current step :  60\n",
      "reward :  0.33458298838190503\n",
      "episode :  5\n",
      "current step :  65\n",
      "reward :  0.23014022471373147\n",
      "episode :  5\n",
      "current step :  70\n",
      "reward :  0.37951868674114836\n",
      "episode :  5\n",
      "current step :  75\n",
      "reward :  0.3574844873265956\n",
      "episode :  5\n",
      "current step :  80\n",
      "reward :  0.4301378090575061\n",
      "episode :  5\n",
      "current step :  85\n",
      "reward :  0.30847655254690814\n",
      "episode :  5\n",
      "current step :  90\n",
      "reward :  0.45829617751423\n",
      "episode :  5\n",
      "current step :  95\n",
      "reward :  0.35500609549153994\n",
      "episode :  5\n",
      "current step :  100\n",
      "reward :  0.4860697450394676\n",
      "episode :  5\n",
      "current step :  105\n",
      "reward :  0.2790919281479224\n",
      "episode :  5\n",
      "current step :  110\n",
      "reward :  0.5308692792249766\n",
      "episode :  5\n",
      "current step :  115\n",
      "reward :  0.20482056432418783\n",
      "episode :  5\n",
      "current step :  120\n",
      "reward :  0.4413373737151495\n",
      "episode :  5\n",
      "current step :  125\n",
      "reward :  0.3864064495278827\n",
      "episode :  5\n",
      "current step :  130\n",
      "reward :  0.46375529437967944\n",
      "episode :  5\n",
      "current step :  135\n",
      "reward :  0.4044767284082236\n",
      "episode :  5\n",
      "current step :  140\n",
      "reward :  0.3754134670314254\n",
      "episode :  5\n",
      "current step :  145\n",
      "reward :  0.43561505441405024\n",
      "Episode 5, Total Reward: 11.08208657397568\n",
      "episode :  6\n",
      "current step :  0\n",
      "reward :  0.2050742667415358\n",
      "episode :  6\n",
      "current step :  5\n",
      "reward :  0.48733381081059013\n",
      "episode :  6\n",
      "current step :  10\n",
      "reward :  0.48070374358273854\n",
      "episode :  6\n",
      "current step :  15\n",
      "reward :  0.42430841099329086\n",
      "episode :  6\n",
      "current step :  20\n",
      "reward :  0.4044121363117503\n",
      "episode :  6\n",
      "current step :  25\n",
      "reward :  0.4487970079189615\n",
      "episode :  6\n",
      "current step :  30\n",
      "reward :  0.5148553766093953\n",
      "episode :  6\n",
      "current step :  35\n",
      "reward :  0.3447704040558702\n",
      "episode :  6\n",
      "current step :  40\n",
      "reward :  0.36245077103258283\n",
      "episode :  6\n",
      "current step :  45\n",
      "reward :  0.505425379467465\n",
      "episode :  6\n",
      "current step :  50\n",
      "reward :  0.42090551575106994\n",
      "episode :  6\n",
      "current step :  55\n",
      "reward :  0.4867630307835783\n",
      "episode :  6\n",
      "current step :  60\n",
      "reward :  0.26789490366441776\n",
      "episode :  6\n",
      "current step :  65\n",
      "reward :  0.19245260167273806\n",
      "episode :  6\n",
      "current step :  70\n",
      "reward :  0.39619469305403804\n",
      "episode :  6\n",
      "current step :  75\n",
      "reward :  0.39938299070700983\n",
      "episode :  6\n",
      "current step :  80\n",
      "reward :  0.39991646388234464\n",
      "episode :  6\n",
      "current step :  85\n",
      "reward :  0.4309182383786101\n",
      "episode :  6\n",
      "current step :  90\n",
      "reward :  0.5593078154234399\n",
      "episode :  6\n",
      "current step :  95\n",
      "reward :  0.39456539590597484\n",
      "episode :  6\n",
      "current step :  100\n",
      "reward :  0.513377851392674\n",
      "episode :  6\n",
      "current step :  105\n",
      "reward :  0.3959176255301732\n",
      "episode :  6\n",
      "current step :  110\n",
      "reward :  0.38043572257616537\n",
      "episode :  6\n",
      "current step :  115\n",
      "reward :  0.5001539156478159\n",
      "episode :  6\n",
      "current step :  120\n",
      "reward :  0.39292153440592054\n",
      "episode :  6\n",
      "current step :  125\n",
      "reward :  0.4224951754064786\n",
      "episode :  6\n",
      "current step :  130\n",
      "reward :  0.4800757921444548\n",
      "episode :  6\n",
      "current step :  135\n",
      "reward :  0.38674750447391115\n",
      "episode :  6\n",
      "current step :  140\n",
      "reward :  0.5328291900172575\n",
      "episode :  6\n",
      "current step :  145\n",
      "reward :  0.39461548440742655\n",
      "Episode 6, Total Reward: 12.526002752749674\n",
      "episode :  7\n",
      "current step :  0\n",
      "reward :  0.5206555404707939\n",
      "episode :  7\n",
      "current step :  5\n",
      "reward :  0.4116115996267619\n",
      "episode :  7\n",
      "current step :  10\n",
      "reward :  0.49879647239807823\n",
      "episode :  7\n",
      "current step :  15\n",
      "reward :  0.4281199939338922\n",
      "episode :  7\n",
      "current step :  20\n",
      "reward :  0.4880430000249346\n",
      "episode :  7\n",
      "current step :  25\n",
      "reward :  0.34507784237253003\n",
      "episode :  7\n",
      "current step :  30\n",
      "reward :  0.2807921103075611\n",
      "episode :  7\n",
      "current step :  35\n",
      "reward :  0.22109004174500524\n",
      "episode :  7\n",
      "current step :  40\n",
      "reward :  0.34504327437540344\n",
      "episode :  7\n",
      "current step :  45\n",
      "reward :  0.4753981189897766\n",
      "episode :  7\n",
      "current step :  50\n",
      "reward :  0.2996363136781457\n",
      "episode :  7\n",
      "current step :  55\n",
      "reward :  0.4209729473384009\n",
      "episode :  7\n",
      "current step :  60\n",
      "reward :  0.48399773394038265\n",
      "episode :  7\n",
      "current step :  65\n",
      "reward :  0.380455402929163\n",
      "episode :  7\n",
      "current step :  70\n",
      "reward :  0.33069320187859474\n",
      "episode :  7\n",
      "current step :  75\n",
      "reward :  0.34281419774872957\n",
      "episode :  7\n",
      "current step :  80\n",
      "reward :  0.3761704028665413\n",
      "episode :  7\n",
      "current step :  85\n",
      "reward :  0.47770376403039533\n",
      "episode :  7\n",
      "current step :  90\n",
      "reward :  0.28185312535412577\n",
      "episode :  7\n",
      "current step :  95\n",
      "reward :  0.4088953283398167\n",
      "episode :  7\n",
      "current step :  100\n",
      "reward :  0.2567656685494763\n",
      "episode :  7\n",
      "current step :  105\n",
      "reward :  0.2419718187245164\n",
      "episode :  7\n",
      "current step :  110\n",
      "reward :  0.5273660355650203\n",
      "episode :  7\n",
      "current step :  115\n",
      "reward :  0.46895734749484475\n",
      "episode :  7\n",
      "current step :  120\n",
      "reward :  0.2590088517399726\n",
      "episode :  7\n",
      "current step :  125\n",
      "reward :  0.5469732529080173\n",
      "episode :  7\n",
      "current step :  130\n",
      "reward :  0.46809886199235473\n",
      "episode :  7\n",
      "current step :  135\n",
      "reward :  0.586960837685272\n",
      "episode :  7\n",
      "current step :  140\n",
      "reward :  0.49873207021697297\n",
      "episode :  7\n",
      "current step :  145\n",
      "reward :  0.4581691193642764\n",
      "Episode 7, Total Reward: 12.130824276589756\n",
      "episode :  8\n",
      "current step :  0\n",
      "reward :  0.4787413860243519\n",
      "episode :  8\n",
      "current step :  5\n",
      "reward :  0.40545900341013746\n",
      "episode :  8\n",
      "current step :  10\n",
      "reward :  0.5211066701396352\n",
      "episode :  8\n",
      "current step :  15\n",
      "reward :  0.45658433328941095\n",
      "episode :  8\n",
      "current step :  20\n",
      "reward :  0.4914577189822371\n",
      "episode :  8\n",
      "current step :  25\n",
      "reward :  0.49264419343099214\n",
      "episode :  8\n",
      "current step :  30\n",
      "reward :  0.5141507825816224\n",
      "episode :  8\n",
      "current step :  35\n",
      "reward :  0.6086924207520964\n",
      "episode :  8\n",
      "current step :  40\n",
      "reward :  0.335072579632749\n",
      "episode :  8\n",
      "current step :  45\n",
      "reward :  0.43700047867184943\n",
      "episode :  8\n",
      "current step :  50\n",
      "reward :  0.3539988419706661\n",
      "episode :  8\n",
      "current step :  55\n",
      "reward :  0.2856255462504419\n",
      "episode :  8\n",
      "current step :  60\n",
      "reward :  0.21169641189450414\n",
      "episode :  8\n",
      "current step :  65\n",
      "reward :  0.210199680001237\n",
      "episode :  8\n",
      "current step :  70\n",
      "reward :  0.3039646541562866\n",
      "episode :  8\n",
      "current step :  75\n",
      "reward :  0.6002826831613077\n",
      "episode :  8\n",
      "current step :  80\n",
      "reward :  0.5289316489915076\n",
      "episode :  8\n",
      "current step :  85\n",
      "reward :  0.5591148713914138\n",
      "episode :  8\n",
      "current step :  90\n",
      "reward :  0.4246127823001078\n",
      "episode :  8\n",
      "current step :  95\n",
      "reward :  0.4566069672047814\n",
      "episode :  8\n",
      "current step :  100\n",
      "reward :  0.42018620014712244\n",
      "episode :  8\n",
      "current step :  105\n",
      "reward :  0.37631682108678854\n",
      "episode :  8\n",
      "current step :  110\n",
      "reward :  0.312192989521812\n",
      "episode :  8\n",
      "current step :  115\n",
      "reward :  0.38292766047995086\n",
      "episode :  8\n",
      "current step :  120\n",
      "reward :  0.3651107286325428\n",
      "episode :  8\n",
      "current step :  125\n",
      "reward :  0.4190520943095566\n",
      "episode :  8\n",
      "current step :  130\n",
      "reward :  0.4205864405246636\n",
      "episode :  8\n",
      "current step :  135\n",
      "reward :  0.45906861551744194\n",
      "episode :  8\n",
      "current step :  140\n",
      "reward :  0.3353000116086169\n",
      "episode :  8\n",
      "current step :  145\n",
      "reward :  0.4017939704270783\n",
      "Episode 8, Total Reward: 12.568479186492908\n",
      "episode :  9\n",
      "current step :  0\n",
      "reward :  0.3340217287409354\n",
      "episode :  9\n",
      "current step :  5\n",
      "reward :  0.5024829609570791\n",
      "episode :  9\n",
      "current step :  10\n",
      "reward :  0.45467822205104264\n",
      "episode :  9\n",
      "current step :  15\n",
      "reward :  0.3349345264457317\n",
      "episode :  9\n",
      "current step :  20\n",
      "reward :  0.5567784306983495\n",
      "episode :  9\n",
      "current step :  25\n",
      "reward :  0.33689805628904007\n",
      "episode :  9\n",
      "current step :  30\n",
      "reward :  0.43625061391622577\n",
      "episode :  9\n",
      "current step :  35\n",
      "reward :  0.0\n",
      "episode :  9\n",
      "current step :  40\n",
      "reward :  0.29573474295184554\n",
      "episode :  9\n",
      "current step :  45\n",
      "reward :  0.27706414507757515\n",
      "episode :  9\n",
      "current step :  50\n",
      "reward :  0.47876479712382536\n",
      "episode :  9\n",
      "current step :  55\n",
      "reward :  0.4197814510120207\n",
      "episode :  9\n",
      "current step :  60\n",
      "reward :  0.39102912499485465\n",
      "episode :  9\n",
      "current step :  65\n",
      "reward :  0.1861298016169944\n",
      "episode :  9\n",
      "current step :  70\n",
      "reward :  0.3803382704417306\n",
      "episode :  9\n",
      "current step :  75\n",
      "reward :  0.30431700380250193\n",
      "episode :  9\n",
      "current step :  80\n",
      "reward :  0.494965205895047\n",
      "episode :  9\n",
      "current step :  85\n",
      "reward :  0.5118269448221714\n",
      "episode :  9\n",
      "current step :  90\n",
      "reward :  0.31855858572050033\n",
      "episode :  9\n",
      "current step :  95\n",
      "reward :  0.3920964509666082\n",
      "episode :  9\n",
      "current step :  100\n",
      "reward :  0.41290836766045347\n",
      "episode :  9\n",
      "current step :  105\n",
      "reward :  0.49606640290213555\n",
      "episode :  9\n",
      "current step :  110\n",
      "reward :  0.2325002043977439\n",
      "episode :  9\n",
      "current step :  115\n",
      "reward :  0.4363289918029415\n",
      "episode :  9\n",
      "current step :  120\n",
      "reward :  0.3500017244421356\n",
      "episode :  9\n",
      "current step :  125\n",
      "reward :  0.5144026199641314\n",
      "episode :  9\n",
      "current step :  130\n",
      "reward :  0.43725398784189823\n",
      "episode :  9\n",
      "current step :  135\n",
      "reward :  0.4330050117242082\n",
      "episode :  9\n",
      "current step :  140\n",
      "reward :  0.48846370707864895\n",
      "episode :  9\n",
      "current step :  145\n",
      "reward :  0.4752598932008586\n",
      "Episode 9, Total Reward: 11.682841974539237\n",
      "episode :  10\n",
      "current step :  0\n",
      "reward :  0.44756477766052377\n",
      "episode :  10\n",
      "current step :  5\n",
      "reward :  0.4025764750299239\n",
      "episode :  10\n",
      "current step :  10\n",
      "reward :  0.46608115844544196\n",
      "episode :  10\n",
      "current step :  15\n",
      "reward :  0.568101518550497\n",
      "episode :  10\n",
      "current step :  20\n",
      "reward :  0.33174276845999995\n",
      "episode :  10\n",
      "current step :  25\n",
      "reward :  0.5147072899426041\n",
      "episode :  10\n",
      "current step :  30\n",
      "reward :  0.25043776433886467\n",
      "episode :  10\n",
      "current step :  35\n",
      "reward :  0.37992375421810065\n",
      "episode :  10\n",
      "current step :  40\n",
      "reward :  0.4304017339697317\n",
      "episode :  10\n",
      "current step :  45\n",
      "reward :  0.3636746428782412\n",
      "episode :  10\n",
      "current step :  50\n",
      "reward :  0.4451168612409936\n",
      "episode :  10\n",
      "current step :  55\n",
      "reward :  0.21738257540418876\n",
      "episode :  10\n",
      "current step :  60\n",
      "reward :  0.36188927055118003\n",
      "episode :  10\n",
      "current step :  65\n",
      "reward :  0.2214794367955643\n",
      "episode :  10\n",
      "current step :  70\n",
      "reward :  0.1937956543342782\n",
      "episode :  10\n",
      "current step :  75\n",
      "reward :  0.41453985450862024\n",
      "episode :  10\n",
      "current step :  80\n",
      "reward :  0.5114605302927907\n",
      "episode :  10\n",
      "current step :  85\n",
      "reward :  0.49609335476261224\n",
      "episode :  10\n",
      "current step :  90\n",
      "reward :  0.47680023574541697\n",
      "episode :  10\n",
      "current step :  95\n",
      "reward :  0.5228435477156983\n",
      "episode :  10\n",
      "current step :  100\n",
      "reward :  0.413582999204016\n",
      "episode :  10\n",
      "current step :  105\n",
      "reward :  0.34791079989814433\n",
      "episode :  10\n",
      "current step :  110\n",
      "reward :  0.3753533323189233\n",
      "episode :  10\n",
      "current step :  115\n",
      "reward :  0.3768410653173703\n",
      "episode :  10\n",
      "current step :  120\n",
      "reward :  0.23168928146469295\n",
      "episode :  10\n",
      "current step :  125\n",
      "reward :  0.522344652330678\n",
      "episode :  10\n",
      "current step :  130\n",
      "reward :  0.381767998856696\n",
      "episode :  10\n",
      "current step :  135\n",
      "reward :  0.39495732962319957\n",
      "episode :  10\n",
      "current step :  140\n",
      "reward :  0.4791842478356113\n",
      "episode :  10\n",
      "current step :  145\n",
      "reward :  0.3935459836359789\n",
      "Episode 10, Total Reward: 11.933790895330583\n",
      "episode :  11\n",
      "current step :  0\n",
      "reward :  0.4209726833486272\n",
      "episode :  11\n",
      "current step :  5\n",
      "reward :  0.378617521956722\n",
      "episode :  11\n",
      "current step :  10\n",
      "reward :  0.39083465328296535\n",
      "episode :  11\n",
      "current step :  15\n",
      "reward :  0.5699132333887365\n",
      "episode :  11\n",
      "current step :  20\n",
      "reward :  0.38370165419728886\n",
      "episode :  11\n",
      "current step :  25\n",
      "reward :  0.41173612761132816\n",
      "episode :  11\n",
      "current step :  30\n",
      "reward :  0.4168873988240259\n",
      "episode :  11\n",
      "current step :  35\n",
      "reward :  0.3504743730178159\n",
      "episode :  11\n",
      "current step :  40\n",
      "reward :  0.5611372727764394\n",
      "episode :  11\n",
      "current step :  45\n",
      "reward :  0.3500117892542994\n",
      "episode :  11\n",
      "current step :  50\n",
      "reward :  0.4100519341321224\n",
      "episode :  11\n",
      "current step :  55\n",
      "reward :  0.26564597362280085\n",
      "episode :  11\n",
      "current step :  60\n",
      "reward :  0.24089289288758173\n",
      "episode :  11\n",
      "current step :  65\n",
      "reward :  0.38068293119128976\n",
      "episode :  11\n",
      "current step :  70\n",
      "reward :  0.5064768128482132\n",
      "episode :  11\n",
      "current step :  75\n",
      "reward :  0.38751734708541125\n",
      "episode :  11\n",
      "current step :  80\n",
      "reward :  0.36169347443266703\n",
      "episode :  11\n",
      "current step :  85\n",
      "reward :  0.43330649733453286\n",
      "episode :  11\n",
      "current step :  90\n",
      "reward :  0.4001222708509149\n",
      "episode :  11\n",
      "current step :  95\n",
      "reward :  0.4502353040077198\n",
      "episode :  11\n",
      "current step :  100\n",
      "reward :  0.5512253232709212\n",
      "episode :  11\n",
      "current step :  105\n",
      "reward :  0.4857703958035941\n",
      "episode :  11\n",
      "current step :  110\n",
      "reward :  0.4693296763732465\n",
      "episode :  11\n",
      "current step :  115\n",
      "reward :  0.4689577586951773\n",
      "episode :  11\n",
      "current step :  120\n",
      "reward :  0.32144221197541667\n",
      "episode :  11\n",
      "current step :  125\n",
      "reward :  0.42003419588149493\n",
      "episode :  11\n",
      "current step :  130\n",
      "reward :  0.4207708943681633\n",
      "episode :  11\n",
      "current step :  135\n",
      "reward :  0.43165166466154536\n",
      "episode :  11\n",
      "current step :  140\n",
      "reward :  0.4520662621669068\n",
      "episode :  11\n",
      "current step :  145\n",
      "reward :  0.5003216132939968\n",
      "Episode 11, Total Reward: 12.592482142541966\n",
      "episode :  12\n",
      "current step :  0\n",
      "reward :  0.5584990732454346\n",
      "episode :  12\n",
      "current step :  5\n",
      "reward :  0.35311354967015085\n",
      "episode :  12\n",
      "current step :  10\n",
      "reward :  0.37567902448608503\n",
      "episode :  12\n",
      "current step :  15\n",
      "reward :  0.466867142803305\n",
      "episode :  12\n",
      "current step :  20\n",
      "reward :  0.5308708348056538\n",
      "episode :  12\n",
      "current step :  25\n",
      "reward :  0.47026928717608196\n",
      "episode :  12\n",
      "current step :  30\n",
      "reward :  0.28172181504835603\n",
      "episode :  12\n",
      "current step :  35\n",
      "reward :  0.4648358263161715\n",
      "episode :  12\n",
      "current step :  40\n",
      "reward :  0.4999395035302274\n",
      "episode :  12\n",
      "current step :  45\n",
      "reward :  0.4909184009646178\n",
      "episode :  12\n",
      "current step :  50\n",
      "reward :  0.4976089206758395\n",
      "episode :  12\n",
      "current step :  55\n",
      "reward :  0.24587450543900963\n",
      "episode :  12\n",
      "current step :  60\n",
      "reward :  0.3527740089010855\n",
      "episode :  12\n",
      "current step :  65\n",
      "reward :  0.30539664931548305\n",
      "episode :  12\n",
      "current step :  70\n",
      "reward :  0.43153908188757667\n",
      "episode :  12\n",
      "current step :  75\n",
      "reward :  0.42014339695813957\n",
      "episode :  12\n",
      "current step :  80\n",
      "reward :  0.5433981720497836\n",
      "episode :  12\n",
      "current step :  85\n",
      "reward :  0.4235407373527923\n",
      "episode :  12\n",
      "current step :  90\n",
      "reward :  0.46194301800272625\n",
      "episode :  12\n",
      "current step :  95\n",
      "reward :  0.22896317878425185\n",
      "episode :  12\n",
      "current step :  100\n",
      "reward :  0.5044361848283535\n",
      "episode :  12\n",
      "current step :  105\n",
      "reward :  0.4528941604083375\n",
      "episode :  12\n",
      "current step :  110\n",
      "reward :  0.5109042644703936\n",
      "episode :  12\n",
      "current step :  115\n",
      "reward :  0.15898788262934102\n",
      "episode :  12\n",
      "current step :  120\n",
      "reward :  0.39793719092300783\n",
      "episode :  12\n",
      "current step :  125\n",
      "reward :  0.45428928785051037\n",
      "episode :  12\n",
      "current step :  130\n",
      "reward :  0.4273514005437711\n",
      "episode :  12\n",
      "current step :  135\n",
      "reward :  0.6089659377459218\n",
      "episode :  12\n",
      "current step :  140\n",
      "reward :  0.4448257693263893\n",
      "episode :  12\n",
      "current step :  145\n",
      "reward :  0.5060668687266728\n",
      "Episode 12, Total Reward: 12.870555074865472\n",
      "episode :  13\n",
      "current step :  0\n",
      "reward :  0.3749541071041031\n",
      "episode :  13\n",
      "current step :  5\n",
      "reward :  0.44322668230733553\n",
      "episode :  13\n",
      "current step :  10\n",
      "reward :  0.4919806477850672\n",
      "episode :  13\n",
      "current step :  15\n",
      "reward :  0.46235774417045233\n",
      "episode :  13\n",
      "current step :  20\n",
      "reward :  0.4102069192769981\n",
      "episode :  13\n",
      "current step :  25\n",
      "reward :  0.40489810379360897\n",
      "episode :  13\n",
      "current step :  30\n",
      "reward :  0.4988679591127618\n",
      "episode :  13\n",
      "current step :  35\n",
      "reward :  0.45279661740316013\n",
      "episode :  13\n",
      "current step :  40\n",
      "reward :  0.3126449042651818\n",
      "episode :  13\n",
      "current step :  45\n",
      "reward :  0.4123635530841022\n",
      "episode :  13\n",
      "current step :  50\n",
      "reward :  0.41469344435497063\n",
      "episode :  13\n",
      "current step :  55\n",
      "reward :  0.4422566228275041\n",
      "episode :  13\n",
      "current step :  60\n",
      "reward :  0.16738393231154133\n",
      "episode :  13\n",
      "current step :  65\n",
      "reward :  0.17712168751517132\n",
      "episode :  13\n",
      "current step :  70\n",
      "reward :  0.24904285175608498\n",
      "episode :  13\n",
      "current step :  75\n",
      "reward :  0.5955944256088178\n",
      "episode :  13\n",
      "current step :  80\n",
      "reward :  0.5660923575108043\n",
      "episode :  13\n",
      "current step :  85\n",
      "reward :  0.48475615569210406\n",
      "episode :  13\n",
      "current step :  90\n",
      "reward :  0.4813143418224454\n",
      "episode :  13\n",
      "current step :  95\n",
      "reward :  0.39177379953246594\n",
      "episode :  13\n",
      "current step :  100\n",
      "reward :  0.44792260411884055\n",
      "episode :  13\n",
      "current step :  105\n",
      "reward :  0.41608611553286545\n",
      "episode :  13\n",
      "current step :  110\n",
      "reward :  0.46475138026603446\n",
      "episode :  13\n",
      "current step :  115\n",
      "reward :  0.44001161753907725\n",
      "episode :  13\n",
      "current step :  120\n",
      "reward :  0.4339010129075691\n",
      "episode :  13\n",
      "current step :  125\n",
      "reward :  0.4086668676218498\n",
      "episode :  13\n",
      "current step :  130\n",
      "reward :  0.3999851078406601\n",
      "episode :  13\n",
      "current step :  135\n",
      "reward :  0.4701706876933088\n",
      "episode :  13\n",
      "current step :  140\n",
      "reward :  0.327703462055455\n",
      "episode :  13\n",
      "current step :  145\n",
      "reward :  0.48993324625560475\n",
      "Episode 13, Total Reward: 12.533458959065944\n",
      "episode :  14\n",
      "current step :  0\n",
      "reward :  0.2427956983289164\n",
      "episode :  14\n",
      "current step :  5\n",
      "reward :  0.4676850829437253\n",
      "episode :  14\n",
      "current step :  10\n",
      "reward :  0.5097173500005396\n",
      "episode :  14\n",
      "current step :  15\n",
      "reward :  0.4327161289747087\n",
      "episode :  14\n",
      "current step :  20\n",
      "reward :  0.4764502834602068\n",
      "episode :  14\n",
      "current step :  25\n",
      "reward :  0.4218545857783598\n",
      "episode :  14\n",
      "current step :  30\n",
      "reward :  0.5323871827811266\n",
      "episode :  14\n",
      "current step :  35\n",
      "reward :  0.34566771394879403\n",
      "episode :  14\n",
      "current step :  40\n",
      "reward :  0.48266871239526354\n",
      "episode :  14\n",
      "current step :  45\n",
      "reward :  0.30277169981765395\n",
      "episode :  14\n",
      "current step :  50\n",
      "reward :  0.2314327971890558\n",
      "episode :  14\n",
      "current step :  55\n",
      "reward :  0.2492478426995597\n",
      "episode :  14\n",
      "current step :  60\n",
      "reward :  0.3143931863683787\n",
      "episode :  14\n",
      "current step :  65\n",
      "reward :  0.20101274521820484\n",
      "episode :  14\n",
      "current step :  70\n",
      "reward :  0.35618089386128615\n",
      "episode :  14\n",
      "current step :  75\n",
      "reward :  0.40729760052841\n",
      "episode :  14\n",
      "current step :  80\n",
      "reward :  0.28486556065534197\n",
      "episode :  14\n",
      "current step :  85\n",
      "reward :  0.4196608553355119\n",
      "episode :  14\n",
      "current step :  90\n",
      "reward :  0.4306412402820695\n",
      "episode :  14\n",
      "current step :  95\n",
      "reward :  0.39429622295593403\n",
      "episode :  14\n",
      "current step :  100\n",
      "reward :  0.3905255061681867\n",
      "episode :  14\n",
      "current step :  105\n",
      "reward :  0.45130133257037264\n",
      "episode :  14\n",
      "current step :  110\n",
      "reward :  0.3991993420126979\n",
      "episode :  14\n",
      "current step :  115\n",
      "reward :  0.428829568708859\n",
      "episode :  14\n",
      "current step :  120\n",
      "reward :  0.22701452351718224\n",
      "episode :  14\n",
      "current step :  125\n",
      "reward :  0.525405588430098\n",
      "episode :  14\n",
      "current step :  130\n",
      "reward :  0.4653374880764685\n",
      "episode :  14\n",
      "current step :  135\n",
      "reward :  0.4737901035765521\n",
      "episode :  14\n",
      "current step :  140\n",
      "reward :  0.5571074776981152\n",
      "episode :  14\n",
      "current step :  145\n",
      "reward :  0.1865472411065551\n",
      "Episode 14, Total Reward: 11.608801555388135\n",
      "episode :  15\n",
      "current step :  0\n",
      "reward :  0.37418549064494155\n",
      "episode :  15\n",
      "current step :  5\n",
      "reward :  0.4240297798237796\n",
      "episode :  15\n",
      "current step :  10\n",
      "reward :  0.5141038748266847\n",
      "episode :  15\n",
      "current step :  15\n",
      "reward :  0.46290334883820566\n",
      "episode :  15\n",
      "current step :  20\n",
      "reward :  0.4222614352184517\n",
      "episode :  15\n",
      "current step :  25\n",
      "reward :  0.42359809442502805\n",
      "episode :  15\n",
      "current step :  30\n",
      "reward :  0.24624417037293064\n",
      "episode :  15\n",
      "current step :  35\n",
      "reward :  0.34347634090040935\n",
      "episode :  15\n",
      "current step :  40\n",
      "reward :  0.28292887471047007\n",
      "episode :  15\n",
      "current step :  45\n",
      "reward :  0.4552310088946683\n",
      "episode :  15\n",
      "current step :  50\n",
      "reward :  0.22139527635003778\n",
      "episode :  15\n",
      "current step :  55\n",
      "reward :  0.2691329398925113\n",
      "episode :  15\n",
      "current step :  60\n",
      "reward :  0.0\n",
      "episode :  15\n",
      "current step :  65\n",
      "reward :  0.43402594207419865\n",
      "episode :  15\n",
      "current step :  70\n",
      "reward :  0.44772960468673273\n",
      "episode :  15\n",
      "current step :  75\n",
      "reward :  0.39373396241903225\n",
      "episode :  15\n",
      "current step :  80\n",
      "reward :  0.5400023970299733\n",
      "episode :  15\n",
      "current step :  85\n",
      "reward :  0.41567089018757486\n",
      "episode :  15\n",
      "current step :  90\n",
      "reward :  0.3792783621012681\n",
      "episode :  15\n",
      "current step :  95\n",
      "reward :  0.4982898301377865\n",
      "episode :  15\n",
      "current step :  100\n",
      "reward :  0.45823404490911357\n",
      "episode :  15\n",
      "current step :  105\n",
      "reward :  0.407753627654335\n",
      "episode :  15\n",
      "current step :  110\n",
      "reward :  0.47785989586224786\n",
      "episode :  15\n",
      "current step :  115\n",
      "reward :  0.22860483511849694\n",
      "episode :  15\n",
      "current step :  120\n",
      "reward :  0.40090328777358997\n",
      "episode :  15\n",
      "current step :  125\n",
      "reward :  0.44613734383354625\n",
      "episode :  15\n",
      "current step :  130\n",
      "reward :  0.5279719964487604\n",
      "episode :  15\n",
      "current step :  135\n",
      "reward :  0.4104183773358818\n",
      "episode :  15\n",
      "current step :  140\n",
      "reward :  0.5431902382046536\n",
      "episode :  15\n",
      "current step :  145\n",
      "reward :  0.42191100879595633\n",
      "Episode 15, Total Reward: 11.871206279471266\n",
      "episode :  16\n",
      "current step :  0\n",
      "reward :  0.3771830355476427\n",
      "episode :  16\n",
      "current step :  5\n",
      "reward :  0.4329099818025046\n",
      "episode :  16\n",
      "current step :  10\n",
      "reward :  0.4050258848986511\n",
      "episode :  16\n",
      "current step :  15\n",
      "reward :  0.4193506178478471\n",
      "episode :  16\n",
      "current step :  20\n",
      "reward :  0.47904131137923944\n",
      "episode :  16\n",
      "current step :  25\n",
      "reward :  0.2589836769491889\n",
      "episode :  16\n",
      "current step :  30\n",
      "reward :  0.6006947734377456\n",
      "episode :  16\n",
      "current step :  35\n",
      "reward :  0.46802278870339786\n",
      "episode :  16\n",
      "current step :  40\n",
      "reward :  0.33126418250923956\n",
      "episode :  16\n",
      "current step :  45\n",
      "reward :  0.5639582580351663\n",
      "episode :  16\n",
      "current step :  50\n",
      "reward :  0.43173733266746467\n",
      "episode :  16\n",
      "current step :  55\n",
      "reward :  0.2947823122040214\n",
      "episode :  16\n",
      "current step :  60\n",
      "reward :  0.3423047574920758\n",
      "episode :  16\n",
      "current step :  65\n",
      "reward :  0.4451829398172855\n",
      "episode :  16\n",
      "current step :  70\n",
      "reward :  0.2405039055734476\n",
      "episode :  16\n",
      "current step :  75\n",
      "reward :  0.4154020017672248\n",
      "episode :  16\n",
      "current step :  80\n",
      "reward :  0.38317592526531613\n",
      "episode :  16\n",
      "current step :  85\n",
      "reward :  0.48515022779181005\n",
      "episode :  16\n",
      "current step :  90\n",
      "reward :  0.4687518508844456\n",
      "episode :  16\n",
      "current step :  95\n",
      "reward :  0.43821602775898316\n",
      "episode :  16\n",
      "current step :  100\n",
      "reward :  0.5033672280131423\n",
      "episode :  16\n",
      "current step :  105\n",
      "reward :  0.4131938431137422\n",
      "episode :  16\n",
      "current step :  110\n",
      "reward :  0.45324410702689993\n",
      "episode :  16\n",
      "current step :  115\n",
      "reward :  0.24649498266576453\n",
      "episode :  16\n",
      "current step :  120\n",
      "reward :  0.4076504877238537\n",
      "episode :  16\n",
      "current step :  125\n",
      "reward :  0.3160851853126176\n",
      "episode :  16\n",
      "current step :  130\n",
      "reward :  0.4380918377867896\n",
      "episode :  16\n",
      "current step :  135\n",
      "reward :  0.5385750685296942\n",
      "episode :  16\n",
      "current step :  140\n",
      "reward :  0.4442488337883427\n",
      "episode :  16\n",
      "current step :  145\n",
      "reward :  0.48118521341155174\n",
      "Episode 16, Total Reward: 12.523778579705095\n",
      "episode :  17\n",
      "current step :  0\n",
      "reward :  0.4676763122440307\n",
      "episode :  17\n",
      "current step :  5\n",
      "reward :  0.4359059467404425\n",
      "episode :  17\n",
      "current step :  10\n",
      "reward :  0.3844003516329882\n",
      "episode :  17\n",
      "current step :  15\n",
      "reward :  0.3651788831979671\n",
      "episode :  17\n",
      "current step :  20\n",
      "reward :  0.4847449829098605\n",
      "episode :  17\n",
      "current step :  25\n",
      "reward :  0.4603190498743141\n",
      "episode :  17\n",
      "current step :  30\n",
      "reward :  0.36778999634341086\n",
      "episode :  17\n",
      "current step :  35\n",
      "reward :  0.3088090506694222\n",
      "episode :  17\n",
      "current step :  40\n",
      "reward :  0.30336152381179204\n",
      "episode :  17\n",
      "current step :  45\n",
      "reward :  0.40927946271125853\n",
      "episode :  17\n",
      "current step :  50\n",
      "reward :  0.22450813162652328\n",
      "episode :  17\n",
      "current step :  55\n",
      "reward :  0.521104290055998\n",
      "episode :  17\n",
      "current step :  60\n",
      "reward :  0.19515181084184863\n",
      "episode :  17\n",
      "current step :  65\n",
      "reward :  0.400449093209572\n",
      "episode :  17\n",
      "current step :  70\n",
      "reward :  0.4851731180814245\n",
      "episode :  17\n",
      "current step :  75\n",
      "reward :  0.42905706905731994\n",
      "episode :  17\n",
      "current step :  80\n",
      "reward :  0.47857473357319036\n",
      "episode :  17\n",
      "current step :  85\n",
      "reward :  0.43616327781429776\n",
      "episode :  17\n",
      "current step :  90\n",
      "reward :  0.35450592739737086\n",
      "episode :  17\n",
      "current step :  95\n",
      "reward :  0.21037523194867494\n",
      "episode :  17\n",
      "current step :  100\n",
      "reward :  0.4041863199797918\n",
      "episode :  17\n",
      "current step :  105\n",
      "reward :  0.4100048200406773\n",
      "episode :  17\n",
      "current step :  110\n",
      "reward :  0.4596034598471984\n",
      "episode :  17\n",
      "current step :  115\n",
      "reward :  0.46878275296788613\n",
      "episode :  17\n",
      "current step :  120\n",
      "reward :  0.33318953883827584\n",
      "episode :  17\n",
      "current step :  125\n",
      "reward :  0.480241176072261\n",
      "episode :  17\n",
      "current step :  130\n",
      "reward :  0.44715376182683786\n",
      "episode :  17\n",
      "current step :  135\n",
      "reward :  0.4628634386313579\n",
      "episode :  17\n",
      "current step :  140\n",
      "reward :  0.4109115106567063\n",
      "episode :  17\n",
      "current step :  145\n",
      "reward :  0.4370653122460974\n",
      "Episode 17, Total Reward: 12.036530334848798\n",
      "episode :  18\n",
      "current step :  0\n",
      "reward :  0.496371971446375\n",
      "episode :  18\n",
      "current step :  5\n",
      "reward :  0.5611251833732407\n",
      "episode :  18\n",
      "current step :  10\n",
      "reward :  0.35485692619052644\n",
      "episode :  18\n",
      "current step :  15\n",
      "reward :  0.5261213855605846\n",
      "episode :  18\n",
      "current step :  20\n",
      "reward :  0.3944727375178557\n",
      "episode :  18\n",
      "current step :  25\n",
      "reward :  0.35464319354366386\n",
      "episode :  18\n",
      "current step :  30\n",
      "reward :  0.34700699203800944\n",
      "episode :  18\n",
      "current step :  35\n",
      "reward :  0.5616320377564291\n",
      "episode :  18\n",
      "current step :  40\n",
      "reward :  0.3786103469647456\n",
      "episode :  18\n",
      "current step :  45\n",
      "reward :  0.35433442712415364\n",
      "episode :  18\n",
      "current step :  50\n",
      "reward :  0.4803811423978467\n",
      "episode :  18\n",
      "current step :  55\n",
      "reward :  0.46896615688924154\n",
      "episode :  18\n",
      "current step :  60\n",
      "reward :  0.19817882037183376\n",
      "episode :  18\n",
      "current step :  65\n",
      "reward :  0.1913103263301084\n",
      "episode :  18\n",
      "current step :  70\n",
      "reward :  0.4425503259717748\n",
      "episode :  18\n",
      "current step :  75\n",
      "reward :  0.3293230679726112\n",
      "episode :  18\n",
      "current step :  80\n",
      "reward :  0.4344154719803107\n",
      "episode :  18\n",
      "current step :  85\n",
      "reward :  0.5117982588266936\n",
      "episode :  18\n",
      "current step :  90\n",
      "reward :  0.45812191265858526\n",
      "episode :  18\n",
      "current step :  95\n",
      "reward :  0.3919587206977605\n",
      "episode :  18\n",
      "current step :  100\n",
      "reward :  0.5408494618111132\n",
      "episode :  18\n",
      "current step :  105\n",
      "reward :  0.3724225253100445\n",
      "episode :  18\n",
      "current step :  110\n",
      "reward :  0.41112733451360906\n",
      "episode :  18\n",
      "current step :  115\n",
      "reward :  0.533661762734035\n",
      "episode :  18\n",
      "current step :  120\n",
      "reward :  0.4123009658469459\n",
      "episode :  18\n",
      "current step :  125\n",
      "reward :  0.374164488127793\n",
      "episode :  18\n",
      "current step :  130\n",
      "reward :  0.5491363581397224\n",
      "episode :  18\n",
      "current step :  135\n",
      "reward :  0.6460966737225307\n",
      "episode :  18\n",
      "current step :  140\n",
      "reward :  0.4527190573225583\n",
      "episode :  18\n",
      "current step :  145\n",
      "reward :  0.3474872044287143\n",
      "Episode 18, Total Reward: 12.876145237569414\n",
      "episode :  19\n",
      "current step :  0\n",
      "reward :  0.38009138548153965\n",
      "episode :  19\n",
      "current step :  5\n",
      "reward :  0.5051571920304607\n",
      "episode :  19\n",
      "current step :  10\n",
      "reward :  0.5021363890962467\n",
      "episode :  19\n",
      "current step :  15\n",
      "reward :  0.5571671094605015\n",
      "episode :  19\n",
      "current step :  20\n",
      "reward :  0.5368023588573163\n",
      "episode :  19\n",
      "current step :  25\n",
      "reward :  0.2853040438241413\n",
      "episode :  19\n",
      "current step :  30\n",
      "reward :  0.3498584900079258\n",
      "episode :  19\n",
      "current step :  35\n",
      "reward :  0.24691626596142355\n",
      "episode :  19\n",
      "current step :  40\n",
      "reward :  0.2636375736689244\n",
      "episode :  19\n",
      "current step :  45\n",
      "reward :  0.48238714657847487\n",
      "episode :  19\n",
      "current step :  50\n",
      "reward :  0.2593746868506365\n",
      "episode :  19\n",
      "current step :  55\n",
      "reward :  0.46456814939338315\n",
      "episode :  19\n",
      "current step :  60\n",
      "reward :  0.20442667283731564\n",
      "episode :  19\n",
      "current step :  65\n",
      "reward :  0.21647983125295636\n",
      "episode :  19\n",
      "current step :  70\n",
      "reward :  0.3611283071174409\n",
      "episode :  19\n",
      "current step :  75\n",
      "reward :  0.311320038944613\n",
      "episode :  19\n",
      "current step :  80\n",
      "reward :  0.3412214680404419\n",
      "episode :  19\n",
      "current step :  85\n",
      "reward :  0.4679503127350293\n",
      "episode :  19\n",
      "current step :  90\n",
      "reward :  0.40067686852092976\n",
      "episode :  19\n",
      "current step :  95\n",
      "reward :  0.495805588417389\n",
      "episode :  19\n",
      "current step :  100\n",
      "reward :  0.4899320128354512\n",
      "episode :  19\n",
      "current step :  105\n",
      "reward :  0.5236811091126489\n",
      "episode :  19\n",
      "current step :  110\n",
      "reward :  0.4089550789773216\n",
      "episode :  19\n",
      "current step :  115\n",
      "reward :  0.3602493139653211\n",
      "episode :  19\n",
      "current step :  120\n",
      "reward :  0.37179613631133235\n",
      "episode :  19\n",
      "current step :  125\n",
      "reward :  0.4444715806651474\n",
      "episode :  19\n",
      "current step :  130\n",
      "reward :  0.48114779347388437\n",
      "episode :  19\n",
      "current step :  135\n",
      "reward :  0.46543259563562495\n",
      "episode :  19\n",
      "current step :  140\n",
      "reward :  0.3819817718409189\n",
      "episode :  19\n",
      "current step :  145\n",
      "reward :  0.2549886179471618\n",
      "Episode 19, Total Reward: 11.815045889841901\n",
      "episode :  20\n",
      "current step :  0\n",
      "reward :  0.5147484213710198\n",
      "episode :  20\n",
      "current step :  5\n",
      "reward :  0.38021485110651804\n",
      "episode :  20\n",
      "current step :  10\n",
      "reward :  0.4065706199785426\n",
      "episode :  20\n",
      "current step :  15\n",
      "reward :  0.35690610178834004\n",
      "episode :  20\n",
      "current step :  20\n",
      "reward :  0.376975490365699\n",
      "episode :  20\n",
      "current step :  25\n",
      "reward :  0.36122448327139134\n",
      "episode :  20\n",
      "current step :  30\n",
      "reward :  0.6008342280830636\n",
      "episode :  20\n",
      "current step :  35\n",
      "reward :  0.45861945358114425\n",
      "episode :  20\n",
      "current step :  40\n",
      "reward :  0.27262893250618114\n",
      "episode :  20\n",
      "current step :  45\n",
      "reward :  0.2622315888912233\n",
      "episode :  20\n",
      "current step :  50\n",
      "reward :  0.48037743808186123\n",
      "episode :  20\n",
      "current step :  55\n",
      "reward :  0.4212349902617184\n",
      "episode :  20\n",
      "current step :  60\n",
      "reward :  0.23270786951396577\n",
      "episode :  20\n",
      "current step :  65\n",
      "reward :  0.4758958999865338\n",
      "episode :  20\n",
      "current step :  70\n",
      "reward :  0.3726585721860786\n",
      "episode :  20\n",
      "current step :  75\n",
      "reward :  0.42529885370259657\n",
      "episode :  20\n",
      "current step :  80\n",
      "reward :  0.4791553578907657\n",
      "episode :  20\n",
      "current step :  85\n",
      "reward :  0.22847882346587295\n",
      "episode :  20\n",
      "current step :  90\n",
      "reward :  0.4405231727383394\n",
      "episode :  20\n",
      "current step :  95\n",
      "reward :  0.5399253157294527\n",
      "episode :  20\n",
      "current step :  100\n",
      "reward :  0.46816131268610994\n",
      "episode :  20\n",
      "current step :  105\n",
      "reward :  0.3959222529750186\n",
      "episode :  20\n",
      "current step :  110\n",
      "reward :  0.41541752511279484\n",
      "episode :  20\n",
      "current step :  115\n",
      "reward :  0.2438111565874691\n",
      "episode :  20\n",
      "current step :  120\n",
      "reward :  0.41314016623347316\n",
      "episode :  20\n",
      "current step :  125\n",
      "reward :  0.47187239130261927\n",
      "episode :  20\n",
      "current step :  130\n",
      "reward :  0.4921105152925735\n",
      "episode :  20\n",
      "current step :  135\n",
      "reward :  0.45475423832745065\n",
      "episode :  20\n",
      "current step :  140\n",
      "reward :  0.45882832446615684\n",
      "episode :  20\n",
      "current step :  145\n",
      "reward :  0.389598160881913\n",
      "Episode 20, Total Reward: 12.290826508365893\n",
      "episode :  21\n",
      "current step :  0\n",
      "reward :  0.3872900979366398\n",
      "episode :  21\n",
      "current step :  5\n",
      "reward :  0.49565152891211145\n",
      "episode :  21\n",
      "current step :  10\n",
      "reward :  0.4136681785724503\n",
      "episode :  21\n",
      "current step :  15\n",
      "reward :  0.40952536141335305\n",
      "episode :  21\n",
      "current step :  20\n",
      "reward :  0.5201705080930545\n",
      "episode :  21\n",
      "current step :  25\n",
      "reward :  0.4201551096155196\n",
      "episode :  21\n",
      "current step :  30\n",
      "reward :  0.5472890397117369\n",
      "episode :  21\n",
      "current step :  35\n",
      "reward :  0.30032911958736136\n",
      "episode :  21\n",
      "current step :  40\n",
      "reward :  0.29703220117266643\n",
      "episode :  21\n",
      "current step :  45\n",
      "reward :  0.30493711559927067\n",
      "episode :  21\n",
      "current step :  50\n",
      "reward :  0.3976215136216582\n",
      "episode :  21\n",
      "current step :  55\n",
      "reward :  0.23308824965338018\n",
      "episode :  21\n",
      "current step :  60\n",
      "reward :  0.3524036023022341\n",
      "episode :  21\n",
      "current step :  65\n",
      "reward :  0.20738194977438185\n",
      "episode :  21\n",
      "current step :  70\n",
      "reward :  0.3046681830289609\n",
      "episode :  21\n",
      "current step :  75\n",
      "reward :  0.4179583925601782\n",
      "episode :  21\n",
      "current step :  80\n",
      "reward :  0.2925759835993311\n",
      "episode :  21\n",
      "current step :  85\n",
      "reward :  0.4299997295485426\n",
      "episode :  21\n",
      "current step :  90\n",
      "reward :  0.5093319890080472\n",
      "episode :  21\n",
      "current step :  95\n",
      "reward :  0.3925294693735961\n",
      "episode :  21\n",
      "current step :  100\n",
      "reward :  0.5081705522867739\n",
      "episode :  21\n",
      "current step :  105\n",
      "reward :  0.37712504422732607\n",
      "episode :  21\n",
      "current step :  110\n",
      "reward :  0.394962955160411\n",
      "episode :  21\n",
      "current step :  115\n",
      "reward :  0.440160613501676\n",
      "episode :  21\n",
      "current step :  120\n",
      "reward :  0.4391836909782464\n",
      "episode :  21\n",
      "current step :  125\n",
      "reward :  0.3608770861734598\n",
      "episode :  21\n",
      "current step :  130\n",
      "reward :  0.4144948088205976\n",
      "episode :  21\n",
      "current step :  135\n",
      "reward :  0.42344488738035396\n",
      "episode :  21\n",
      "current step :  140\n",
      "reward :  0.47628829823918806\n",
      "episode :  21\n",
      "current step :  145\n",
      "reward :  0.4279791209954442\n",
      "Episode 21, Total Reward: 11.896294380847952\n",
      "episode :  22\n",
      "current step :  0\n",
      "reward :  0.44837088941742365\n",
      "episode :  22\n",
      "current step :  5\n",
      "reward :  0.38955610380684313\n",
      "episode :  22\n",
      "current step :  10\n",
      "reward :  0.48509940859356304\n",
      "episode :  22\n",
      "current step :  15\n",
      "reward :  0.4755692202437686\n",
      "episode :  22\n",
      "current step :  20\n",
      "reward :  0.4038070714127139\n",
      "episode :  22\n",
      "current step :  25\n",
      "reward :  0.45544661759437666\n",
      "episode :  22\n",
      "current step :  30\n",
      "reward :  0.4636356391746266\n",
      "episode :  22\n",
      "current step :  35\n",
      "reward :  0.19239724135724456\n",
      "episode :  22\n",
      "current step :  40\n",
      "reward :  0.31733068701001577\n",
      "episode :  22\n",
      "current step :  45\n",
      "reward :  0.24703122843434425\n",
      "episode :  22\n",
      "current step :  50\n",
      "reward :  0.4729416120114116\n",
      "episode :  22\n",
      "current step :  55\n",
      "reward :  0.4386679454495923\n",
      "episode :  22\n",
      "current step :  60\n",
      "reward :  0.2395142847422327\n",
      "episode :  22\n",
      "current step :  65\n",
      "reward :  0.20375211362031145\n",
      "episode :  22\n",
      "current step :  70\n",
      "reward :  0.389958217048059\n",
      "episode :  22\n",
      "current step :  75\n",
      "reward :  0.6253438003919778\n",
      "episode :  22\n",
      "current step :  80\n",
      "reward :  0.44825059948676854\n",
      "episode :  22\n",
      "current step :  85\n",
      "reward :  0.47776385080063943\n",
      "episode :  22\n",
      "current step :  90\n",
      "reward :  0.44288785465272756\n",
      "episode :  22\n",
      "current step :  95\n",
      "reward :  0.35584405861874824\n",
      "episode :  22\n",
      "current step :  100\n",
      "reward :  0.3945229916546161\n",
      "episode :  22\n",
      "current step :  105\n",
      "reward :  0.37174844543511076\n",
      "episode :  22\n",
      "current step :  110\n",
      "reward :  0.503234728875204\n",
      "episode :  22\n",
      "current step :  115\n",
      "reward :  0.42804447090163456\n",
      "episode :  22\n",
      "current step :  120\n",
      "reward :  0.3424888041009018\n",
      "episode :  22\n",
      "current step :  125\n",
      "reward :  0.4383289783528398\n",
      "episode :  22\n",
      "current step :  130\n",
      "reward :  0.4556704841543105\n",
      "episode :  22\n",
      "current step :  135\n",
      "reward :  0.4777198343135221\n",
      "episode :  22\n",
      "current step :  140\n",
      "reward :  0.47315534356581135\n",
      "episode :  22\n",
      "current step :  145\n",
      "reward :  0.46599064897517406\n",
      "Episode 22, Total Reward: 12.324073174196513\n",
      "episode :  23\n",
      "current step :  0\n",
      "reward :  0.5721837583107496\n",
      "episode :  23\n",
      "current step :  5\n",
      "reward :  0.38082087220807687\n",
      "episode :  23\n",
      "current step :  10\n",
      "reward :  0.4371894800506415\n",
      "episode :  23\n",
      "current step :  15\n",
      "reward :  0.44145467524162885\n",
      "episode :  23\n",
      "current step :  20\n",
      "reward :  0.4428673356090379\n",
      "episode :  23\n",
      "current step :  25\n",
      "reward :  0.4071399873581199\n",
      "episode :  23\n",
      "current step :  30\n",
      "reward :  0.4766646732087304\n",
      "episode :  23\n",
      "current step :  35\n",
      "reward :  0.2393030571572791\n",
      "episode :  23\n",
      "current step :  40\n",
      "reward :  0.47849383314187777\n",
      "episode :  23\n",
      "current step :  45\n",
      "reward :  0.47463515224876235\n",
      "episode :  23\n",
      "current step :  50\n",
      "reward :  0.25538718931187965\n",
      "episode :  23\n",
      "current step :  55\n",
      "reward :  0.19332679190055296\n",
      "episode :  23\n",
      "current step :  60\n",
      "reward :  0.1928479249829551\n",
      "episode :  23\n",
      "current step :  65\n",
      "reward :  0.23417186011620433\n",
      "episode :  23\n",
      "current step :  70\n",
      "reward :  0.310818574707229\n",
      "episode :  23\n",
      "current step :  75\n",
      "reward :  0.30070245620725905\n",
      "episode :  23\n",
      "current step :  80\n",
      "reward :  0.5720519370447921\n",
      "episode :  23\n",
      "current step :  85\n",
      "reward :  0.33027082262090834\n",
      "episode :  23\n",
      "current step :  90\n",
      "reward :  0.49674704888501076\n",
      "episode :  23\n",
      "current step :  95\n",
      "reward :  0.40667445556616205\n",
      "episode :  23\n",
      "current step :  100\n",
      "reward :  0.4220419010443033\n",
      "episode :  23\n",
      "current step :  105\n",
      "reward :  0.43793903297850023\n",
      "episode :  23\n",
      "current step :  110\n",
      "reward :  0.4604625909766875\n",
      "episode :  23\n",
      "current step :  115\n",
      "reward :  0.29521092777746927\n",
      "episode :  23\n",
      "current step :  120\n",
      "reward :  0.3735519848249676\n",
      "episode :  23\n",
      "current step :  125\n",
      "reward :  0.47540237195473717\n",
      "episode :  23\n",
      "current step :  130\n",
      "reward :  0.4134015106581168\n",
      "episode :  23\n",
      "current step :  135\n",
      "reward :  0.4191394381463545\n",
      "episode :  23\n",
      "current step :  140\n",
      "reward :  0.5251216691261786\n",
      "episode :  23\n",
      "current step :  145\n",
      "reward :  0.4608813380418523\n",
      "Episode 23, Total Reward: 11.926904651407026\n",
      "episode :  24\n",
      "current step :  0\n",
      "reward :  0.4574626365215003\n",
      "episode :  24\n",
      "current step :  5\n",
      "reward :  0.4720783382885713\n",
      "episode :  24\n",
      "current step :  10\n",
      "reward :  0.5201219544263094\n",
      "episode :  24\n",
      "current step :  15\n",
      "reward :  0.4915155912699839\n",
      "episode :  24\n",
      "current step :  20\n",
      "reward :  0.3709338336542889\n",
      "episode :  24\n",
      "current step :  25\n",
      "reward :  0.4077502767276384\n",
      "episode :  24\n",
      "current step :  30\n",
      "reward :  0.46768653712104724\n",
      "episode :  24\n",
      "current step :  35\n",
      "reward :  0.4666700597383774\n",
      "episode :  24\n",
      "current step :  40\n",
      "reward :  0.26411614861638927\n",
      "episode :  24\n",
      "current step :  45\n",
      "reward :  0.49301110906826195\n",
      "episode :  24\n",
      "current step :  50\n",
      "reward :  0.47870981048884986\n",
      "episode :  24\n",
      "current step :  55\n",
      "reward :  0.37942421437062956\n",
      "episode :  24\n",
      "current step :  60\n",
      "reward :  0.0\n",
      "episode :  24\n",
      "current step :  65\n",
      "reward :  0.21680904268555598\n",
      "episode :  24\n",
      "current step :  70\n",
      "reward :  0.2532076039097561\n",
      "episode :  24\n",
      "current step :  75\n",
      "reward :  0.3851880975626255\n",
      "episode :  24\n",
      "current step :  80\n",
      "reward :  0.4316410312123499\n",
      "episode :  24\n",
      "current step :  85\n",
      "reward :  0.6926304575924915\n",
      "episode :  24\n",
      "current step :  90\n",
      "reward :  0.5190624655550735\n",
      "episode :  24\n",
      "current step :  95\n",
      "reward :  0.4544685836098037\n",
      "episode :  24\n",
      "current step :  100\n",
      "reward :  0.4166688363337157\n",
      "episode :  24\n",
      "current step :  105\n",
      "reward :  0.374221691741852\n",
      "episode :  24\n",
      "current step :  110\n",
      "reward :  0.3976155394748011\n",
      "episode :  24\n",
      "current step :  115\n",
      "reward :  0.2371451012142961\n",
      "episode :  24\n",
      "current step :  120\n",
      "reward :  0.45040480969283003\n",
      "episode :  24\n",
      "current step :  125\n",
      "reward :  0.45977130277898337\n",
      "episode :  24\n",
      "current step :  130\n",
      "reward :  0.48025457843228714\n",
      "episode :  24\n",
      "current step :  135\n",
      "reward :  0.4062739507694122\n",
      "episode :  24\n",
      "current step :  140\n",
      "reward :  0.49682479880433283\n",
      "episode :  24\n",
      "current step :  145\n",
      "reward :  0.392404336745725\n",
      "Episode 24, Total Reward: 12.334072738407741\n",
      "episode :  25\n",
      "current step :  0\n",
      "reward :  0.4834078316378104\n",
      "episode :  25\n",
      "current step :  5\n",
      "reward :  0.49645401588630117\n",
      "episode :  25\n",
      "current step :  10\n",
      "reward :  0.44890807327008164\n",
      "episode :  25\n",
      "current step :  15\n",
      "reward :  0.34323147809057214\n",
      "episode :  25\n",
      "current step :  20\n",
      "reward :  0.4855656430635424\n",
      "episode :  25\n",
      "current step :  25\n",
      "reward :  0.4478488825434174\n",
      "episode :  25\n",
      "current step :  30\n",
      "reward :  0.48554731993989886\n",
      "episode :  25\n",
      "current step :  35\n",
      "reward :  0.24537390581451105\n",
      "episode :  25\n",
      "current step :  40\n",
      "reward :  0.21986222885214715\n",
      "episode :  25\n",
      "current step :  45\n",
      "reward :  0.5076915931395976\n",
      "episode :  25\n",
      "current step :  50\n",
      "reward :  0.28136305353487273\n",
      "episode :  25\n",
      "current step :  55\n",
      "reward :  0.40993800197182095\n",
      "episode :  25\n",
      "current step :  60\n",
      "reward :  0.2693316464898387\n",
      "episode :  25\n",
      "current step :  65\n",
      "reward :  0.26628743926299175\n",
      "episode :  25\n",
      "current step :  70\n",
      "reward :  0.4930557605134588\n",
      "episode :  25\n",
      "current step :  75\n",
      "reward :  0.3900109768944294\n",
      "episode :  25\n",
      "current step :  80\n",
      "reward :  0.4728351841714322\n",
      "episode :  25\n",
      "current step :  85\n",
      "reward :  0.35812550556146905\n",
      "episode :  25\n",
      "current step :  90\n",
      "reward :  0.4452695025038516\n",
      "episode :  25\n",
      "current step :  95\n",
      "reward :  0.37137303018201084\n",
      "episode :  25\n",
      "current step :  100\n",
      "reward :  0.26395921545945705\n",
      "episode :  25\n",
      "current step :  105\n",
      "reward :  0.4492329671934132\n",
      "episode :  25\n",
      "current step :  110\n",
      "reward :  0.43347911003129735\n",
      "episode :  25\n",
      "current step :  115\n",
      "reward :  0.38430851302169206\n",
      "episode :  25\n",
      "current step :  120\n",
      "reward :  0.4850743760046612\n",
      "episode :  25\n",
      "current step :  125\n",
      "reward :  0.4545280557318223\n",
      "episode :  25\n",
      "current step :  130\n",
      "reward :  0.44501070115004115\n",
      "episode :  25\n",
      "current step :  135\n",
      "reward :  0.3855017920647333\n",
      "episode :  25\n",
      "current step :  140\n",
      "reward :  0.5004934779606296\n",
      "episode :  25\n",
      "current step :  145\n",
      "reward :  0.481365918216781\n",
      "Episode 25, Total Reward: 12.204435200158585\n",
      "episode :  26\n",
      "current step :  0\n",
      "reward :  0.40723557706631236\n",
      "episode :  26\n",
      "current step :  5\n",
      "reward :  0.4267389149385197\n",
      "episode :  26\n",
      "current step :  10\n",
      "reward :  0.3714842407783489\n",
      "episode :  26\n",
      "current step :  15\n",
      "reward :  0.4263966760951438\n",
      "episode :  26\n",
      "current step :  20\n",
      "reward :  0.5037222571407092\n",
      "episode :  26\n",
      "current step :  25\n",
      "reward :  0.4670555495626831\n",
      "episode :  26\n",
      "current step :  30\n",
      "reward :  0.4598393500980307\n",
      "episode :  26\n",
      "current step :  35\n",
      "reward :  0.4794659091966974\n",
      "episode :  26\n",
      "current step :  40\n",
      "reward :  0.398101653740367\n",
      "episode :  26\n",
      "current step :  45\n",
      "reward :  0.44223199341575864\n",
      "episode :  26\n",
      "current step :  50\n",
      "reward :  0.46708858967144096\n",
      "episode :  26\n",
      "current step :  55\n",
      "reward :  0.3341665228994437\n",
      "episode :  26\n",
      "current step :  60\n",
      "reward :  0.25243085226604733\n",
      "episode :  26\n",
      "current step :  65\n",
      "reward :  0.3968329948109964\n",
      "episode :  26\n",
      "current step :  70\n",
      "reward :  0.43487707262895914\n",
      "episode :  26\n",
      "current step :  75\n",
      "reward :  0.43302294294742644\n",
      "episode :  26\n",
      "current step :  80\n",
      "reward :  0.4600767483916556\n",
      "episode :  26\n",
      "current step :  85\n",
      "reward :  0.48078380543250104\n",
      "episode :  26\n",
      "current step :  90\n",
      "reward :  0.4716202905305767\n",
      "episode :  26\n",
      "current step :  95\n",
      "reward :  0.4318414347660455\n",
      "episode :  26\n",
      "current step :  100\n",
      "reward :  0.4009168510612028\n",
      "episode :  26\n",
      "current step :  105\n",
      "reward :  0.38608377750330736\n",
      "episode :  26\n",
      "current step :  110\n",
      "reward :  0.3987143357515309\n",
      "episode :  26\n",
      "current step :  115\n",
      "reward :  0.3571658514131319\n",
      "episode :  26\n",
      "current step :  120\n",
      "reward :  0.4492182837427563\n",
      "episode :  26\n",
      "current step :  125\n",
      "reward :  0.4506392606807041\n",
      "episode :  26\n",
      "current step :  130\n",
      "reward :  0.45217763046227377\n",
      "episode :  26\n",
      "current step :  135\n",
      "reward :  0.3969438844602824\n",
      "episode :  26\n",
      "current step :  140\n",
      "reward :  0.4222737069326191\n",
      "episode :  26\n",
      "current step :  145\n",
      "reward :  0.45275642546054007\n",
      "Episode 26, Total Reward: 12.711903383846012\n",
      "episode :  27\n",
      "current step :  0\n",
      "reward :  0.45609985553900834\n",
      "episode :  27\n",
      "current step :  5\n",
      "reward :  0.47175388678644636\n",
      "episode :  27\n",
      "current step :  10\n",
      "reward :  0.6137483272558502\n",
      "episode :  27\n",
      "current step :  15\n",
      "reward :  0.46237448051000085\n",
      "episode :  27\n",
      "current step :  20\n",
      "reward :  0.4739124866181724\n",
      "episode :  27\n",
      "current step :  25\n",
      "reward :  0.34366529387045774\n",
      "episode :  27\n",
      "current step :  30\n",
      "reward :  0.5734233215131949\n",
      "episode :  27\n",
      "current step :  35\n",
      "reward :  0.3853872399640537\n",
      "episode :  27\n",
      "current step :  40\n",
      "reward :  0.24308192098637144\n",
      "episode :  27\n",
      "current step :  45\n",
      "reward :  0.31785579689560295\n",
      "episode :  27\n",
      "current step :  50\n",
      "reward :  0.3492221065182317\n",
      "episode :  27\n",
      "current step :  55\n",
      "reward :  0.21677205830555718\n",
      "episode :  27\n",
      "current step :  60\n",
      "reward :  0.22732560734396573\n",
      "episode :  27\n",
      "current step :  65\n",
      "reward :  0.24244082298030745\n",
      "episode :  27\n",
      "current step :  70\n",
      "reward :  0.5044624969551792\n",
      "episode :  27\n",
      "current step :  75\n",
      "reward :  0.3198430287455365\n",
      "episode :  27\n",
      "current step :  80\n",
      "reward :  0.5568644258552442\n",
      "episode :  27\n",
      "current step :  85\n",
      "reward :  0.47348484362604315\n",
      "episode :  27\n",
      "current step :  90\n",
      "reward :  0.48362673602777595\n",
      "episode :  27\n",
      "current step :  95\n",
      "reward :  0.35682117729587337\n",
      "episode :  27\n",
      "current step :  100\n",
      "reward :  0.4616561328785236\n",
      "episode :  27\n",
      "current step :  105\n",
      "reward :  0.43166228351011793\n",
      "episode :  27\n",
      "current step :  110\n",
      "reward :  0.2895828428387256\n",
      "episode :  27\n",
      "current step :  115\n",
      "reward :  0.40120858507541757\n",
      "episode :  27\n",
      "current step :  120\n",
      "reward :  0.42670515884823823\n",
      "episode :  27\n",
      "current step :  125\n",
      "reward :  0.25036656860691175\n",
      "episode :  27\n",
      "current step :  130\n",
      "reward :  0.20089252070774652\n",
      "episode :  27\n",
      "current step :  135\n",
      "reward :  0.40501189269386595\n",
      "episode :  27\n",
      "current step :  140\n",
      "reward :  0.4186634649260004\n",
      "episode :  27\n",
      "current step :  145\n",
      "reward :  0.4689819064339674\n",
      "Episode 27, Total Reward: 11.826897270112388\n",
      "episode :  28\n",
      "current step :  0\n",
      "reward :  0.28387389184814743\n",
      "episode :  28\n",
      "current step :  5\n",
      "reward :  0.4861084231349885\n",
      "episode :  28\n",
      "current step :  10\n",
      "reward :  0.5370213228563997\n",
      "episode :  28\n",
      "current step :  15\n",
      "reward :  0.3815834449150547\n",
      "episode :  28\n",
      "current step :  20\n",
      "reward :  0.47164891728287167\n",
      "episode :  28\n",
      "current step :  25\n",
      "reward :  0.5138153103851898\n",
      "episode :  28\n",
      "current step :  30\n",
      "reward :  0.4760776127065246\n",
      "episode :  28\n",
      "current step :  35\n",
      "reward :  0.2983071135758465\n",
      "episode :  28\n",
      "current step :  40\n",
      "reward :  0.2540619875795075\n",
      "episode :  28\n",
      "current step :  45\n",
      "reward :  0.4432049213466005\n",
      "episode :  28\n",
      "current step :  50\n",
      "reward :  0.46783859347588846\n",
      "episode :  28\n",
      "current step :  55\n",
      "reward :  0.4190213048477358\n",
      "episode :  28\n",
      "current step :  60\n",
      "reward :  0.2405976731998443\n",
      "episode :  28\n",
      "current step :  65\n",
      "reward :  0.2698918438650737\n",
      "episode :  28\n",
      "current step :  70\n",
      "reward :  0.22874510022457548\n",
      "episode :  28\n",
      "current step :  75\n",
      "reward :  0.4422554012500762\n",
      "episode :  28\n",
      "current step :  80\n",
      "reward :  0.4417495590888808\n",
      "episode :  28\n",
      "current step :  85\n",
      "reward :  0.4744691447791345\n",
      "episode :  28\n",
      "current step :  90\n",
      "reward :  0.4983786007048401\n",
      "episode :  28\n",
      "current step :  95\n",
      "reward :  0.45673804827658127\n",
      "episode :  28\n",
      "current step :  100\n",
      "reward :  0.4722976089084865\n",
      "episode :  28\n",
      "current step :  105\n",
      "reward :  0.46738332184371395\n",
      "episode :  28\n",
      "current step :  110\n",
      "reward :  0.4035930240486737\n",
      "episode :  28\n",
      "current step :  115\n",
      "reward :  0.44662223517923105\n",
      "episode :  28\n",
      "current step :  120\n",
      "reward :  0.3001123778926804\n",
      "episode :  28\n",
      "current step :  125\n",
      "reward :  0.46196215892457104\n",
      "episode :  28\n",
      "current step :  130\n",
      "reward :  0.5090889392312716\n",
      "episode :  28\n",
      "current step :  135\n",
      "reward :  0.3667073652984886\n",
      "episode :  28\n",
      "current step :  140\n",
      "reward :  0.46807419303164327\n",
      "episode :  28\n",
      "current step :  145\n",
      "reward :  0.5366561786997999\n",
      "Episode 28, Total Reward: 12.517885618402325\n",
      "episode :  29\n",
      "current step :  0\n",
      "reward :  0.4336363648334841\n",
      "episode :  29\n",
      "current step :  5\n",
      "reward :  0.5616077445886878\n",
      "episode :  29\n",
      "current step :  10\n",
      "reward :  0.3868410980744406\n",
      "episode :  29\n",
      "current step :  15\n",
      "reward :  0.38481542514851386\n",
      "episode :  29\n",
      "current step :  20\n",
      "reward :  0.6395678835214808\n",
      "episode :  29\n",
      "current step :  25\n",
      "reward :  0.5605715977356991\n",
      "episode :  29\n",
      "current step :  30\n",
      "reward :  0.3358443660450095\n",
      "episode :  29\n",
      "current step :  35\n",
      "reward :  0.38824746184840897\n",
      "episode :  29\n",
      "current step :  40\n",
      "reward :  0.33488884745292985\n",
      "episode :  29\n",
      "current step :  45\n",
      "reward :  0.2607635998745627\n",
      "episode :  29\n",
      "current step :  50\n",
      "reward :  0.2708436253027565\n",
      "episode :  29\n",
      "current step :  55\n",
      "reward :  0.4977985138674226\n",
      "episode :  29\n",
      "current step :  60\n",
      "reward :  0.2516453011251701\n",
      "episode :  29\n",
      "current step :  65\n",
      "reward :  0.19817573526744073\n",
      "episode :  29\n",
      "current step :  70\n",
      "reward :  0.3109937523673716\n",
      "episode :  29\n",
      "current step :  75\n",
      "reward :  0.6143651297994902\n",
      "episode :  29\n",
      "current step :  80\n",
      "reward :  0.30125400694092574\n",
      "episode :  29\n",
      "current step :  85\n",
      "reward :  0.6161473579354952\n",
      "episode :  29\n",
      "current step :  90\n",
      "reward :  0.45305661409756937\n",
      "episode :  29\n",
      "current step :  95\n",
      "reward :  0.4225060120672105\n",
      "episode :  29\n",
      "current step :  100\n",
      "reward :  0.5277565288638886\n",
      "episode :  29\n",
      "current step :  105\n",
      "reward :  0.41463151141037496\n",
      "episode :  29\n",
      "current step :  110\n",
      "reward :  0.5011516195575771\n",
      "episode :  29\n",
      "current step :  115\n",
      "reward :  0.44143071861957306\n",
      "episode :  29\n",
      "current step :  120\n",
      "reward :  0.4878212276744144\n",
      "episode :  29\n",
      "current step :  125\n",
      "reward :  0.5127549471172304\n",
      "episode :  29\n",
      "current step :  130\n",
      "reward :  0.5601935973337462\n",
      "episode :  29\n",
      "current step :  135\n",
      "reward :  0.39936840165104304\n",
      "episode :  29\n",
      "current step :  140\n",
      "reward :  0.3739073732983571\n",
      "episode :  29\n",
      "current step :  145\n",
      "reward :  0.4028884183344086\n",
      "Episode 29, Total Reward: 12.845474781754682\n",
      "episode :  30\n",
      "current step :  0\n",
      "reward :  0.49016203626609134\n",
      "episode :  30\n",
      "current step :  5\n",
      "reward :  0.4747852851094727\n",
      "episode :  30\n",
      "current step :  10\n",
      "reward :  0.41331088785255643\n",
      "episode :  30\n",
      "current step :  15\n",
      "reward :  0.5066638388471345\n",
      "episode :  30\n",
      "current step :  20\n",
      "reward :  0.1974101449135939\n",
      "episode :  30\n",
      "current step :  25\n",
      "reward :  0.5297134007553076\n",
      "episode :  30\n",
      "current step :  30\n",
      "reward :  0.4993429714888822\n",
      "episode :  30\n",
      "current step :  35\n",
      "reward :  0.25255212022470186\n",
      "episode :  30\n",
      "current step :  40\n",
      "reward :  0.34884162400124546\n",
      "episode :  30\n",
      "current step :  45\n",
      "reward :  0.5492574248277697\n",
      "episode :  30\n",
      "current step :  50\n",
      "reward :  0.26489365235441376\n",
      "episode :  30\n",
      "current step :  55\n",
      "reward :  0.5876532598788038\n",
      "episode :  30\n",
      "current step :  60\n",
      "reward :  0.2617178336135556\n",
      "episode :  30\n",
      "current step :  65\n",
      "reward :  0.23132153156408658\n",
      "episode :  30\n",
      "current step :  70\n",
      "reward :  0.29782315389016184\n",
      "episode :  30\n",
      "current step :  75\n",
      "reward :  0.3296994848605131\n",
      "episode :  30\n",
      "current step :  80\n",
      "reward :  0.2403220445553988\n",
      "episode :  30\n",
      "current step :  85\n",
      "reward :  0.3161199116197378\n",
      "episode :  30\n",
      "current step :  90\n",
      "reward :  0.48327561435847455\n",
      "episode :  30\n",
      "current step :  95\n",
      "reward :  0.42069847208892974\n",
      "episode :  30\n",
      "current step :  100\n",
      "reward :  0.4102875692125334\n",
      "episode :  30\n",
      "current step :  105\n",
      "reward :  0.3587961661752772\n",
      "episode :  30\n",
      "current step :  110\n",
      "reward :  0.48045756401940354\n",
      "episode :  30\n",
      "current step :  115\n",
      "reward :  0.4075931482511229\n",
      "episode :  30\n",
      "current step :  120\n",
      "reward :  0.4832787177147011\n",
      "episode :  30\n",
      "current step :  125\n",
      "reward :  0.48275794224382873\n",
      "episode :  30\n",
      "current step :  130\n",
      "reward :  0.503584579868217\n",
      "episode :  30\n",
      "current step :  135\n",
      "reward :  0.23009242853238254\n",
      "episode :  30\n",
      "current step :  140\n",
      "reward :  0.4986181619607517\n",
      "episode :  30\n",
      "current step :  145\n",
      "reward :  0.42307660039476125\n",
      "Episode 30, Total Reward: 11.97410757144381\n",
      "episode :  31\n",
      "current step :  0\n",
      "reward :  0.36780617663618215\n",
      "episode :  31\n",
      "current step :  5\n",
      "reward :  0.4696239988349888\n",
      "episode :  31\n",
      "current step :  10\n",
      "reward :  0.3774303006796846\n",
      "episode :  31\n",
      "current step :  15\n",
      "reward :  0.5277201145399981\n",
      "episode :  31\n",
      "current step :  20\n",
      "reward :  0.5715372763070797\n",
      "episode :  31\n",
      "current step :  25\n",
      "reward :  0.21521945057493086\n",
      "episode :  31\n",
      "current step :  30\n",
      "reward :  0.35125893082937204\n",
      "episode :  31\n",
      "current step :  35\n",
      "reward :  0.4375797151286084\n",
      "episode :  31\n",
      "current step :  40\n",
      "reward :  0.330604507487169\n",
      "episode :  31\n",
      "current step :  45\n",
      "reward :  0.21015366211155437\n",
      "episode :  31\n",
      "current step :  50\n",
      "reward :  0.2991749171906003\n",
      "episode :  31\n",
      "current step :  55\n",
      "reward :  0.36863371437524817\n",
      "episode :  31\n",
      "current step :  60\n",
      "reward :  0.355844155222055\n",
      "episode :  31\n",
      "current step :  65\n",
      "reward :  0.3914502866258629\n",
      "episode :  31\n",
      "current step :  70\n",
      "reward :  0.27309884384612493\n",
      "episode :  31\n",
      "current step :  75\n",
      "reward :  0.34238365345982646\n",
      "episode :  31\n",
      "current step :  80\n",
      "reward :  0.516639837905381\n",
      "episode :  31\n",
      "current step :  85\n",
      "reward :  0.41460166609084786\n",
      "episode :  31\n",
      "current step :  90\n",
      "reward :  0.5850489708894104\n",
      "episode :  31\n",
      "current step :  95\n",
      "reward :  0.3931576083030792\n",
      "episode :  31\n",
      "current step :  100\n",
      "reward :  0.4416907547703212\n",
      "episode :  31\n",
      "current step :  105\n",
      "reward :  0.47566131019934843\n",
      "episode :  31\n",
      "current step :  110\n",
      "reward :  0.5464723602653763\n",
      "episode :  31\n",
      "current step :  115\n",
      "reward :  0.2814102256993558\n",
      "episode :  31\n",
      "current step :  120\n",
      "reward :  0.35577629336327354\n",
      "episode :  31\n",
      "current step :  125\n",
      "reward :  0.4619671036636477\n",
      "episode :  31\n",
      "current step :  130\n",
      "reward :  0.46405863026830396\n",
      "episode :  31\n",
      "current step :  135\n",
      "reward :  0.39272035401718475\n",
      "episode :  31\n",
      "current step :  140\n",
      "reward :  0.5242329948999078\n",
      "episode :  31\n",
      "current step :  145\n",
      "reward :  0.4069527052066537\n",
      "Episode 31, Total Reward: 12.149910519391376\n",
      "episode :  32\n",
      "current step :  0\n",
      "reward :  0.3968604519243132\n",
      "episode :  32\n",
      "current step :  5\n",
      "reward :  0.5065596510894683\n",
      "episode :  32\n",
      "current step :  10\n",
      "reward :  0.42041396094635897\n",
      "episode :  32\n",
      "current step :  15\n",
      "reward :  0.54240669731708\n",
      "episode :  32\n",
      "current step :  20\n",
      "reward :  0.5150764695658532\n",
      "episode :  32\n",
      "current step :  25\n",
      "reward :  0.3671594960441269\n",
      "episode :  32\n",
      "current step :  30\n",
      "reward :  0.5159808566723383\n",
      "episode :  32\n",
      "current step :  35\n",
      "reward :  0.4622352595544401\n",
      "episode :  32\n",
      "current step :  40\n",
      "reward :  0.2577312788480768\n",
      "episode :  32\n",
      "current step :  45\n",
      "reward :  0.5125100005155214\n",
      "episode :  32\n",
      "current step :  50\n",
      "reward :  0.4611476257739336\n",
      "episode :  32\n",
      "current step :  55\n",
      "reward :  0.2752989338500817\n",
      "episode :  32\n",
      "current step :  60\n",
      "reward :  0.2215908794978597\n",
      "episode :  32\n",
      "current step :  65\n",
      "reward :  0.3176652127336608\n",
      "episode :  32\n",
      "current step :  70\n",
      "reward :  0.2554838420012619\n",
      "episode :  32\n",
      "current step :  75\n",
      "reward :  0.44611638795560227\n",
      "episode :  32\n",
      "current step :  80\n",
      "reward :  0.4407121008307733\n",
      "episode :  32\n",
      "current step :  85\n",
      "reward :  0.4391384686542401\n",
      "episode :  32\n",
      "current step :  90\n",
      "reward :  0.5421513940099328\n",
      "episode :  32\n",
      "current step :  95\n",
      "reward :  0.450127011655745\n",
      "episode :  32\n",
      "current step :  100\n",
      "reward :  0.6016694652061708\n",
      "episode :  32\n",
      "current step :  105\n",
      "reward :  0.41575365195546093\n",
      "episode :  32\n",
      "current step :  110\n",
      "reward :  0.33483348489728526\n",
      "episode :  32\n",
      "current step :  115\n",
      "reward :  0.36459176274709404\n",
      "episode :  32\n",
      "current step :  120\n",
      "reward :  0.503628751422774\n",
      "episode :  32\n",
      "current step :  125\n",
      "reward :  0.44158300845757126\n",
      "episode :  32\n",
      "current step :  130\n",
      "reward :  0.47582766577158675\n",
      "episode :  32\n",
      "current step :  135\n",
      "reward :  0.41570378959452947\n",
      "episode :  32\n",
      "current step :  140\n",
      "reward :  0.604548270385607\n",
      "episode :  32\n",
      "current step :  145\n",
      "reward :  0.5345882922889351\n",
      "Episode 32, Total Reward: 13.039094122167683\n",
      "episode :  33\n",
      "current step :  0\n",
      "reward :  0.4597239552031116\n",
      "episode :  33\n",
      "current step :  5\n",
      "reward :  0.5466108813913886\n",
      "episode :  33\n",
      "current step :  10\n",
      "reward :  0.4355358483973646\n",
      "episode :  33\n",
      "current step :  15\n",
      "reward :  0.519866657568684\n",
      "episode :  33\n",
      "current step :  20\n",
      "reward :  0.3611983585123577\n",
      "episode :  33\n",
      "current step :  25\n",
      "reward :  0.441115055194489\n",
      "episode :  33\n",
      "current step :  30\n",
      "reward :  0.5357261054369667\n",
      "episode :  33\n",
      "current step :  35\n",
      "reward :  0.286620699752921\n",
      "episode :  33\n",
      "current step :  40\n",
      "reward :  0.4611365503513938\n",
      "episode :  33\n",
      "current step :  45\n",
      "reward :  0.2832751757680718\n",
      "episode :  33\n",
      "current step :  50\n",
      "reward :  0.4246775519028717\n",
      "episode :  33\n",
      "current step :  55\n",
      "reward :  0.2093509019033987\n",
      "episode :  33\n",
      "current step :  60\n",
      "reward :  0.2489814217052098\n",
      "episode :  33\n",
      "current step :  65\n",
      "reward :  0.5705953019458508\n",
      "episode :  33\n",
      "current step :  70\n",
      "reward :  0.2508058351823672\n",
      "episode :  33\n",
      "current step :  75\n",
      "reward :  0.32360568792827427\n",
      "episode :  33\n",
      "current step :  80\n",
      "reward :  0.5211988205719223\n",
      "episode :  33\n",
      "current step :  85\n",
      "reward :  0.45729143844892206\n",
      "episode :  33\n",
      "current step :  90\n",
      "reward :  0.44821270587325684\n",
      "episode :  33\n",
      "current step :  95\n",
      "reward :  0.46749371473222234\n",
      "episode :  33\n",
      "current step :  100\n",
      "reward :  0.3988192537640477\n",
      "episode :  33\n",
      "current step :  105\n",
      "reward :  0.39196874550386646\n",
      "episode :  33\n",
      "current step :  110\n",
      "reward :  0.39069612428000794\n",
      "episode :  33\n",
      "current step :  115\n",
      "reward :  0.2198349957284511\n",
      "episode :  33\n",
      "current step :  120\n",
      "reward :  0.3926270160982306\n",
      "episode :  33\n",
      "current step :  125\n",
      "reward :  0.47152378327871414\n",
      "episode :  33\n",
      "current step :  130\n",
      "reward :  0.45510641200355884\n",
      "episode :  33\n",
      "current step :  135\n",
      "reward :  0.3676163528804216\n",
      "episode :  33\n",
      "current step :  140\n",
      "reward :  0.47460301349629586\n",
      "episode :  33\n",
      "current step :  145\n",
      "reward :  0.36849303801767014\n",
      "Episode 33, Total Reward: 12.184311402822312\n",
      "episode :  34\n",
      "current step :  0\n",
      "reward :  0.5139827273776645\n",
      "episode :  34\n",
      "current step :  5\n",
      "reward :  0.5265512420866654\n",
      "episode :  34\n",
      "current step :  10\n",
      "reward :  0.4519491166680983\n",
      "episode :  34\n",
      "current step :  15\n",
      "reward :  0.445593001677244\n",
      "episode :  34\n",
      "current step :  20\n",
      "reward :  0.4664708642588043\n",
      "episode :  34\n",
      "current step :  25\n",
      "reward :  0.5002725938046564\n",
      "episode :  34\n",
      "current step :  30\n",
      "reward :  0.42558887873615714\n",
      "episode :  34\n",
      "current step :  35\n",
      "reward :  0.49757514732770336\n",
      "episode :  34\n",
      "current step :  40\n",
      "reward :  0.25995041813884373\n",
      "episode :  34\n",
      "current step :  45\n",
      "reward :  0.23095108518603744\n",
      "episode :  34\n",
      "current step :  50\n",
      "reward :  0.33598054391739457\n",
      "episode :  34\n",
      "current step :  55\n",
      "reward :  0.46135251653054066\n",
      "episode :  34\n",
      "current step :  60\n",
      "reward :  0.5351619636009186\n",
      "episode :  34\n",
      "current step :  65\n",
      "reward :  0.25545755100331335\n",
      "episode :  34\n",
      "current step :  70\n",
      "reward :  0.44681565208060603\n",
      "episode :  34\n",
      "current step :  75\n",
      "reward :  0.317066093361858\n",
      "episode :  34\n",
      "current step :  80\n",
      "reward :  0.3293372178327286\n",
      "episode :  34\n",
      "current step :  85\n",
      "reward :  0.44243589567223107\n",
      "episode :  34\n",
      "current step :  90\n",
      "reward :  0.46415767347804704\n",
      "episode :  34\n",
      "current step :  95\n",
      "reward :  0.43365989070758815\n",
      "episode :  34\n",
      "current step :  100\n",
      "reward :  0.5039738947564187\n",
      "episode :  34\n",
      "current step :  105\n",
      "reward :  0.43947994429922066\n",
      "episode :  34\n",
      "current step :  110\n",
      "reward :  0.4228815759549346\n",
      "episode :  34\n",
      "current step :  115\n",
      "reward :  0.32587718506550023\n",
      "episode :  34\n",
      "current step :  120\n",
      "reward :  0.48292102477589494\n",
      "episode :  34\n",
      "current step :  125\n",
      "reward :  0.4245737076565329\n",
      "episode :  34\n",
      "current step :  130\n",
      "reward :  0.41511273998103276\n",
      "episode :  34\n",
      "current step :  135\n",
      "reward :  0.4560810000537603\n",
      "episode :  34\n",
      "current step :  140\n",
      "reward :  0.5292479561110844\n",
      "episode :  34\n",
      "current step :  145\n",
      "reward :  0.43349012119074637\n",
      "Episode 34, Total Reward: 12.773949223292227\n",
      "episode :  35\n",
      "current step :  0\n",
      "reward :  0.4606029184395528\n",
      "episode :  35\n",
      "current step :  5\n",
      "reward :  0.31473883722805707\n",
      "episode :  35\n",
      "current step :  10\n",
      "reward :  0.48708401668674084\n",
      "episode :  35\n",
      "current step :  15\n",
      "reward :  0.5205716066704401\n",
      "episode :  35\n",
      "current step :  20\n",
      "reward :  0.5257464835869892\n",
      "episode :  35\n",
      "current step :  25\n",
      "reward :  0.35998926973301265\n",
      "episode :  35\n",
      "current step :  30\n",
      "reward :  0.2955062153381589\n",
      "episode :  35\n",
      "current step :  35\n",
      "reward :  0.3592862634356839\n",
      "episode :  35\n",
      "current step :  40\n",
      "reward :  0.23886805265147332\n",
      "episode :  35\n",
      "current step :  45\n",
      "reward :  0.34098095178022486\n",
      "episode :  35\n",
      "current step :  50\n",
      "reward :  0.4583611657647474\n",
      "episode :  35\n",
      "current step :  55\n",
      "reward :  0.45206822455650153\n",
      "episode :  35\n",
      "current step :  60\n",
      "reward :  0.1533934044860561\n",
      "episode :  35\n",
      "current step :  65\n",
      "reward :  0.4252418625393631\n",
      "episode :  35\n",
      "current step :  70\n",
      "reward :  0.32216052796855327\n",
      "episode :  35\n",
      "current step :  75\n",
      "reward :  0.4426920169924195\n",
      "episode :  35\n",
      "current step :  80\n",
      "reward :  0.3343890560507934\n",
      "episode :  35\n",
      "current step :  85\n",
      "reward :  0.5655237788180142\n",
      "episode :  35\n",
      "current step :  90\n",
      "reward :  0.3914389983492448\n",
      "episode :  35\n",
      "current step :  95\n",
      "reward :  0.3936371326681444\n",
      "episode :  35\n",
      "current step :  100\n",
      "reward :  0.48935140971828406\n",
      "episode :  35\n",
      "current step :  105\n",
      "reward :  0.5016343650071468\n",
      "episode :  35\n",
      "current step :  110\n",
      "reward :  0.41273977157182684\n",
      "episode :  35\n",
      "current step :  115\n",
      "reward :  0.44525388186817594\n",
      "episode :  35\n",
      "current step :  120\n",
      "reward :  0.4810661341922855\n",
      "episode :  35\n",
      "current step :  125\n",
      "reward :  0.4155065890440152\n",
      "episode :  35\n",
      "current step :  130\n",
      "reward :  0.42775359436161886\n",
      "episode :  35\n",
      "current step :  135\n",
      "reward :  0.4136069482301724\n",
      "episode :  35\n",
      "current step :  140\n",
      "reward :  0.4640645793189242\n",
      "episode :  35\n",
      "current step :  145\n",
      "reward :  0.4395440828502849\n",
      "Episode 35, Total Reward: 12.332802139906908\n",
      "episode :  36\n",
      "current step :  0\n",
      "reward :  0.3897939053229315\n",
      "episode :  36\n",
      "current step :  5\n",
      "reward :  0.3433835322639356\n",
      "episode :  36\n",
      "current step :  10\n",
      "reward :  0.4560986228105467\n",
      "episode :  36\n",
      "current step :  15\n",
      "reward :  0.5033532998286459\n",
      "episode :  36\n",
      "current step :  20\n",
      "reward :  0.39711067337980194\n",
      "episode :  36\n",
      "current step :  25\n",
      "reward :  0.46977001087597414\n",
      "episode :  36\n",
      "current step :  30\n",
      "reward :  0.44257809388486336\n",
      "episode :  36\n",
      "current step :  35\n",
      "reward :  0.3632909642179472\n",
      "episode :  36\n",
      "current step :  40\n",
      "reward :  0.22944092706394734\n",
      "episode :  36\n",
      "current step :  45\n",
      "reward :  0.284136522435805\n",
      "episode :  36\n",
      "current step :  50\n",
      "reward :  0.24110972388154223\n",
      "episode :  36\n",
      "current step :  55\n",
      "reward :  0.26035078944778123\n",
      "episode :  36\n",
      "current step :  60\n",
      "reward :  0.20729994060775844\n",
      "episode :  36\n",
      "current step :  65\n",
      "reward :  0.28547088144007826\n",
      "episode :  36\n",
      "current step :  70\n",
      "reward :  0.5854772890828043\n",
      "episode :  36\n",
      "current step :  75\n",
      "reward :  0.41479634662332143\n",
      "episode :  36\n",
      "current step :  80\n",
      "reward :  0.5011932276177928\n",
      "episode :  36\n",
      "current step :  85\n",
      "reward :  0.41299219378748714\n",
      "episode :  36\n",
      "current step :  90\n",
      "reward :  0.3631851057314117\n",
      "episode :  36\n",
      "current step :  95\n",
      "reward :  0.4958687836648973\n",
      "episode :  36\n",
      "current step :  100\n",
      "reward :  0.5343915760710052\n",
      "episode :  36\n",
      "current step :  105\n",
      "reward :  0.46047789340138906\n",
      "episode :  36\n",
      "current step :  110\n",
      "reward :  0.5256509780080287\n",
      "episode :  36\n",
      "current step :  115\n",
      "reward :  0.20195156864462338\n",
      "episode :  36\n",
      "current step :  120\n",
      "reward :  0.4348830526815989\n",
      "episode :  36\n",
      "current step :  125\n",
      "reward :  0.4076059434504506\n",
      "episode :  36\n",
      "current step :  130\n",
      "reward :  0.4795675740284059\n",
      "episode :  36\n",
      "current step :  135\n",
      "reward :  0.48105812608348364\n",
      "episode :  36\n",
      "current step :  140\n",
      "reward :  0.48818762633602697\n",
      "episode :  36\n",
      "current step :  145\n",
      "reward :  0.3648345440052014\n",
      "Episode 36, Total Reward: 12.02530971667949\n",
      "episode :  37\n",
      "current step :  0\n",
      "reward :  0.4099507706662151\n",
      "episode :  37\n",
      "current step :  5\n",
      "reward :  0.5695316968485893\n",
      "episode :  37\n",
      "current step :  10\n",
      "reward :  0.4031066468025106\n",
      "episode :  37\n",
      "current step :  15\n",
      "reward :  0.5381280187609728\n",
      "episode :  37\n",
      "current step :  20\n",
      "reward :  0.3941419051099875\n",
      "episode :  37\n",
      "current step :  25\n",
      "reward :  0.4081566076427251\n",
      "episode :  37\n",
      "current step :  30\n",
      "reward :  0.18674532180603462\n",
      "episode :  37\n",
      "current step :  35\n",
      "reward :  0.3244752103566269\n",
      "episode :  37\n",
      "current step :  40\n",
      "reward :  0.3649133078870551\n",
      "episode :  37\n",
      "current step :  45\n",
      "reward :  0.42025423309189225\n",
      "episode :  37\n",
      "current step :  50\n",
      "reward :  0.3010702999395388\n",
      "episode :  37\n",
      "current step :  55\n",
      "reward :  0.2194175386101141\n",
      "episode :  37\n",
      "current step :  60\n",
      "reward :  0.25140939407177076\n",
      "episode :  37\n",
      "current step :  65\n",
      "reward :  0.2721132633195897\n",
      "episode :  37\n",
      "current step :  70\n",
      "reward :  0.28786651996738344\n",
      "episode :  37\n",
      "current step :  75\n",
      "reward :  0.5158707574773411\n",
      "episode :  37\n",
      "current step :  80\n",
      "reward :  0.5984251718994239\n",
      "episode :  37\n",
      "current step :  85\n",
      "reward :  0.4328216396522213\n",
      "episode :  37\n",
      "current step :  90\n",
      "reward :  0.1950424134500344\n",
      "episode :  37\n",
      "current step :  95\n",
      "reward :  0.4326609378357972\n",
      "episode :  37\n",
      "current step :  100\n",
      "reward :  0.5357458388170107\n",
      "episode :  37\n",
      "current step :  105\n",
      "reward :  0.3461613483259123\n",
      "episode :  37\n",
      "current step :  110\n",
      "reward :  0.4918477830362668\n",
      "episode :  37\n",
      "current step :  115\n",
      "reward :  0.41446241972391484\n",
      "episode :  37\n",
      "current step :  120\n",
      "reward :  0.22379837089808755\n",
      "episode :  37\n",
      "current step :  125\n",
      "reward :  0.48439004603580726\n",
      "episode :  37\n",
      "current step :  130\n",
      "reward :  0.2026879982470429\n",
      "episode :  37\n",
      "current step :  135\n",
      "reward :  0.4509739602724998\n",
      "episode :  37\n",
      "current step :  140\n",
      "reward :  0.3825454257060258\n",
      "episode :  37\n",
      "current step :  145\n",
      "reward :  0.3861996331650077\n",
      "Episode 37, Total Reward: 11.444914479423398\n",
      "episode :  38\n",
      "current step :  0\n",
      "reward :  0.5380755808982745\n",
      "episode :  38\n",
      "current step :  5\n",
      "reward :  0.44620707347314964\n",
      "episode :  38\n",
      "current step :  10\n",
      "reward :  0.42584407938380187\n",
      "episode :  38\n",
      "current step :  15\n",
      "reward :  0.4695889525109741\n",
      "episode :  38\n",
      "current step :  20\n",
      "reward :  0.3453643753835566\n",
      "episode :  38\n",
      "current step :  25\n",
      "reward :  0.2096037915948664\n",
      "episode :  38\n",
      "current step :  30\n",
      "reward :  0.3035942481207834\n",
      "episode :  38\n",
      "current step :  35\n",
      "reward :  0.41912301856402995\n",
      "episode :  38\n",
      "current step :  40\n",
      "reward :  0.300891960578964\n",
      "episode :  38\n",
      "current step :  45\n",
      "reward :  0.38854736816874835\n",
      "episode :  38\n",
      "current step :  50\n",
      "reward :  0.47465031332080326\n",
      "episode :  38\n",
      "current step :  55\n",
      "reward :  0.3008382487840426\n",
      "episode :  38\n",
      "current step :  60\n",
      "reward :  0.384204365520745\n",
      "episode :  38\n",
      "current step :  65\n",
      "reward :  0.2789565829288622\n",
      "episode :  38\n",
      "current step :  70\n",
      "reward :  0.23333478352952136\n",
      "episode :  38\n",
      "current step :  75\n",
      "reward :  0.5260257547201272\n",
      "episode :  38\n",
      "current step :  80\n",
      "reward :  0.46431539879815686\n",
      "episode :  38\n",
      "current step :  85\n",
      "reward :  0.4151305004429873\n",
      "episode :  38\n",
      "current step :  90\n",
      "reward :  0.45423513246100694\n",
      "episode :  38\n",
      "current step :  95\n",
      "reward :  0.3788726065379737\n",
      "episode :  38\n",
      "current step :  100\n",
      "reward :  0.45444106638525716\n",
      "episode :  38\n",
      "current step :  105\n",
      "reward :  0.3592835043261504\n",
      "episode :  38\n",
      "current step :  110\n",
      "reward :  0.4854358359621528\n",
      "episode :  38\n",
      "current step :  115\n",
      "reward :  0.2572948998057224\n",
      "episode :  38\n",
      "current step :  120\n",
      "reward :  0.42620561588249994\n",
      "episode :  38\n",
      "current step :  125\n",
      "reward :  0.3440904274443236\n",
      "episode :  38\n",
      "current step :  130\n",
      "reward :  0.5270446557906889\n",
      "episode :  38\n",
      "current step :  135\n",
      "reward :  0.6069799341014293\n",
      "episode :  38\n",
      "current step :  140\n",
      "reward :  0.4607327630495431\n",
      "episode :  38\n",
      "current step :  145\n",
      "reward :  0.43787405472499125\n",
      "Episode 38, Total Reward: 12.116786893194137\n",
      "episode :  39\n",
      "current step :  0\n",
      "reward :  0.42176940712024324\n",
      "episode :  39\n",
      "current step :  5\n",
      "reward :  0.4296016311445515\n",
      "episode :  39\n",
      "current step :  10\n",
      "reward :  0.3725476724329121\n",
      "episode :  39\n",
      "current step :  15\n",
      "reward :  0.4991760026050023\n",
      "episode :  39\n",
      "current step :  20\n",
      "reward :  0.42622570808219623\n",
      "episode :  39\n",
      "current step :  25\n",
      "reward :  0.5123604768029542\n",
      "episode :  39\n",
      "current step :  30\n",
      "reward :  0.24232006490473337\n",
      "episode :  39\n",
      "current step :  35\n",
      "reward :  0.34565373268301003\n",
      "episode :  39\n",
      "current step :  40\n",
      "reward :  0.29469894125830903\n",
      "episode :  39\n",
      "current step :  45\n",
      "reward :  0.38199067633055395\n",
      "episode :  39\n",
      "current step :  50\n",
      "reward :  0.3413703576731524\n",
      "episode :  39\n",
      "current step :  55\n",
      "reward :  0.25947309769973925\n",
      "episode :  39\n",
      "current step :  60\n",
      "reward :  0.37326034056700597\n",
      "episode :  39\n",
      "current step :  65\n",
      "reward :  0.26615234946763827\n",
      "episode :  39\n",
      "current step :  70\n",
      "reward :  0.43075228299485135\n",
      "episode :  39\n",
      "current step :  75\n",
      "reward :  0.5486130055748882\n",
      "episode :  39\n",
      "current step :  80\n",
      "reward :  0.49474352896955115\n",
      "episode :  39\n",
      "current step :  85\n",
      "reward :  0.4803966428433478\n",
      "episode :  39\n",
      "current step :  90\n",
      "reward :  0.46203288285209054\n",
      "episode :  39\n",
      "current step :  95\n",
      "reward :  0.35940854627843993\n",
      "episode :  39\n",
      "current step :  100\n",
      "reward :  0.3665081501027234\n",
      "episode :  39\n",
      "current step :  105\n",
      "reward :  0.4603032429791577\n",
      "episode :  39\n",
      "current step :  110\n",
      "reward :  0.43795552496892376\n",
      "episode :  39\n",
      "current step :  115\n",
      "reward :  0.0\n",
      "episode :  39\n",
      "current step :  120\n",
      "reward :  0.47514581458276794\n",
      "episode :  39\n",
      "current step :  125\n",
      "reward :  0.39923151566487136\n",
      "episode :  39\n",
      "current step :  130\n",
      "reward :  0.4290497379558739\n",
      "episode :  39\n",
      "current step :  135\n",
      "reward :  0.5562062571442893\n",
      "episode :  39\n",
      "current step :  140\n",
      "reward :  0.4558562530951491\n",
      "episode :  39\n",
      "current step :  145\n",
      "reward :  0.5274297103382203\n",
      "Episode 39, Total Reward: 12.050233555117149\n",
      "episode :  40\n",
      "current step :  0\n",
      "reward :  0.48530807556421085\n",
      "episode :  40\n",
      "current step :  5\n",
      "reward :  0.5535336438414219\n",
      "episode :  40\n",
      "current step :  10\n",
      "reward :  0.41994275501785844\n",
      "episode :  40\n",
      "current step :  15\n",
      "reward :  0.5027648944536851\n",
      "episode :  40\n",
      "current step :  20\n",
      "reward :  0.4122626052100233\n",
      "episode :  40\n",
      "current step :  25\n",
      "reward :  0.4483926999835012\n",
      "episode :  40\n",
      "current step :  30\n",
      "reward :  0.4438454668925346\n",
      "episode :  40\n",
      "current step :  35\n",
      "reward :  0.32543071683615277\n",
      "episode :  40\n",
      "current step :  40\n",
      "reward :  0.2923691893595653\n",
      "episode :  40\n",
      "current step :  45\n",
      "reward :  0.5083111833478554\n",
      "episode :  40\n",
      "current step :  50\n",
      "reward :  0.3977533095076741\n",
      "episode :  40\n",
      "current step :  55\n",
      "reward :  0.30979407635363004\n",
      "episode :  40\n",
      "current step :  60\n",
      "reward :  0.18947815444869212\n",
      "episode :  40\n",
      "current step :  65\n",
      "reward :  0.24472165963696393\n",
      "episode :  40\n",
      "current step :  70\n",
      "reward :  0.3777748935829747\n",
      "episode :  40\n",
      "current step :  75\n",
      "reward :  0.3554942124719844\n",
      "episode :  40\n",
      "current step :  80\n",
      "reward :  0.5616968088771871\n",
      "episode :  40\n",
      "current step :  85\n",
      "reward :  0.5256886142465035\n",
      "episode :  40\n",
      "current step :  90\n",
      "reward :  0.3132662389188833\n",
      "episode :  40\n",
      "current step :  95\n",
      "reward :  0.3634339151578103\n",
      "episode :  40\n",
      "current step :  100\n",
      "reward :  0.4118826668111089\n",
      "episode :  40\n",
      "current step :  105\n",
      "reward :  0.5369416277998501\n",
      "episode :  40\n",
      "current step :  110\n",
      "reward :  0.41931269495187495\n",
      "episode :  40\n",
      "current step :  115\n",
      "reward :  0.43773429399971486\n",
      "episode :  40\n",
      "current step :  120\n",
      "reward :  0.44011636175248653\n",
      "episode :  40\n",
      "current step :  125\n",
      "reward :  0.5314546095899009\n",
      "episode :  40\n",
      "current step :  130\n",
      "reward :  0.3629104188234951\n",
      "episode :  40\n",
      "current step :  135\n",
      "reward :  0.3566030028463178\n",
      "episode :  40\n",
      "current step :  140\n",
      "reward :  0.4429097202432278\n",
      "episode :  40\n",
      "current step :  145\n",
      "reward :  0.47497008003418684\n",
      "Episode 40, Total Reward: 12.446098590561274\n",
      "episode :  41\n",
      "current step :  0\n",
      "reward :  0.39324518409434533\n",
      "episode :  41\n",
      "current step :  5\n",
      "reward :  0.3805300329081355\n",
      "episode :  41\n",
      "current step :  10\n",
      "reward :  0.4375506267729746\n",
      "episode :  41\n",
      "current step :  15\n",
      "reward :  0.429111987961887\n",
      "episode :  41\n",
      "current step :  20\n",
      "reward :  0.4653973801730401\n",
      "episode :  41\n",
      "current step :  25\n",
      "reward :  0.45240562666357415\n",
      "episode :  41\n",
      "current step :  30\n",
      "reward :  0.30173347466097034\n",
      "episode :  41\n",
      "current step :  35\n",
      "reward :  0.279268459944417\n",
      "episode :  41\n",
      "current step :  40\n",
      "reward :  0.31350734283423437\n",
      "episode :  41\n",
      "current step :  45\n",
      "reward :  0.48671817678701146\n",
      "episode :  41\n",
      "current step :  50\n",
      "reward :  0.38556400992670187\n",
      "episode :  41\n",
      "current step :  55\n",
      "reward :  0.40726894050159157\n",
      "episode :  41\n",
      "current step :  60\n",
      "reward :  0.15365840747163267\n",
      "episode :  41\n",
      "current step :  65\n",
      "reward :  0.22495763125710722\n",
      "episode :  41\n",
      "current step :  70\n",
      "reward :  0.20879156145139946\n",
      "episode :  41\n",
      "current step :  75\n",
      "reward :  0.43045277016239564\n",
      "episode :  41\n",
      "current step :  80\n",
      "reward :  0.5057865912047785\n",
      "episode :  41\n",
      "current step :  85\n",
      "reward :  0.2539158526633783\n",
      "episode :  41\n",
      "current step :  90\n",
      "reward :  0.39718303590198434\n",
      "episode :  41\n",
      "current step :  95\n",
      "reward :  0.4282794462377478\n",
      "episode :  41\n",
      "current step :  100\n",
      "reward :  0.40366892439879054\n",
      "episode :  41\n",
      "current step :  105\n",
      "reward :  0.3800791538490917\n",
      "episode :  41\n",
      "current step :  110\n",
      "reward :  0.4023726726953817\n",
      "episode :  41\n",
      "current step :  115\n",
      "reward :  0.1713412752034746\n",
      "episode :  41\n",
      "current step :  120\n",
      "reward :  0.4920365546405898\n",
      "episode :  41\n",
      "current step :  125\n",
      "reward :  0.5074471236833265\n",
      "episode :  41\n",
      "current step :  130\n",
      "reward :  0.4494867106536805\n",
      "episode :  41\n",
      "current step :  135\n",
      "reward :  0.5393320196983261\n",
      "episode :  41\n",
      "current step :  140\n",
      "reward :  0.495353650895591\n",
      "episode :  41\n",
      "current step :  145\n",
      "reward :  0.45158173902748777\n",
      "Episode 41, Total Reward: 11.628026364325047\n",
      "episode :  42\n",
      "current step :  0\n",
      "reward :  0.5007143902279929\n",
      "episode :  42\n",
      "current step :  5\n",
      "reward :  0.4498028442469414\n",
      "episode :  42\n",
      "current step :  10\n",
      "reward :  0.34281976711667006\n",
      "episode :  42\n",
      "current step :  15\n",
      "reward :  0.4646475805651608\n",
      "episode :  42\n",
      "current step :  20\n",
      "reward :  0.46904160320519794\n",
      "episode :  42\n",
      "current step :  25\n",
      "reward :  0.2169864705083677\n",
      "episode :  42\n",
      "current step :  30\n",
      "reward :  0.5320149406530557\n",
      "episode :  42\n",
      "current step :  35\n",
      "reward :  0.43427143576465566\n",
      "episode :  42\n",
      "current step :  40\n",
      "reward :  0.3126399744499146\n",
      "episode :  42\n",
      "current step :  45\n",
      "reward :  0.3408171042354215\n",
      "episode :  42\n",
      "current step :  50\n",
      "reward :  0.4247108805772214\n",
      "episode :  42\n",
      "current step :  55\n",
      "reward :  0.2849636800243133\n",
      "episode :  42\n",
      "current step :  60\n",
      "reward :  0.3579671178446584\n",
      "episode :  42\n",
      "current step :  65\n",
      "reward :  0.4015734428440729\n",
      "episode :  42\n",
      "current step :  70\n",
      "reward :  0.6084067545224413\n",
      "episode :  42\n",
      "current step :  75\n",
      "reward :  0.6027919531209776\n",
      "episode :  42\n",
      "current step :  80\n",
      "reward :  0.2906753382317866\n",
      "episode :  42\n",
      "current step :  85\n",
      "reward :  0.3050296792075895\n",
      "episode :  42\n",
      "current step :  90\n",
      "reward :  0.40127944383989256\n",
      "episode :  42\n",
      "current step :  95\n",
      "reward :  0.4228836209940277\n",
      "episode :  42\n",
      "current step :  100\n",
      "reward :  0.22265905076469106\n",
      "episode :  42\n",
      "current step :  105\n",
      "reward :  0.3941467388514859\n",
      "episode :  42\n",
      "current step :  110\n",
      "reward :  0.5341573963450514\n",
      "episode :  42\n",
      "current step :  115\n",
      "reward :  0.4106265556272279\n",
      "episode :  42\n",
      "current step :  120\n",
      "reward :  0.3587889266329244\n",
      "episode :  42\n",
      "current step :  125\n",
      "reward :  0.4084850044848931\n",
      "episode :  42\n",
      "current step :  130\n",
      "reward :  0.5232288922915749\n",
      "episode :  42\n",
      "current step :  135\n",
      "reward :  0.19823436928566016\n",
      "episode :  42\n",
      "current step :  140\n",
      "reward :  0.49869148117886786\n",
      "episode :  42\n",
      "current step :  145\n",
      "reward :  0.461809875457595\n",
      "Episode 42, Total Reward: 12.174866313100333\n",
      "episode :  43\n",
      "current step :  0\n",
      "reward :  0.46666947738991094\n",
      "episode :  43\n",
      "current step :  5\n",
      "reward :  0.43539269418857296\n",
      "episode :  43\n",
      "current step :  10\n",
      "reward :  0.3636922935133544\n",
      "episode :  43\n",
      "current step :  15\n",
      "reward :  0.45322238835036033\n",
      "episode :  43\n",
      "current step :  20\n",
      "reward :  0.5237951649233883\n",
      "episode :  43\n",
      "current step :  25\n",
      "reward :  0.21518390195127313\n",
      "episode :  43\n",
      "current step :  30\n",
      "reward :  0.3891367512827937\n",
      "episode :  43\n",
      "current step :  35\n",
      "reward :  0.4465416586438293\n",
      "episode :  43\n",
      "current step :  40\n",
      "reward :  0.5588240475273654\n",
      "episode :  43\n",
      "current step :  45\n",
      "reward :  0.4562695032552752\n",
      "episode :  43\n",
      "current step :  50\n",
      "reward :  0.3951343186277779\n",
      "episode :  43\n",
      "current step :  55\n",
      "reward :  0.5256982488571739\n",
      "episode :  43\n",
      "current step :  60\n",
      "reward :  0.20981451290854122\n",
      "episode :  43\n",
      "current step :  65\n",
      "reward :  0.41443814685501273\n",
      "episode :  43\n",
      "current step :  70\n",
      "reward :  0.40634753554706626\n",
      "episode :  43\n",
      "current step :  75\n",
      "reward :  0.3873913640476113\n",
      "episode :  43\n",
      "current step :  80\n",
      "reward :  0.620471668548276\n",
      "episode :  43\n",
      "current step :  85\n",
      "reward :  0.501274432571799\n",
      "episode :  43\n",
      "current step :  90\n",
      "reward :  0.26276109806437475\n",
      "episode :  43\n",
      "current step :  95\n",
      "reward :  0.18971066815355125\n",
      "episode :  43\n",
      "current step :  100\n",
      "reward :  0.4659891369037074\n",
      "episode :  43\n",
      "current step :  105\n",
      "reward :  0.5134500470508292\n",
      "episode :  43\n",
      "current step :  110\n",
      "reward :  0.407479308820923\n",
      "episode :  43\n",
      "current step :  115\n",
      "reward :  0.42322098518591034\n",
      "episode :  43\n",
      "current step :  120\n",
      "reward :  0.3764060086450595\n",
      "episode :  43\n",
      "current step :  125\n",
      "reward :  0.470119060861385\n",
      "episode :  43\n",
      "current step :  130\n",
      "reward :  0.44470202784004365\n",
      "episode :  43\n",
      "current step :  135\n",
      "reward :  0.46824382430380074\n",
      "episode :  43\n",
      "current step :  140\n",
      "reward :  0.5168478340644604\n",
      "episode :  43\n",
      "current step :  145\n",
      "reward :  0.47458689665334663\n",
      "Episode 43, Total Reward: 12.78281500553678\n",
      "episode :  44\n",
      "current step :  0\n",
      "reward :  0.4590138447175739\n",
      "episode :  44\n",
      "current step :  5\n",
      "reward :  0.36924608649759405\n",
      "episode :  44\n",
      "current step :  10\n",
      "reward :  0.45327411990452693\n",
      "episode :  44\n",
      "current step :  15\n",
      "reward :  0.49382676529615177\n",
      "episode :  44\n",
      "current step :  20\n",
      "reward :  0.5623991356575804\n",
      "episode :  44\n",
      "current step :  25\n",
      "reward :  0.4712012985241032\n",
      "episode :  44\n",
      "current step :  30\n",
      "reward :  0.4451944141616143\n",
      "episode :  44\n",
      "current step :  35\n",
      "reward :  0.5252057412756809\n",
      "episode :  44\n",
      "current step :  40\n",
      "reward :  0.34041680563631443\n",
      "episode :  44\n",
      "current step :  45\n",
      "reward :  0.2795856803598893\n",
      "episode :  44\n",
      "current step :  50\n",
      "reward :  0.22192461074346576\n",
      "episode :  44\n",
      "current step :  55\n",
      "reward :  0.3763857619697707\n",
      "episode :  44\n",
      "current step :  60\n",
      "reward :  0.200868895961807\n",
      "episode :  44\n",
      "current step :  65\n",
      "reward :  0.20107233134870411\n",
      "episode :  44\n",
      "current step :  70\n",
      "reward :  0.37149762301855427\n",
      "episode :  44\n",
      "current step :  75\n",
      "reward :  0.3304825562803795\n",
      "episode :  44\n",
      "current step :  80\n",
      "reward :  0.5277998711349088\n",
      "episode :  44\n",
      "current step :  85\n",
      "reward :  0.4331633196544421\n",
      "episode :  44\n",
      "current step :  90\n",
      "reward :  0.220140182038145\n",
      "episode :  44\n",
      "current step :  95\n",
      "reward :  0.45315722355485216\n",
      "episode :  44\n",
      "current step :  100\n",
      "reward :  0.43332946122051896\n",
      "episode :  44\n",
      "current step :  105\n",
      "reward :  0.4728600298336827\n",
      "episode :  44\n",
      "current step :  110\n",
      "reward :  0.4162292407056456\n",
      "episode :  44\n",
      "current step :  115\n",
      "reward :  0.20604235590113806\n",
      "episode :  44\n",
      "current step :  120\n",
      "reward :  0.42992776577764036\n",
      "episode :  44\n",
      "current step :  125\n",
      "reward :  0.4441455868003016\n",
      "episode :  44\n",
      "current step :  130\n",
      "reward :  0.4285892903504672\n",
      "episode :  44\n",
      "current step :  135\n",
      "reward :  0.4689200081940139\n",
      "episode :  44\n",
      "current step :  140\n",
      "reward :  0.4772744451502242\n",
      "episode :  44\n",
      "current step :  145\n",
      "reward :  0.47968404071517323\n",
      "Episode 44, Total Reward: 11.992858492384865\n",
      "episode :  45\n",
      "current step :  0\n",
      "reward :  0.3436528815380755\n",
      "episode :  45\n",
      "current step :  5\n",
      "reward :  0.4945445918318933\n",
      "episode :  45\n",
      "current step :  10\n",
      "reward :  0.3948162696109871\n",
      "episode :  45\n",
      "current step :  15\n",
      "reward :  0.5692313858562177\n",
      "episode :  45\n",
      "current step :  20\n",
      "reward :  0.45627858025588\n",
      "episode :  45\n",
      "current step :  25\n",
      "reward :  0.4007994686178801\n",
      "episode :  45\n",
      "current step :  30\n",
      "reward :  0.29868651301017846\n",
      "episode :  45\n",
      "current step :  35\n",
      "reward :  0.43516294904048836\n",
      "episode :  45\n",
      "current step :  40\n",
      "reward :  0.3434807916776545\n",
      "episode :  45\n",
      "current step :  45\n",
      "reward :  0.2743239019599309\n",
      "episode :  45\n",
      "current step :  50\n",
      "reward :  0.5008848976163313\n",
      "episode :  45\n",
      "current step :  55\n",
      "reward :  0.3027538072048527\n",
      "episode :  45\n",
      "current step :  60\n",
      "reward :  0.255970919464604\n",
      "episode :  45\n",
      "current step :  65\n",
      "reward :  0.2546231818548918\n",
      "episode :  45\n",
      "current step :  70\n",
      "reward :  0.5869092713222055\n",
      "episode :  45\n",
      "current step :  75\n",
      "reward :  0.3475959531518259\n",
      "episode :  45\n",
      "current step :  80\n",
      "reward :  0.5389472752816887\n",
      "episode :  45\n",
      "current step :  85\n",
      "reward :  0.5411988633670755\n",
      "episode :  45\n",
      "current step :  90\n",
      "reward :  0.3905196911069231\n",
      "episode :  45\n",
      "current step :  95\n",
      "reward :  0.49507330486475704\n",
      "episode :  45\n",
      "current step :  100\n",
      "reward :  0.5306534079898668\n",
      "episode :  45\n",
      "current step :  105\n",
      "reward :  0.3680915284696978\n",
      "episode :  45\n",
      "current step :  110\n",
      "reward :  0.5060877828609487\n",
      "episode :  45\n",
      "current step :  115\n",
      "reward :  0.3996442413470306\n",
      "episode :  45\n",
      "current step :  120\n",
      "reward :  0.3697227643639879\n",
      "episode :  45\n",
      "current step :  125\n",
      "reward :  0.38381809502818187\n",
      "episode :  45\n",
      "current step :  130\n",
      "reward :  0.517840116732599\n",
      "episode :  45\n",
      "current step :  135\n",
      "reward :  0.4319705609436772\n",
      "episode :  45\n",
      "current step :  140\n",
      "reward :  0.4753225442297246\n",
      "episode :  45\n",
      "current step :  145\n",
      "reward :  0.4775633227330479\n",
      "Episode 45, Total Reward: 12.686168863333101\n",
      "episode :  46\n",
      "current step :  0\n",
      "reward :  0.38948229320202343\n",
      "episode :  46\n",
      "current step :  5\n",
      "reward :  0.6187452077571156\n",
      "episode :  46\n",
      "current step :  10\n",
      "reward :  0.5645759538185768\n",
      "episode :  46\n",
      "current step :  15\n",
      "reward :  0.4841175171999471\n",
      "episode :  46\n",
      "current step :  20\n",
      "reward :  0.4982177873172854\n",
      "episode :  46\n",
      "current step :  25\n",
      "reward :  0.523006032009174\n",
      "episode :  46\n",
      "current step :  30\n",
      "reward :  0.44618942850862914\n",
      "episode :  46\n",
      "current step :  35\n",
      "reward :  0.4095920789378222\n",
      "episode :  46\n",
      "current step :  40\n",
      "reward :  0.2835255036794996\n",
      "episode :  46\n",
      "current step :  45\n",
      "reward :  0.2605333305049521\n",
      "episode :  46\n",
      "current step :  50\n",
      "reward :  0.48724970982070653\n",
      "episode :  46\n",
      "current step :  55\n",
      "reward :  0.23045345484923677\n",
      "episode :  46\n",
      "current step :  60\n",
      "reward :  0.33646024559613896\n",
      "episode :  46\n",
      "current step :  65\n",
      "reward :  0.2833144630362187\n",
      "episode :  46\n",
      "current step :  70\n",
      "reward :  0.5301810203144288\n",
      "episode :  46\n",
      "current step :  75\n",
      "reward :  0.31156174486056826\n",
      "episode :  46\n",
      "current step :  80\n",
      "reward :  0.4883036330644253\n",
      "episode :  46\n",
      "current step :  85\n",
      "reward :  0.44715388804265355\n",
      "episode :  46\n",
      "current step :  90\n",
      "reward :  0.4423903689711923\n",
      "episode :  46\n",
      "current step :  95\n",
      "reward :  0.4149745313917804\n",
      "episode :  46\n",
      "current step :  100\n",
      "reward :  0.46582866321127786\n",
      "episode :  46\n",
      "current step :  105\n",
      "reward :  0.4758420231304171\n",
      "episode :  46\n",
      "current step :  110\n",
      "reward :  0.3918940616389741\n",
      "episode :  46\n",
      "current step :  115\n",
      "reward :  0.5737285691252311\n",
      "episode :  46\n",
      "current step :  120\n",
      "reward :  0.4411807385312245\n",
      "episode :  46\n",
      "current step :  125\n",
      "reward :  0.4695218694154906\n",
      "episode :  46\n",
      "current step :  130\n",
      "reward :  0.5226487661991333\n",
      "episode :  46\n",
      "current step :  135\n",
      "reward :  0.46308356841864773\n",
      "episode :  46\n",
      "current step :  140\n",
      "reward :  0.2913940024997897\n",
      "episode :  46\n",
      "current step :  145\n",
      "reward :  0.43522052864105765\n",
      "Episode 46, Total Reward: 12.98037098369362\n",
      "episode :  47\n",
      "current step :  0\n",
      "reward :  0.5384106689640942\n",
      "episode :  47\n",
      "current step :  5\n",
      "reward :  0.49352852682106196\n",
      "episode :  47\n",
      "current step :  10\n",
      "reward :  0.4086422327166912\n",
      "episode :  47\n",
      "current step :  15\n",
      "reward :  0.6372875052850662\n",
      "episode :  47\n",
      "current step :  20\n",
      "reward :  0.5071482748392596\n",
      "episode :  47\n",
      "current step :  25\n",
      "reward :  0.4479205982304567\n",
      "episode :  47\n",
      "current step :  30\n",
      "reward :  0.44467314809220937\n",
      "episode :  47\n",
      "current step :  35\n",
      "reward :  0.4824855755471372\n",
      "episode :  47\n",
      "current step :  40\n",
      "reward :  0.32879339575731686\n",
      "episode :  47\n",
      "current step :  45\n",
      "reward :  0.3686040803874592\n",
      "episode :  47\n",
      "current step :  50\n",
      "reward :  0.45020328493615147\n",
      "episode :  47\n",
      "current step :  55\n",
      "reward :  0.2588101526625658\n",
      "episode :  47\n",
      "current step :  60\n",
      "reward :  0.22012270342475113\n",
      "episode :  47\n",
      "current step :  65\n",
      "reward :  0.25890345877467885\n",
      "episode :  47\n",
      "current step :  70\n",
      "reward :  0.36911985894544586\n",
      "episode :  47\n",
      "current step :  75\n",
      "reward :  0.37312863030761495\n",
      "episode :  47\n",
      "current step :  80\n",
      "reward :  0.4593567432283523\n",
      "episode :  47\n",
      "current step :  85\n",
      "reward :  0.5177089473751894\n",
      "episode :  47\n",
      "current step :  90\n",
      "reward :  0.3699330812275781\n",
      "episode :  47\n",
      "current step :  95\n",
      "reward :  0.39439316666346236\n",
      "episode :  47\n",
      "current step :  100\n",
      "reward :  0.45406012650595157\n",
      "episode :  47\n",
      "current step :  105\n",
      "reward :  0.41907992539093797\n",
      "episode :  47\n",
      "current step :  110\n",
      "reward :  0.45784129793910244\n",
      "episode :  47\n",
      "current step :  115\n",
      "reward :  0.40901286910637946\n",
      "episode :  47\n",
      "current step :  120\n",
      "reward :  0.17283644069821116\n",
      "episode :  47\n",
      "current step :  125\n",
      "reward :  0.397598545230338\n",
      "episode :  47\n",
      "current step :  130\n",
      "reward :  0.4013556147186821\n",
      "episode :  47\n",
      "current step :  135\n",
      "reward :  0.4214084098506091\n",
      "episode :  47\n",
      "current step :  140\n",
      "reward :  0.4920408050219603\n",
      "episode :  47\n",
      "current step :  145\n",
      "reward :  0.39952106103723767\n",
      "Episode 47, Total Reward: 12.353929129685948\n",
      "episode :  48\n",
      "current step :  0\n",
      "reward :  0.19267574556350828\n",
      "episode :  48\n",
      "current step :  5\n",
      "reward :  0.4420033830881983\n",
      "episode :  48\n",
      "current step :  10\n",
      "reward :  0.5693946781627832\n",
      "episode :  48\n",
      "current step :  15\n",
      "reward :  0.5262014877741261\n",
      "episode :  48\n",
      "current step :  20\n",
      "reward :  0.4608949167792342\n",
      "episode :  48\n",
      "current step :  25\n",
      "reward :  0.4655670216221947\n",
      "episode :  48\n",
      "current step :  30\n",
      "reward :  0.6213950185994638\n",
      "episode :  48\n",
      "current step :  35\n",
      "reward :  0.2599120921713748\n",
      "episode :  48\n",
      "current step :  40\n",
      "reward :  0.31071222689032024\n",
      "episode :  48\n",
      "current step :  45\n",
      "reward :  0.27022800937601704\n",
      "episode :  48\n",
      "current step :  50\n",
      "reward :  0.4686170255539909\n",
      "episode :  48\n",
      "current step :  55\n",
      "reward :  0.21310104198693922\n",
      "episode :  48\n",
      "current step :  60\n",
      "reward :  0.42714130731435374\n",
      "episode :  48\n",
      "current step :  65\n",
      "reward :  0.1846273999650214\n",
      "episode :  48\n",
      "current step :  70\n",
      "reward :  0.5223008916198314\n",
      "episode :  48\n",
      "current step :  75\n",
      "reward :  0.38475767207530126\n",
      "episode :  48\n",
      "current step :  80\n",
      "reward :  0.5736926911260272\n",
      "episode :  48\n",
      "current step :  85\n",
      "reward :  0.2824984760769366\n",
      "episode :  48\n",
      "current step :  90\n",
      "reward :  0.4442218685905366\n",
      "episode :  48\n",
      "current step :  95\n",
      "reward :  0.4154702037479341\n",
      "episode :  48\n",
      "current step :  100\n",
      "reward :  0.36373059839513316\n",
      "episode :  48\n",
      "current step :  105\n",
      "reward :  0.4981099494460155\n",
      "episode :  48\n",
      "current step :  110\n",
      "reward :  0.4642921332748762\n",
      "episode :  48\n",
      "current step :  115\n",
      "reward :  0.524969252183445\n",
      "episode :  48\n",
      "current step :  120\n",
      "reward :  0.4608213707732723\n",
      "episode :  48\n",
      "current step :  125\n",
      "reward :  0.4171913452332846\n",
      "episode :  48\n",
      "current step :  130\n",
      "reward :  0.41287698937845596\n",
      "episode :  48\n",
      "current step :  135\n",
      "reward :  0.48295040921471066\n",
      "episode :  48\n",
      "current step :  140\n",
      "reward :  0.4821642686657092\n",
      "episode :  48\n",
      "current step :  145\n",
      "reward :  0.4228951366118203\n",
      "Episode 48, Total Reward: 12.565414611260815\n",
      "episode :  49\n",
      "current step :  0\n",
      "reward :  0.25533778599768825\n",
      "episode :  49\n",
      "current step :  5\n",
      "reward :  0.544852553110406\n",
      "episode :  49\n",
      "current step :  10\n",
      "reward :  0.37664565558936597\n",
      "episode :  49\n",
      "current step :  15\n",
      "reward :  0.5049391349491501\n",
      "episode :  49\n",
      "current step :  20\n",
      "reward :  0.3747972222351335\n",
      "episode :  49\n",
      "current step :  25\n",
      "reward :  0.4752627563089695\n",
      "episode :  49\n",
      "current step :  30\n",
      "reward :  0.6141759327405172\n",
      "episode :  49\n",
      "current step :  35\n",
      "reward :  0.19795065429792855\n",
      "episode :  49\n",
      "current step :  40\n",
      "reward :  0.6001385925650461\n",
      "episode :  49\n",
      "current step :  45\n",
      "reward :  0.21171309949442133\n",
      "episode :  49\n",
      "current step :  50\n",
      "reward :  0.24570514514105135\n",
      "episode :  49\n",
      "current step :  55\n",
      "reward :  0.214850503939843\n",
      "episode :  49\n",
      "current step :  60\n",
      "reward :  0.19964632208717392\n",
      "episode :  49\n",
      "current step :  65\n",
      "reward :  0.2777033433404422\n",
      "episode :  49\n",
      "current step :  70\n",
      "reward :  0.390789173901561\n",
      "episode :  49\n",
      "current step :  75\n",
      "reward :  0.30300198654261495\n",
      "episode :  49\n",
      "current step :  80\n",
      "reward :  0.5116941926671934\n",
      "episode :  49\n",
      "current step :  85\n",
      "reward :  0.3109470832020291\n",
      "episode :  49\n",
      "current step :  90\n",
      "reward :  0.2230189522810946\n",
      "episode :  49\n",
      "current step :  95\n",
      "reward :  0.3757337149880946\n",
      "episode :  49\n",
      "current step :  100\n",
      "reward :  0.6062886015599347\n",
      "episode :  49\n",
      "current step :  105\n",
      "reward :  0.47001706617889466\n",
      "episode :  49\n",
      "current step :  110\n",
      "reward :  0.42603896150898296\n",
      "episode :  49\n",
      "current step :  115\n",
      "reward :  0.4468115886095956\n",
      "episode :  49\n",
      "current step :  120\n",
      "reward :  0.2349876609953219\n",
      "episode :  49\n",
      "current step :  125\n",
      "reward :  0.5035780268870642\n",
      "episode :  49\n",
      "current step :  130\n",
      "reward :  0.49883049987702427\n",
      "episode :  49\n",
      "current step :  135\n",
      "reward :  0.4854975241896935\n",
      "episode :  49\n",
      "current step :  140\n",
      "reward :  0.3988598236643839\n",
      "episode :  49\n",
      "current step :  145\n",
      "reward :  0.41693420972286255\n",
      "Episode 49, Total Reward: 11.696747768573484\n",
      "episode :  50\n",
      "current step :  0\n",
      "reward :  0.5325373910095579\n",
      "episode :  50\n",
      "current step :  5\n",
      "reward :  0.5290879256269796\n",
      "episode :  50\n",
      "current step :  10\n",
      "reward :  0.17694516408312996\n",
      "episode :  50\n",
      "current step :  15\n",
      "reward :  0.24906770700786887\n",
      "episode :  50\n",
      "current step :  20\n",
      "reward :  0.4678809733749596\n",
      "episode :  50\n",
      "current step :  25\n",
      "reward :  0.38161141064091153\n",
      "episode :  50\n",
      "current step :  30\n",
      "reward :  0.6164297487442603\n",
      "episode :  50\n",
      "current step :  35\n",
      "reward :  0.4171371335599632\n",
      "episode :  50\n",
      "current step :  40\n",
      "reward :  0.3586513470039702\n",
      "episode :  50\n",
      "current step :  45\n",
      "reward :  0.2513288002744303\n",
      "episode :  50\n",
      "current step :  50\n",
      "reward :  0.5545264467099422\n",
      "episode :  50\n",
      "current step :  55\n",
      "reward :  0.21939239328025592\n",
      "episode :  50\n",
      "current step :  60\n",
      "reward :  0.2371461365769137\n",
      "episode :  50\n",
      "current step :  65\n",
      "reward :  0.17668448931277364\n",
      "episode :  50\n",
      "current step :  70\n",
      "reward :  0.4898286063700841\n",
      "episode :  50\n",
      "current step :  75\n",
      "reward :  0.38869790295973783\n",
      "episode :  50\n",
      "current step :  80\n",
      "reward :  0.48536049645940493\n",
      "episode :  50\n",
      "current step :  85\n",
      "reward :  0.44326311334731133\n",
      "episode :  50\n",
      "current step :  90\n",
      "reward :  0.4841010280150084\n",
      "episode :  50\n",
      "current step :  95\n",
      "reward :  0.4232748912727635\n",
      "episode :  50\n",
      "current step :  100\n",
      "reward :  0.3878559358308272\n",
      "episode :  50\n",
      "current step :  105\n",
      "reward :  0.4706329585020102\n",
      "episode :  50\n",
      "current step :  110\n",
      "reward :  0.4620087121161818\n",
      "episode :  50\n",
      "current step :  115\n",
      "reward :  0.4004076701914681\n",
      "episode :  50\n",
      "current step :  120\n",
      "reward :  0.42112329704961393\n",
      "episode :  50\n",
      "current step :  125\n",
      "reward :  0.33741617747113606\n",
      "episode :  50\n",
      "current step :  130\n",
      "reward :  0.465953566283725\n",
      "episode :  50\n",
      "current step :  135\n",
      "reward :  0.17611493801147535\n",
      "episode :  50\n",
      "current step :  140\n",
      "reward :  0.22690272302088968\n",
      "episode :  50\n",
      "current step :  145\n",
      "reward :  0.39723900222552155\n",
      "Episode 50, Total Reward: 11.628608086333074\n",
      "episode :  51\n",
      "current step :  0\n",
      "reward :  0.4573520347778798\n",
      "episode :  51\n",
      "current step :  5\n",
      "reward :  0.412802344624183\n",
      "episode :  51\n",
      "current step :  10\n",
      "reward :  0.5344755842356657\n",
      "episode :  51\n",
      "current step :  15\n",
      "reward :  0.611150536559151\n",
      "episode :  51\n",
      "current step :  20\n",
      "reward :  0.5676220134085133\n",
      "episode :  51\n",
      "current step :  25\n",
      "reward :  0.3776304887814762\n",
      "episode :  51\n",
      "current step :  30\n",
      "reward :  0.5016290340878773\n",
      "episode :  51\n",
      "current step :  35\n",
      "reward :  0.30213416291050366\n",
      "episode :  51\n",
      "current step :  40\n",
      "reward :  0.2724139451989313\n",
      "episode :  51\n",
      "current step :  45\n",
      "reward :  0.555724282926526\n",
      "episode :  51\n",
      "current step :  50\n",
      "reward :  0.23998839116124507\n",
      "episode :  51\n",
      "current step :  55\n",
      "reward :  0.37384884244652195\n",
      "episode :  51\n",
      "current step :  60\n",
      "reward :  0.2801081092433612\n",
      "episode :  51\n",
      "current step :  65\n",
      "reward :  0.18497509346556526\n",
      "episode :  51\n",
      "current step :  70\n",
      "reward :  0.23561476315644503\n",
      "episode :  51\n",
      "current step :  75\n",
      "reward :  0.33471962626377527\n",
      "episode :  51\n",
      "current step :  80\n",
      "reward :  0.36504623271610215\n",
      "episode :  51\n",
      "current step :  85\n",
      "reward :  0.5086957326791086\n",
      "episode :  51\n",
      "current step :  90\n",
      "reward :  0.3724267833752312\n",
      "episode :  51\n",
      "current step :  95\n",
      "reward :  0.38038800779738013\n",
      "episode :  51\n",
      "current step :  100\n",
      "reward :  0.45309646835551864\n",
      "episode :  51\n",
      "current step :  105\n",
      "reward :  0.3789344656459659\n",
      "episode :  51\n",
      "current step :  110\n",
      "reward :  0.4062796155524962\n",
      "episode :  51\n",
      "current step :  115\n",
      "reward :  0.4417657955338219\n",
      "episode :  51\n",
      "current step :  120\n",
      "reward :  0.4953876765766674\n",
      "episode :  51\n",
      "current step :  125\n",
      "reward :  0.462972404107694\n",
      "episode :  51\n",
      "current step :  130\n",
      "reward :  0.5093909822279263\n",
      "episode :  51\n",
      "current step :  135\n",
      "reward :  0.4892520487442553\n",
      "episode :  51\n",
      "current step :  140\n",
      "reward :  0.5450883604999774\n",
      "episode :  51\n",
      "current step :  145\n",
      "reward :  0.4193796880058226\n",
      "Episode 51, Total Reward: 12.470293515065588\n",
      "episode :  52\n",
      "current step :  0\n",
      "reward :  0.22755568845996862\n",
      "episode :  52\n",
      "current step :  5\n",
      "reward :  0.38885456210030966\n",
      "episode :  52\n",
      "current step :  10\n",
      "reward :  0.41750301195778944\n",
      "episode :  52\n",
      "current step :  15\n",
      "reward :  0.42157005122529567\n",
      "episode :  52\n",
      "current step :  20\n",
      "reward :  0.42346805903494533\n",
      "episode :  52\n",
      "current step :  25\n",
      "reward :  0.4737830524071196\n",
      "episode :  52\n",
      "current step :  30\n",
      "reward :  0.51808906439425\n",
      "episode :  52\n",
      "current step :  35\n",
      "reward :  0.5215877810542531\n",
      "episode :  52\n",
      "current step :  40\n",
      "reward :  0.488183392674882\n",
      "episode :  52\n",
      "current step :  45\n",
      "reward :  0.2815747221970561\n",
      "episode :  52\n",
      "current step :  50\n",
      "reward :  0.32707727977493456\n",
      "episode :  52\n",
      "current step :  55\n",
      "reward :  0.39965241301193893\n",
      "episode :  52\n",
      "current step :  60\n",
      "reward :  0.18829605918492834\n",
      "episode :  52\n",
      "current step :  65\n",
      "reward :  0.21268876996359612\n",
      "episode :  52\n",
      "current step :  70\n",
      "reward :  0.2596135598642219\n",
      "episode :  52\n",
      "current step :  75\n",
      "reward :  0.3088412228297061\n",
      "episode :  52\n",
      "current step :  80\n",
      "reward :  0.5330093188108161\n",
      "episode :  52\n",
      "current step :  85\n",
      "reward :  0.5103616480434835\n",
      "episode :  52\n",
      "current step :  90\n",
      "reward :  0.2925613061835223\n",
      "episode :  52\n",
      "current step :  95\n",
      "reward :  0.3632002562613338\n",
      "episode :  52\n",
      "current step :  100\n",
      "reward :  0.447728415811882\n",
      "episode :  52\n",
      "current step :  105\n",
      "reward :  0.4410082504701539\n",
      "episode :  52\n",
      "current step :  110\n",
      "reward :  0.40079052449030994\n",
      "episode :  52\n",
      "current step :  115\n",
      "reward :  0.4631192242322034\n",
      "episode :  52\n",
      "current step :  120\n",
      "reward :  0.41937657054230393\n",
      "episode :  52\n",
      "current step :  125\n",
      "reward :  0.43722981995106275\n",
      "episode :  52\n",
      "current step :  130\n",
      "reward :  0.3789827266476471\n",
      "episode :  52\n",
      "current step :  135\n",
      "reward :  0.41902106218119983\n",
      "episode :  52\n",
      "current step :  140\n",
      "reward :  0.4951206387289065\n",
      "episode :  52\n",
      "current step :  145\n",
      "reward :  0.4077667668621958\n",
      "Episode 52, Total Reward: 11.867615219352214\n",
      "episode :  53\n",
      "current step :  0\n",
      "reward :  0.47522083203861404\n",
      "episode :  53\n",
      "current step :  5\n",
      "reward :  0.38799647893055167\n",
      "episode :  53\n",
      "current step :  10\n",
      "reward :  0.3433340590074366\n",
      "episode :  53\n",
      "current step :  15\n",
      "reward :  0.4568309265735327\n",
      "episode :  53\n",
      "current step :  20\n",
      "reward :  0.6389285713851591\n",
      "episode :  53\n",
      "current step :  25\n",
      "reward :  0.5667190574697893\n",
      "episode :  53\n",
      "current step :  30\n",
      "reward :  0.46542416607600584\n",
      "episode :  53\n",
      "current step :  35\n",
      "reward :  0.3026268919417613\n",
      "episode :  53\n",
      "current step :  40\n",
      "reward :  0.29869825407830614\n",
      "episode :  53\n",
      "current step :  45\n",
      "reward :  0.5438004417278629\n",
      "episode :  53\n",
      "current step :  50\n",
      "reward :  0.3875868551366572\n",
      "episode :  53\n",
      "current step :  55\n",
      "reward :  0.40354822879714125\n",
      "episode :  53\n",
      "current step :  60\n",
      "reward :  0.1719794259143768\n",
      "episode :  53\n",
      "current step :  65\n",
      "reward :  0.3179246173741288\n",
      "episode :  53\n",
      "current step :  70\n",
      "reward :  0.38293012222048045\n",
      "episode :  53\n",
      "current step :  75\n",
      "reward :  0.42519615932129\n",
      "episode :  53\n",
      "current step :  80\n",
      "reward :  0.34598432384342703\n",
      "episode :  53\n",
      "current step :  85\n",
      "reward :  0.425681509117592\n",
      "episode :  53\n",
      "current step :  90\n",
      "reward :  0.4643940390829009\n",
      "episode :  53\n",
      "current step :  95\n",
      "reward :  0.5061195170932513\n",
      "episode :  53\n",
      "current step :  100\n",
      "reward :  0.3492911981921031\n",
      "episode :  53\n",
      "current step :  105\n",
      "reward :  0.5069998166489493\n",
      "episode :  53\n",
      "current step :  110\n",
      "reward :  0.42783899561084565\n",
      "episode :  53\n",
      "current step :  115\n",
      "reward :  0.40530421629293834\n",
      "episode :  53\n",
      "current step :  120\n",
      "reward :  0.20117570014972316\n",
      "episode :  53\n",
      "current step :  125\n",
      "reward :  0.3506547526991593\n",
      "episode :  53\n",
      "current step :  130\n",
      "reward :  0.4734952643393401\n",
      "episode :  53\n",
      "current step :  135\n",
      "reward :  0.42184679936056574\n",
      "episode :  53\n",
      "current step :  140\n",
      "reward :  0.44471031473233563\n",
      "episode :  53\n",
      "current step :  145\n",
      "reward :  0.47016671898818857\n",
      "Episode 53, Total Reward: 12.362408254144412\n",
      "episode :  54\n",
      "current step :  0\n",
      "reward :  0.6316634798198194\n",
      "episode :  54\n",
      "current step :  5\n",
      "reward :  0.41704384473704315\n",
      "episode :  54\n",
      "current step :  10\n",
      "reward :  0.4458809650491956\n",
      "episode :  54\n",
      "current step :  15\n",
      "reward :  0.44937542736164604\n",
      "episode :  54\n",
      "current step :  20\n",
      "reward :  0.38658051945824257\n",
      "episode :  54\n",
      "current step :  25\n",
      "reward :  0.4854881364919294\n",
      "episode :  54\n",
      "current step :  30\n",
      "reward :  0.27330429497202974\n",
      "episode :  54\n",
      "current step :  35\n",
      "reward :  0.4729547922783658\n",
      "episode :  54\n",
      "current step :  40\n",
      "reward :  0.290841266284586\n",
      "episode :  54\n",
      "current step :  45\n",
      "reward :  0.40432131313956454\n",
      "episode :  54\n",
      "current step :  50\n",
      "reward :  0.25616993925482134\n",
      "episode :  54\n",
      "current step :  55\n",
      "reward :  0.5012157266714989\n",
      "episode :  54\n",
      "current step :  60\n",
      "reward :  0.37944074311277204\n",
      "episode :  54\n",
      "current step :  65\n",
      "reward :  0.20928957879141746\n",
      "episode :  54\n",
      "current step :  70\n",
      "reward :  0.28642661611570513\n",
      "episode :  54\n",
      "current step :  75\n",
      "reward :  0.4235553401056126\n",
      "episode :  54\n",
      "current step :  80\n",
      "reward :  0.41854651608273896\n",
      "episode :  54\n",
      "current step :  85\n",
      "reward :  0.5246886421436343\n",
      "episode :  54\n",
      "current step :  90\n",
      "reward :  0.4805514941441962\n",
      "episode :  54\n",
      "current step :  95\n",
      "reward :  0.43945236600315796\n",
      "episode :  54\n",
      "current step :  100\n",
      "reward :  0.5290361355141214\n",
      "episode :  54\n",
      "current step :  105\n",
      "reward :  0.4368871049827965\n",
      "episode :  54\n",
      "current step :  110\n",
      "reward :  0.3866579986342641\n",
      "episode :  54\n",
      "current step :  115\n",
      "reward :  0.28097341491806443\n",
      "episode :  54\n",
      "current step :  120\n",
      "reward :  0.21709915231451193\n",
      "episode :  54\n",
      "current step :  125\n",
      "reward :  0.4105100023686235\n",
      "episode :  54\n",
      "current step :  130\n",
      "reward :  0.4313162327198856\n",
      "episode :  54\n",
      "current step :  135\n",
      "reward :  0.47595648615647146\n",
      "episode :  54\n",
      "current step :  140\n",
      "reward :  0.6155083840332431\n",
      "episode :  54\n",
      "current step :  145\n",
      "reward :  0.38529279075376316\n",
      "Episode 54, Total Reward: 12.34602870441372\n",
      "episode :  55\n",
      "current step :  0\n",
      "reward :  0.414437074322368\n",
      "episode :  55\n",
      "current step :  5\n",
      "reward :  0.3934820159541118\n",
      "episode :  55\n",
      "current step :  10\n",
      "reward :  0.47922127043576446\n",
      "episode :  55\n",
      "current step :  15\n",
      "reward :  0.5390443531233261\n",
      "episode :  55\n",
      "current step :  20\n",
      "reward :  0.4113438609265103\n",
      "episode :  55\n",
      "current step :  25\n",
      "reward :  0.24901641578290654\n",
      "episode :  55\n",
      "current step :  30\n",
      "reward :  0.45762325565436773\n",
      "episode :  55\n",
      "current step :  35\n",
      "reward :  0.23815146958269595\n",
      "episode :  55\n",
      "current step :  40\n",
      "reward :  0.36119065663013283\n",
      "episode :  55\n",
      "current step :  45\n",
      "reward :  0.27986443374776804\n",
      "episode :  55\n",
      "current step :  50\n",
      "reward :  0.46753972629517626\n",
      "episode :  55\n",
      "current step :  55\n",
      "reward :  0.37791612090776877\n",
      "episode :  55\n",
      "current step :  60\n",
      "reward :  0.20245869317896178\n",
      "episode :  55\n",
      "current step :  65\n",
      "reward :  0.1934012580568414\n",
      "episode :  55\n",
      "current step :  70\n",
      "reward :  0.31536618105908265\n",
      "episode :  55\n",
      "current step :  75\n",
      "reward :  0.2452772078767846\n",
      "episode :  55\n",
      "current step :  80\n",
      "reward :  0.2879623624841102\n",
      "episode :  55\n",
      "current step :  85\n",
      "reward :  0.21178388743027474\n",
      "episode :  55\n",
      "current step :  90\n",
      "reward :  0.0\n",
      "episode :  55\n",
      "current step :  95\n",
      "reward :  0.3680307082095703\n",
      "episode :  55\n",
      "current step :  100\n",
      "reward :  0.40327317063427215\n",
      "episode :  55\n",
      "current step :  105\n",
      "reward :  0.42179828062815894\n",
      "episode :  55\n",
      "current step :  110\n",
      "reward :  0.48899097399766756\n",
      "episode :  55\n",
      "current step :  115\n",
      "reward :  0.4263501935714733\n",
      "episode :  55\n",
      "current step :  120\n",
      "reward :  0.368230312354195\n",
      "episode :  55\n",
      "current step :  125\n",
      "reward :  0.46832970492082093\n",
      "episode :  55\n",
      "current step :  130\n",
      "reward :  0.4086422450391579\n",
      "episode :  55\n",
      "current step :  135\n",
      "reward :  0.4148658014668507\n",
      "episode :  55\n",
      "current step :  140\n",
      "reward :  0.37740211451118366\n",
      "episode :  55\n",
      "current step :  145\n",
      "reward :  0.34744274685836257\n",
      "Episode 55, Total Reward: 10.618436495640665\n",
      "episode :  56\n",
      "current step :  0\n",
      "reward :  0.4962404877472072\n",
      "episode :  56\n",
      "current step :  5\n",
      "reward :  0.3752204850525047\n",
      "episode :  56\n",
      "current step :  10\n",
      "reward :  0.3893096252384143\n",
      "episode :  56\n",
      "current step :  15\n",
      "reward :  0.44331057020403186\n",
      "episode :  56\n",
      "current step :  20\n",
      "reward :  0.4756512831525507\n",
      "episode :  56\n",
      "current step :  25\n",
      "reward :  0.3635578220530731\n",
      "episode :  56\n",
      "current step :  30\n",
      "reward :  0.48642173897504926\n",
      "episode :  56\n",
      "current step :  35\n",
      "reward :  0.2965769516703095\n",
      "episode :  56\n",
      "current step :  40\n",
      "reward :  0.29157311390304474\n",
      "episode :  56\n",
      "current step :  45\n",
      "reward :  0.41350924882525064\n",
      "episode :  56\n",
      "current step :  50\n",
      "reward :  0.5014096318839261\n",
      "episode :  56\n",
      "current step :  55\n",
      "reward :  0.2749836778002851\n",
      "episode :  56\n",
      "current step :  60\n",
      "reward :  0.2369554866934552\n",
      "episode :  56\n",
      "current step :  65\n",
      "reward :  0.1843643776916861\n",
      "episode :  56\n",
      "current step :  70\n",
      "reward :  0.5313398196302576\n",
      "episode :  56\n",
      "current step :  75\n",
      "reward :  0.36127727861499787\n",
      "episode :  56\n",
      "current step :  80\n",
      "reward :  0.34045854209437104\n",
      "episode :  56\n",
      "current step :  85\n",
      "reward :  0.5252457326342441\n",
      "episode :  56\n",
      "current step :  90\n",
      "reward :  0.45454563343851484\n",
      "episode :  56\n",
      "current step :  95\n",
      "reward :  0.43494047300379207\n",
      "episode :  56\n",
      "current step :  100\n",
      "reward :  0.45856230828248246\n",
      "episode :  56\n",
      "current step :  105\n",
      "reward :  0.3630757817799698\n",
      "episode :  56\n",
      "current step :  110\n",
      "reward :  0.41649044808727387\n",
      "episode :  56\n",
      "current step :  115\n",
      "reward :  0.45479441755947525\n",
      "episode :  56\n",
      "current step :  120\n",
      "reward :  0.4535045178065764\n",
      "episode :  56\n",
      "current step :  125\n",
      "reward :  0.4102930760239763\n",
      "episode :  56\n",
      "current step :  130\n",
      "reward :  0.4493507320546728\n",
      "episode :  56\n",
      "current step :  135\n",
      "reward :  0.38781662848235543\n",
      "episode :  56\n",
      "current step :  140\n",
      "reward :  0.4713465406285735\n",
      "episode :  56\n",
      "current step :  145\n",
      "reward :  0.465719049343822\n",
      "Episode 56, Total Reward: 12.207845480356141\n",
      "episode :  57\n",
      "current step :  0\n",
      "reward :  0.4402035488645045\n",
      "episode :  57\n",
      "current step :  5\n",
      "reward :  0.34760647933629707\n",
      "episode :  57\n",
      "current step :  10\n",
      "reward :  0.39390892108780984\n",
      "episode :  57\n",
      "current step :  15\n",
      "reward :  0.41475024105184716\n",
      "episode :  57\n",
      "current step :  20\n",
      "reward :  0.5145531729649389\n",
      "episode :  57\n",
      "current step :  25\n",
      "reward :  0.45658127360548195\n",
      "episode :  57\n",
      "current step :  30\n",
      "reward :  0.2400784310873212\n",
      "episode :  57\n",
      "current step :  35\n",
      "reward :  0.23470477941059084\n",
      "episode :  57\n",
      "current step :  40\n",
      "reward :  0.3091640427402072\n",
      "episode :  57\n",
      "current step :  45\n",
      "reward :  0.5389351713815859\n",
      "episode :  57\n",
      "current step :  50\n",
      "reward :  0.36255510131965835\n",
      "episode :  57\n",
      "current step :  55\n",
      "reward :  0.28620544875653536\n",
      "episode :  57\n",
      "current step :  60\n",
      "reward :  0.3583297164526333\n",
      "episode :  57\n",
      "current step :  65\n",
      "reward :  0.28217658703633935\n",
      "episode :  57\n",
      "current step :  70\n",
      "reward :  0.38959199425318247\n",
      "episode :  57\n",
      "current step :  75\n",
      "reward :  0.28323472434657826\n",
      "episode :  57\n",
      "current step :  80\n",
      "reward :  0.46846785701291005\n",
      "episode :  57\n",
      "current step :  85\n",
      "reward :  0.3157968922987575\n",
      "episode :  57\n",
      "current step :  90\n",
      "reward :  0.48093136414900556\n",
      "episode :  57\n",
      "current step :  95\n",
      "reward :  0.4285732717417561\n",
      "episode :  57\n",
      "current step :  100\n",
      "reward :  0.5035162801169242\n",
      "episode :  57\n",
      "current step :  105\n",
      "reward :  0.44085693613180654\n",
      "episode :  57\n",
      "current step :  110\n",
      "reward :  0.20933497233727763\n",
      "episode :  57\n",
      "current step :  115\n",
      "reward :  0.4756095393826526\n",
      "episode :  57\n",
      "current step :  120\n",
      "reward :  0.4228447257013953\n",
      "episode :  57\n",
      "current step :  125\n",
      "reward :  0.5440772087408148\n",
      "episode :  57\n",
      "current step :  130\n",
      "reward :  0.43311687228971074\n",
      "episode :  57\n",
      "current step :  135\n",
      "reward :  0.49534055683820954\n",
      "episode :  57\n",
      "current step :  140\n",
      "reward :  0.4581593239331404\n",
      "episode :  57\n",
      "current step :  145\n",
      "reward :  0.3452004406506355\n",
      "Episode 57, Total Reward: 11.874405875020509\n",
      "episode :  58\n",
      "current step :  0\n",
      "reward :  0.24872612538068087\n",
      "episode :  58\n",
      "current step :  5\n",
      "reward :  0.42422579404393024\n",
      "episode :  58\n",
      "current step :  10\n",
      "reward :  0.518913833141976\n",
      "episode :  58\n",
      "current step :  15\n",
      "reward :  0.4584774682381678\n",
      "episode :  58\n",
      "current step :  20\n",
      "reward :  0.39283274535094515\n",
      "episode :  58\n",
      "current step :  25\n",
      "reward :  0.2378845828168045\n",
      "episode :  58\n",
      "current step :  30\n",
      "reward :  0.2438125305508325\n",
      "episode :  58\n",
      "current step :  35\n",
      "reward :  0.3409393089408575\n",
      "episode :  58\n",
      "current step :  40\n",
      "reward :  0.43528326152860136\n",
      "episode :  58\n",
      "current step :  45\n",
      "reward :  0.23324790240040139\n",
      "episode :  58\n",
      "current step :  50\n",
      "reward :  0.3079724059518369\n",
      "episode :  58\n",
      "current step :  55\n",
      "reward :  0.1754948365734308\n",
      "episode :  58\n",
      "current step :  60\n",
      "reward :  0.3164049110737317\n",
      "episode :  58\n",
      "current step :  65\n",
      "reward :  0.2860702089031701\n",
      "episode :  58\n",
      "current step :  70\n",
      "reward :  0.4138919891089792\n",
      "episode :  58\n",
      "current step :  75\n",
      "reward :  0.40394700738276\n",
      "episode :  58\n",
      "current step :  80\n",
      "reward :  0.4993705926461733\n",
      "episode :  58\n",
      "current step :  85\n",
      "reward :  0.2862926653889091\n",
      "episode :  58\n",
      "current step :  90\n",
      "reward :  0.48000375065821504\n",
      "episode :  58\n",
      "current step :  95\n",
      "reward :  0.46664812591715976\n",
      "episode :  58\n",
      "current step :  100\n",
      "reward :  0.4761999342026946\n",
      "episode :  58\n",
      "current step :  105\n",
      "reward :  0.4120960298134993\n",
      "episode :  58\n",
      "current step :  110\n",
      "reward :  0.4110063033136863\n",
      "episode :  58\n",
      "current step :  115\n",
      "reward :  0.40290227316538607\n",
      "episode :  58\n",
      "current step :  120\n",
      "reward :  0.4410908410904553\n",
      "episode :  58\n",
      "current step :  125\n",
      "reward :  0.4798980677154899\n",
      "episode :  58\n",
      "current step :  130\n",
      "reward :  0.4237614411749089\n",
      "episode :  58\n",
      "current step :  135\n",
      "reward :  0.4466339245415931\n",
      "episode :  58\n",
      "current step :  140\n",
      "reward :  0.48137474803688113\n",
      "episode :  58\n",
      "current step :  145\n",
      "reward :  0.36392703014035727\n",
      "Episode 58, Total Reward: 11.509330639192514\n",
      "episode :  59\n",
      "current step :  0\n",
      "reward :  0.2223280059468149\n",
      "episode :  59\n",
      "current step :  5\n",
      "reward :  0.38702431750682825\n",
      "episode :  59\n",
      "current step :  10\n",
      "reward :  0.5021654747437236\n",
      "episode :  59\n",
      "current step :  15\n",
      "reward :  0.37559237623247227\n",
      "episode :  59\n",
      "current step :  20\n",
      "reward :  0.5259411249059494\n",
      "episode :  59\n",
      "current step :  25\n",
      "reward :  0.3835893534574407\n",
      "episode :  59\n",
      "current step :  30\n",
      "reward :  0.4827979090849427\n",
      "episode :  59\n",
      "current step :  35\n",
      "reward :  0.2881231931622842\n",
      "episode :  59\n",
      "current step :  40\n",
      "reward :  0.46033877414966984\n",
      "episode :  59\n",
      "current step :  45\n",
      "reward :  0.37512711078464195\n",
      "episode :  59\n",
      "current step :  50\n",
      "reward :  0.49729504084290393\n",
      "episode :  59\n",
      "current step :  55\n",
      "reward :  0.19217233858291305\n",
      "episode :  59\n",
      "current step :  60\n",
      "reward :  0.3808792769765781\n",
      "episode :  59\n",
      "current step :  65\n",
      "reward :  0.23068089689181873\n",
      "episode :  59\n",
      "current step :  70\n",
      "reward :  0.46280782531659675\n",
      "episode :  59\n",
      "current step :  75\n",
      "reward :  0.5313641861837214\n",
      "episode :  59\n",
      "current step :  80\n",
      "reward :  0.5017274397483379\n",
      "episode :  59\n",
      "current step :  85\n",
      "reward :  0.568306509176103\n",
      "episode :  59\n",
      "current step :  90\n",
      "reward :  0.4072083554304911\n",
      "episode :  59\n",
      "current step :  95\n",
      "reward :  0.41899522344249057\n",
      "episode :  59\n",
      "current step :  100\n",
      "reward :  0.5518611571700387\n",
      "episode :  59\n",
      "current step :  105\n",
      "reward :  0.40738935958055167\n",
      "episode :  59\n",
      "current step :  110\n",
      "reward :  0.4863739792571904\n",
      "episode :  59\n",
      "current step :  115\n",
      "reward :  0.20227496077649215\n",
      "episode :  59\n",
      "current step :  120\n",
      "reward :  0.36028886093516665\n",
      "episode :  59\n",
      "current step :  125\n",
      "reward :  0.5064099660180577\n",
      "episode :  59\n",
      "current step :  130\n",
      "reward :  0.32566389090034636\n",
      "episode :  59\n",
      "current step :  135\n",
      "reward :  0.324637895035037\n",
      "episode :  59\n",
      "current step :  140\n",
      "reward :  0.4853349560121146\n",
      "episode :  59\n",
      "current step :  145\n",
      "reward :  0.4621900318366728\n",
      "Episode 59, Total Reward: 12.306889790088391\n",
      "episode :  60\n",
      "current step :  0\n",
      "reward :  0.25352302794392567\n",
      "episode :  60\n",
      "current step :  5\n",
      "reward :  0.4877124224667024\n",
      "episode :  60\n",
      "current step :  10\n",
      "reward :  0.45842778660018957\n",
      "episode :  60\n",
      "current step :  15\n",
      "reward :  0.49909055000251934\n",
      "episode :  60\n",
      "current step :  20\n",
      "reward :  0.2923141144492354\n",
      "episode :  60\n",
      "current step :  25\n",
      "reward :  0.43940054658461325\n",
      "episode :  60\n",
      "current step :  30\n",
      "reward :  0.5044701697200245\n",
      "episode :  60\n",
      "current step :  35\n",
      "reward :  0.32405665948791273\n",
      "episode :  60\n",
      "current step :  40\n",
      "reward :  0.3034753232272982\n",
      "episode :  60\n",
      "current step :  45\n",
      "reward :  0.22485464948003414\n",
      "episode :  60\n",
      "current step :  50\n",
      "reward :  0.45019900057771606\n",
      "episode :  60\n",
      "current step :  55\n",
      "reward :  0.466352845587605\n",
      "episode :  60\n",
      "current step :  60\n",
      "reward :  0.24720973705080462\n",
      "episode :  60\n",
      "current step :  65\n",
      "reward :  0.18268328176809182\n",
      "episode :  60\n",
      "current step :  70\n",
      "reward :  0.21869878330587067\n",
      "episode :  60\n",
      "current step :  75\n",
      "reward :  0.3861810146066896\n",
      "episode :  60\n",
      "current step :  80\n",
      "reward :  0.5384776459157617\n",
      "episode :  60\n",
      "current step :  85\n",
      "reward :  0.4508798548288057\n",
      "episode :  60\n",
      "current step :  90\n",
      "reward :  0.4276600223398464\n",
      "episode :  60\n",
      "current step :  95\n",
      "reward :  0.4220889631773731\n",
      "episode :  60\n",
      "current step :  100\n",
      "reward :  0.5323368297052549\n",
      "episode :  60\n",
      "current step :  105\n",
      "reward :  0.41096260979607735\n",
      "episode :  60\n",
      "current step :  110\n",
      "reward :  0.41905572545143954\n",
      "episode :  60\n",
      "current step :  115\n",
      "reward :  0.431714323106156\n",
      "episode :  60\n",
      "current step :  120\n",
      "reward :  0.43455765931733736\n",
      "episode :  60\n",
      "current step :  125\n",
      "reward :  0.40707137043594477\n",
      "episode :  60\n",
      "current step :  130\n",
      "reward :  0.4003419446846128\n",
      "episode :  60\n",
      "current step :  135\n",
      "reward :  0.4726300641033273\n",
      "episode :  60\n",
      "current step :  140\n",
      "reward :  0.18775622912809678\n",
      "episode :  60\n",
      "current step :  145\n",
      "reward :  0.4166195159063403\n",
      "Episode 60, Total Reward: 11.690802670755607\n",
      "episode :  61\n",
      "current step :  0\n",
      "reward :  0.5117654603139143\n",
      "episode :  61\n",
      "current step :  5\n",
      "reward :  0.41912615917904483\n",
      "episode :  61\n",
      "current step :  10\n",
      "reward :  0.4173027415192495\n",
      "episode :  61\n",
      "current step :  15\n",
      "reward :  0.6109238964774282\n",
      "episode :  61\n",
      "current step :  20\n",
      "reward :  0.4126027683959869\n",
      "episode :  61\n",
      "current step :  25\n",
      "reward :  0.3451450271881422\n",
      "episode :  61\n",
      "current step :  30\n",
      "reward :  0.5856475133126567\n",
      "episode :  61\n",
      "current step :  35\n",
      "reward :  0.19650301719760394\n",
      "episode :  61\n",
      "current step :  40\n",
      "reward :  0.2584749341673995\n",
      "episode :  61\n",
      "current step :  45\n",
      "reward :  0.5565003835672511\n",
      "episode :  61\n",
      "current step :  50\n",
      "reward :  0.28324284844464076\n",
      "episode :  61\n",
      "current step :  55\n",
      "reward :  0.4204016446446419\n",
      "episode :  61\n",
      "current step :  60\n",
      "reward :  0.2278883954205252\n",
      "episode :  61\n",
      "current step :  65\n",
      "reward :  0.4163126605309409\n",
      "episode :  61\n",
      "current step :  70\n",
      "reward :  0.3592275648916619\n",
      "episode :  61\n",
      "current step :  75\n",
      "reward :  0.34756854643548596\n",
      "episode :  61\n",
      "current step :  80\n",
      "reward :  0.5207225532088581\n",
      "episode :  61\n",
      "current step :  85\n",
      "reward :  0.600345298866797\n",
      "episode :  61\n",
      "current step :  90\n",
      "reward :  0.4349221006588577\n",
      "episode :  61\n",
      "current step :  95\n",
      "reward :  0.4932139533865066\n",
      "episode :  61\n",
      "current step :  100\n",
      "reward :  0.4109927030388896\n",
      "episode :  61\n",
      "current step :  105\n",
      "reward :  0.4618857542355668\n",
      "episode :  61\n",
      "current step :  110\n",
      "reward :  0.5050520654539595\n",
      "episode :  61\n",
      "current step :  115\n",
      "reward :  0.4195294300406017\n",
      "episode :  61\n",
      "current step :  120\n",
      "reward :  0.4103346540154791\n",
      "episode :  61\n",
      "current step :  125\n",
      "reward :  0.0\n",
      "episode :  61\n",
      "current step :  130\n",
      "reward :  0.49570952336414564\n",
      "episode :  61\n",
      "current step :  135\n",
      "reward :  0.47218779374088965\n",
      "episode :  61\n",
      "current step :  140\n",
      "reward :  0.47692895142113245\n",
      "episode :  61\n",
      "current step :  145\n",
      "reward :  0.4602536773219763\n",
      "Episode 61, Total Reward: 12.530712020440236\n",
      "episode :  62\n",
      "current step :  0\n",
      "reward :  0.34774402135548044\n",
      "episode :  62\n",
      "current step :  5\n",
      "reward :  0.4611888055027197\n",
      "episode :  62\n",
      "current step :  10\n",
      "reward :  0.4748258047323148\n",
      "episode :  62\n",
      "current step :  15\n",
      "reward :  0.483740592267886\n",
      "episode :  62\n",
      "current step :  20\n",
      "reward :  0.3659059521804338\n",
      "episode :  62\n",
      "current step :  25\n",
      "reward :  0.4362192893510496\n",
      "episode :  62\n",
      "current step :  30\n",
      "reward :  0.28762625650503443\n",
      "episode :  62\n",
      "current step :  35\n",
      "reward :  0.38048086982842444\n",
      "episode :  62\n",
      "current step :  40\n",
      "reward :  0.2780971684473697\n",
      "episode :  62\n",
      "current step :  45\n",
      "reward :  0.29318034698741857\n",
      "episode :  62\n",
      "current step :  50\n",
      "reward :  0.3008358569846227\n",
      "episode :  62\n",
      "current step :  55\n",
      "reward :  0.3919593284704997\n",
      "episode :  62\n",
      "current step :  60\n",
      "reward :  0.17981332483860238\n",
      "episode :  62\n",
      "current step :  65\n",
      "reward :  0.6082737251095384\n",
      "episode :  62\n",
      "current step :  70\n",
      "reward :  0.4573037564059711\n",
      "episode :  62\n",
      "current step :  75\n",
      "reward :  0.6245345939221025\n",
      "episode :  62\n",
      "current step :  80\n",
      "reward :  0.51092135165622\n",
      "episode :  62\n",
      "current step :  85\n",
      "reward :  0.4778722976740399\n",
      "episode :  62\n",
      "current step :  90\n",
      "reward :  0.37052168168081834\n",
      "episode :  62\n",
      "current step :  95\n",
      "reward :  0.42769726237365535\n",
      "episode :  62\n",
      "current step :  100\n",
      "reward :  0.3976573166722712\n",
      "episode :  62\n",
      "current step :  105\n",
      "reward :  0.46527261847758855\n",
      "episode :  62\n",
      "current step :  110\n",
      "reward :  0.4568979116016575\n",
      "episode :  62\n",
      "current step :  115\n",
      "reward :  0.4307729930608367\n",
      "episode :  62\n",
      "current step :  120\n",
      "reward :  0.41620124350303955\n",
      "episode :  62\n",
      "current step :  125\n",
      "reward :  0.3824377518310956\n",
      "episode :  62\n",
      "current step :  130\n",
      "reward :  0.43344388863712185\n",
      "episode :  62\n",
      "current step :  135\n",
      "reward :  0.3840918112103797\n",
      "episode :  62\n",
      "current step :  140\n",
      "reward :  0.5165322491486396\n",
      "episode :  62\n",
      "current step :  145\n",
      "reward :  0.420630088903328\n",
      "Episode 62, Total Reward: 12.462680159320161\n",
      "episode :  63\n",
      "current step :  0\n",
      "reward :  0.4088627343025828\n",
      "episode :  63\n",
      "current step :  5\n",
      "reward :  0.5412421225795001\n",
      "episode :  63\n",
      "current step :  10\n",
      "reward :  0.434335889986596\n",
      "episode :  63\n",
      "current step :  15\n",
      "reward :  0.550116397270192\n",
      "episode :  63\n",
      "current step :  20\n",
      "reward :  0.5795681651760654\n",
      "episode :  63\n",
      "current step :  25\n",
      "reward :  0.6177874788186611\n",
      "episode :  63\n",
      "current step :  30\n",
      "reward :  0.467697956401923\n",
      "episode :  63\n",
      "current step :  35\n",
      "reward :  0.5064351274713945\n",
      "episode :  63\n",
      "current step :  40\n",
      "reward :  0.3286531370889157\n",
      "episode :  63\n",
      "current step :  45\n",
      "reward :  0.26513668505502436\n",
      "episode :  63\n",
      "current step :  50\n",
      "reward :  0.5933736703418723\n",
      "episode :  63\n",
      "current step :  55\n",
      "reward :  0.2460588160655594\n",
      "episode :  63\n",
      "current step :  60\n",
      "reward :  0.3995624665030042\n",
      "episode :  63\n",
      "current step :  65\n",
      "reward :  0.22888377203987942\n",
      "episode :  63\n",
      "current step :  70\n",
      "reward :  0.440067690396995\n",
      "episode :  63\n",
      "current step :  75\n",
      "reward :  0.3590336965402666\n",
      "episode :  63\n",
      "current step :  80\n",
      "reward :  0.4912704592677383\n",
      "episode :  63\n",
      "current step :  85\n",
      "reward :  0.4434837672169131\n",
      "episode :  63\n",
      "current step :  90\n",
      "reward :  0.38945729898235837\n",
      "episode :  63\n",
      "current step :  95\n",
      "reward :  0.3868693720567946\n",
      "episode :  63\n",
      "current step :  100\n",
      "reward :  0.4527106318914766\n",
      "episode :  63\n",
      "current step :  105\n",
      "reward :  0.3631214890374508\n",
      "episode :  63\n",
      "current step :  110\n",
      "reward :  0.5206796788349297\n",
      "episode :  63\n",
      "current step :  115\n",
      "reward :  0.43663460206751936\n",
      "episode :  63\n",
      "current step :  120\n",
      "reward :  0.3556171564830516\n",
      "episode :  63\n",
      "current step :  125\n",
      "reward :  0.5099273843294214\n",
      "episode :  63\n",
      "current step :  130\n",
      "reward :  0.5170695273720328\n",
      "episode :  63\n",
      "current step :  135\n",
      "reward :  0.48301140495994416\n",
      "episode :  63\n",
      "current step :  140\n",
      "reward :  0.47571784108887316\n",
      "episode :  63\n",
      "current step :  145\n",
      "reward :  0.3519006261910418\n",
      "Episode 63, Total Reward: 13.144287045817975\n",
      "episode :  64\n",
      "current step :  0\n",
      "reward :  0.5089624993230071\n",
      "episode :  64\n",
      "current step :  5\n",
      "reward :  0.4227786268448961\n",
      "episode :  64\n",
      "current step :  10\n",
      "reward :  0.5014520050275275\n",
      "episode :  64\n",
      "current step :  15\n",
      "reward :  0.4435820400065137\n",
      "episode :  64\n",
      "current step :  20\n",
      "reward :  0.49159166419396294\n",
      "episode :  64\n",
      "current step :  25\n",
      "reward :  0.43968662885882726\n",
      "episode :  64\n",
      "current step :  30\n",
      "reward :  0.5337668975222529\n",
      "episode :  64\n",
      "current step :  35\n",
      "reward :  0.24471091858387783\n",
      "episode :  64\n",
      "current step :  40\n",
      "reward :  0.5038036537367288\n",
      "episode :  64\n",
      "current step :  45\n",
      "reward :  0.25794368059463535\n",
      "episode :  64\n",
      "current step :  50\n",
      "reward :  0.4068715696589845\n",
      "episode :  64\n",
      "current step :  55\n",
      "reward :  0.22758455733958816\n",
      "episode :  64\n",
      "current step :  60\n",
      "reward :  0.2063584559327387\n",
      "episode :  64\n",
      "current step :  65\n",
      "reward :  0.26964429535128454\n",
      "episode :  64\n",
      "current step :  70\n",
      "reward :  0.47490787768449305\n",
      "episode :  64\n",
      "current step :  75\n",
      "reward :  0.28281500958756256\n",
      "episode :  64\n",
      "current step :  80\n",
      "reward :  0.43842318744600717\n",
      "episode :  64\n",
      "current step :  85\n",
      "reward :  0.47689880221092734\n",
      "episode :  64\n",
      "current step :  90\n",
      "reward :  0.40771321284163214\n",
      "episode :  64\n",
      "current step :  95\n",
      "reward :  0.24073501120405572\n",
      "episode :  64\n",
      "current step :  100\n",
      "reward :  0.4583215312934492\n",
      "episode :  64\n",
      "current step :  105\n",
      "reward :  0.4092796039423947\n",
      "episode :  64\n",
      "current step :  110\n",
      "reward :  0.46340194248434186\n",
      "episode :  64\n",
      "current step :  115\n",
      "reward :  0.3725246719745817\n",
      "episode :  64\n",
      "current step :  120\n",
      "reward :  0.21582265719024832\n",
      "episode :  64\n",
      "current step :  125\n",
      "reward :  0.38770872299137216\n",
      "episode :  64\n",
      "current step :  130\n",
      "reward :  0.4166759050002039\n",
      "episode :  64\n",
      "current step :  135\n",
      "reward :  0.4353298012998966\n",
      "episode :  64\n",
      "current step :  140\n",
      "reward :  0.5392754411954791\n",
      "episode :  64\n",
      "current step :  145\n",
      "reward :  0.5056321102341184\n",
      "Episode 64, Total Reward: 11.984202981555589\n",
      "episode :  65\n",
      "current step :  0\n",
      "reward :  0.49229468534887194\n",
      "episode :  65\n",
      "current step :  5\n",
      "reward :  0.47828245258076996\n",
      "episode :  65\n",
      "current step :  10\n",
      "reward :  0.4005492931450696\n",
      "episode :  65\n",
      "current step :  15\n",
      "reward :  0.5116180204457343\n",
      "episode :  65\n",
      "current step :  20\n",
      "reward :  0.25245568773207366\n",
      "episode :  65\n",
      "current step :  25\n",
      "reward :  0.5104691730183799\n",
      "episode :  65\n",
      "current step :  30\n",
      "reward :  0.587342099535835\n",
      "episode :  65\n",
      "current step :  35\n",
      "reward :  0.4695129300404527\n",
      "episode :  65\n",
      "current step :  40\n",
      "reward :  0.3622404958327676\n",
      "episode :  65\n",
      "current step :  45\n",
      "reward :  0.2700393954901142\n",
      "episode :  65\n",
      "current step :  50\n",
      "reward :  0.5041272586533787\n",
      "episode :  65\n",
      "current step :  55\n",
      "reward :  0.26849452446957356\n",
      "episode :  65\n",
      "current step :  60\n",
      "reward :  0.1599543847959409\n",
      "episode :  65\n",
      "current step :  65\n",
      "reward :  0.280553994062542\n",
      "episode :  65\n",
      "current step :  70\n",
      "reward :  0.24859446758592438\n",
      "episode :  65\n",
      "current step :  75\n",
      "reward :  0.43495291639194517\n",
      "episode :  65\n",
      "current step :  80\n",
      "reward :  0.2689956903395355\n",
      "episode :  65\n",
      "current step :  85\n",
      "reward :  0.4396512022140055\n",
      "episode :  65\n",
      "current step :  90\n",
      "reward :  0.42249079033911385\n",
      "episode :  65\n",
      "current step :  95\n",
      "reward :  0.42590934146336695\n",
      "episode :  65\n",
      "current step :  100\n",
      "reward :  0.45174299778684035\n",
      "episode :  65\n",
      "current step :  105\n",
      "reward :  0.38598886675893596\n",
      "episode :  65\n",
      "current step :  110\n",
      "reward :  0.33959644471195527\n",
      "episode :  65\n",
      "current step :  115\n",
      "reward :  0.5364360967426842\n",
      "episode :  65\n",
      "current step :  120\n",
      "reward :  0.25664443935043446\n",
      "episode :  65\n",
      "current step :  125\n",
      "reward :  0.3838430874928026\n",
      "episode :  65\n",
      "current step :  130\n",
      "reward :  0.40946618280833624\n",
      "episode :  65\n",
      "current step :  135\n",
      "reward :  0.49756659980957424\n",
      "episode :  65\n",
      "current step :  140\n",
      "reward :  0.4535099247863109\n",
      "episode :  65\n",
      "current step :  145\n",
      "reward :  0.4253946600640899\n",
      "Episode 65, Total Reward: 11.928718103797356\n",
      "episode :  66\n",
      "current step :  0\n",
      "reward :  0.37384816273978544\n",
      "episode :  66\n",
      "current step :  5\n",
      "reward :  0.5595189876507308\n",
      "episode :  66\n",
      "current step :  10\n",
      "reward :  0.4565086341828922\n",
      "episode :  66\n",
      "current step :  15\n",
      "reward :  0.5233964670141664\n",
      "episode :  66\n",
      "current step :  20\n",
      "reward :  0.5209716469638362\n",
      "episode :  66\n",
      "current step :  25\n",
      "reward :  0.3854395078843784\n",
      "episode :  66\n",
      "current step :  30\n",
      "reward :  0.3972569426618779\n",
      "episode :  66\n",
      "current step :  35\n",
      "reward :  0.22809376836557302\n",
      "episode :  66\n",
      "current step :  40\n",
      "reward :  0.3144588323525962\n",
      "episode :  66\n",
      "current step :  45\n",
      "reward :  0.48790734920444456\n",
      "episode :  66\n",
      "current step :  50\n",
      "reward :  0.5513242892086576\n",
      "episode :  66\n",
      "current step :  55\n",
      "reward :  0.23574725707448294\n",
      "episode :  66\n",
      "current step :  60\n",
      "reward :  0.18831792798476668\n",
      "episode :  66\n",
      "current step :  65\n",
      "reward :  0.20090053037217334\n",
      "episode :  66\n",
      "current step :  70\n",
      "reward :  0.5001423116625463\n",
      "episode :  66\n",
      "current step :  75\n",
      "reward :  0.4321459664671624\n",
      "episode :  66\n",
      "current step :  80\n",
      "reward :  0.34104790509991617\n",
      "episode :  66\n",
      "current step :  85\n",
      "reward :  0.4826950998366697\n",
      "episode :  66\n",
      "current step :  90\n",
      "reward :  0.39143829515475737\n",
      "episode :  66\n",
      "current step :  95\n",
      "reward :  0.4804784720345533\n",
      "episode :  66\n",
      "current step :  100\n",
      "reward :  0.4073268085607008\n",
      "episode :  66\n",
      "current step :  105\n",
      "reward :  0.3860337326942122\n",
      "episode :  66\n",
      "current step :  110\n",
      "reward :  0.44592299955170306\n",
      "episode :  66\n",
      "current step :  115\n",
      "reward :  0.37890627863960996\n",
      "episode :  66\n",
      "current step :  120\n",
      "reward :  0.4563310181252287\n",
      "episode :  66\n",
      "current step :  125\n",
      "reward :  0.45710725373888583\n",
      "episode :  66\n",
      "current step :  130\n",
      "reward :  0.48826640878608846\n",
      "episode :  66\n",
      "current step :  135\n",
      "reward :  0.44788069546864195\n",
      "episode :  66\n",
      "current step :  140\n",
      "reward :  0.4086287718488273\n",
      "episode :  66\n",
      "current step :  145\n",
      "reward :  0.4645353111259063\n",
      "Episode 66, Total Reward: 12.392577632455772\n",
      "episode :  67\n",
      "current step :  0\n",
      "reward :  0.5585014747989724\n",
      "episode :  67\n",
      "current step :  5\n",
      "reward :  0.4656049079071849\n",
      "episode :  67\n",
      "current step :  10\n",
      "reward :  0.5355615336593506\n",
      "episode :  67\n",
      "current step :  15\n",
      "reward :  0.6173871337657086\n",
      "episode :  67\n",
      "current step :  20\n",
      "reward :  0.43276994868598906\n",
      "episode :  67\n",
      "current step :  25\n",
      "reward :  0.4209111395126236\n",
      "episode :  67\n",
      "current step :  30\n",
      "reward :  0.4087431816917938\n",
      "episode :  67\n",
      "current step :  35\n",
      "reward :  0.38300612799344347\n",
      "episode :  67\n",
      "current step :  40\n",
      "reward :  0.47862253692385226\n",
      "episode :  67\n",
      "current step :  45\n",
      "reward :  0.5498224971728976\n",
      "episode :  67\n",
      "current step :  50\n",
      "reward :  0.4446666451387681\n",
      "episode :  67\n",
      "current step :  55\n",
      "reward :  0.21907242043435954\n",
      "episode :  67\n",
      "current step :  60\n",
      "reward :  0.38303240570951524\n",
      "episode :  67\n",
      "current step :  65\n",
      "reward :  0.23081230247086423\n",
      "episode :  67\n",
      "current step :  70\n",
      "reward :  0.2995243542162793\n",
      "episode :  67\n",
      "current step :  75\n",
      "reward :  0.6163279623052957\n",
      "episode :  67\n",
      "current step :  80\n",
      "reward :  0.46786921140534665\n",
      "episode :  67\n",
      "current step :  85\n",
      "reward :  0.48298613340255975\n",
      "episode :  67\n",
      "current step :  90\n",
      "reward :  0.4150999689926572\n",
      "episode :  67\n",
      "current step :  95\n",
      "reward :  0.4297635742812279\n",
      "episode :  67\n",
      "current step :  100\n",
      "reward :  0.4412226857031265\n",
      "episode :  67\n",
      "current step :  105\n",
      "reward :  0.4050357930557673\n",
      "episode :  67\n",
      "current step :  110\n",
      "reward :  0.4435086056429183\n",
      "episode :  67\n",
      "current step :  115\n",
      "reward :  0.4031103533060251\n",
      "episode :  67\n",
      "current step :  120\n",
      "reward :  0.2111749534916639\n",
      "episode :  67\n",
      "current step :  125\n",
      "reward :  0.3791444497655981\n",
      "episode :  67\n",
      "current step :  130\n",
      "reward :  0.5349594181426576\n",
      "episode :  67\n",
      "current step :  135\n",
      "reward :  0.5236932912022123\n",
      "episode :  67\n",
      "current step :  140\n",
      "reward :  0.38063025121397054\n",
      "episode :  67\n",
      "current step :  145\n",
      "reward :  0.5712415954454295\n",
      "Episode 67, Total Reward: 13.133806857438056\n",
      "episode :  68\n",
      "current step :  0\n",
      "reward :  0.527304176322138\n",
      "episode :  68\n",
      "current step :  5\n",
      "reward :  0.5168782651599113\n",
      "episode :  68\n",
      "current step :  10\n",
      "reward :  0.502463320265331\n",
      "episode :  68\n",
      "current step :  15\n",
      "reward :  0.41163378101407044\n",
      "episode :  68\n",
      "current step :  20\n",
      "reward :  0.573907950884345\n",
      "episode :  68\n",
      "current step :  25\n",
      "reward :  0.3754454537356253\n",
      "episode :  68\n",
      "current step :  30\n",
      "reward :  0.279048143463078\n",
      "episode :  68\n",
      "current step :  35\n",
      "reward :  0.2892521793294419\n",
      "episode :  68\n",
      "current step :  40\n",
      "reward :  0.3644127651082048\n",
      "episode :  68\n",
      "current step :  45\n",
      "reward :  0.6130968736303898\n",
      "episode :  68\n",
      "current step :  50\n",
      "reward :  0.416456374585245\n",
      "episode :  68\n",
      "current step :  55\n",
      "reward :  0.4706342929982691\n",
      "episode :  68\n",
      "current step :  60\n",
      "reward :  0.4158836904906027\n",
      "episode :  68\n",
      "current step :  65\n",
      "reward :  0.20314888845724677\n",
      "episode :  68\n",
      "current step :  70\n",
      "reward :  0.21754259114767063\n",
      "episode :  68\n",
      "current step :  75\n",
      "reward :  0.4471327765277128\n",
      "episode :  68\n",
      "current step :  80\n",
      "reward :  0.5149553394807647\n",
      "episode :  68\n",
      "current step :  85\n",
      "reward :  0.4376077364148543\n",
      "episode :  68\n",
      "current step :  90\n",
      "reward :  0.2555113619901592\n",
      "episode :  68\n",
      "current step :  95\n",
      "reward :  0.4890079364452531\n",
      "episode :  68\n",
      "current step :  100\n",
      "reward :  0.3261462667860912\n",
      "episode :  68\n",
      "current step :  105\n",
      "reward :  0.5916950642430492\n",
      "episode :  68\n",
      "current step :  110\n",
      "reward :  0.47813372915420316\n",
      "episode :  68\n",
      "current step :  115\n",
      "reward :  0.3791382255618676\n",
      "episode :  68\n",
      "current step :  120\n",
      "reward :  0.47328853557741524\n",
      "episode :  68\n",
      "current step :  125\n",
      "reward :  0.255221851856114\n",
      "episode :  68\n",
      "current step :  130\n",
      "reward :  0.45780958682247586\n",
      "episode :  68\n",
      "current step :  135\n",
      "reward :  0.4067235030481618\n",
      "episode :  68\n",
      "current step :  140\n",
      "reward :  0.4262984741639516\n",
      "episode :  68\n",
      "current step :  145\n",
      "reward :  0.43442569413074694\n",
      "Episode 68, Total Reward: 12.55020482879439\n",
      "episode :  69\n",
      "current step :  0\n",
      "reward :  0.4706910774762328\n",
      "episode :  69\n",
      "current step :  5\n",
      "reward :  0.5886105961953025\n",
      "episode :  69\n",
      "current step :  10\n",
      "reward :  0.3779549526273419\n",
      "episode :  69\n",
      "current step :  15\n",
      "reward :  0.3802696392756126\n",
      "episode :  69\n",
      "current step :  20\n",
      "reward :  0.4789794442063113\n",
      "episode :  69\n",
      "current step :  25\n",
      "reward :  0.3302568818057864\n",
      "episode :  69\n",
      "current step :  30\n",
      "reward :  0.4381966463081704\n",
      "episode :  69\n",
      "current step :  35\n",
      "reward :  0.3522966411341575\n",
      "episode :  69\n",
      "current step :  40\n",
      "reward :  0.4319027618385611\n",
      "episode :  69\n",
      "current step :  45\n",
      "reward :  0.3352354791744765\n",
      "episode :  69\n",
      "current step :  50\n",
      "reward :  0.5153404144905545\n",
      "episode :  69\n",
      "current step :  55\n",
      "reward :  0.3519165787207301\n",
      "episode :  69\n",
      "current step :  60\n",
      "reward :  0.35739864997307097\n",
      "episode :  69\n",
      "current step :  65\n",
      "reward :  0.2633421336402685\n",
      "episode :  69\n",
      "current step :  70\n",
      "reward :  0.43115369231482714\n",
      "episode :  69\n",
      "current step :  75\n",
      "reward :  0.24849281905688822\n",
      "episode :  69\n",
      "current step :  80\n",
      "reward :  0.5463191462816939\n",
      "episode :  69\n",
      "current step :  85\n",
      "reward :  0.4930805139838107\n",
      "episode :  69\n",
      "current step :  90\n",
      "reward :  0.3542922061678934\n",
      "episode :  69\n",
      "current step :  95\n",
      "reward :  0.38703776999472933\n",
      "episode :  69\n",
      "current step :  100\n",
      "reward :  0.42701756374703903\n",
      "episode :  69\n",
      "current step :  105\n",
      "reward :  0.38499251134790846\n",
      "episode :  69\n",
      "current step :  110\n",
      "reward :  0.4202468251713475\n",
      "episode :  69\n",
      "current step :  115\n",
      "reward :  0.20291680207720322\n",
      "episode :  69\n",
      "current step :  120\n",
      "reward :  0.2855766193291499\n",
      "episode :  69\n",
      "current step :  125\n",
      "reward :  0.382037399015989\n",
      "episode :  69\n",
      "current step :  130\n",
      "reward :  0.4207366236101904\n",
      "episode :  69\n",
      "current step :  135\n",
      "reward :  0.45138949160484254\n",
      "episode :  69\n",
      "current step :  140\n",
      "reward :  0.5449647331800255\n",
      "episode :  69\n",
      "current step :  145\n",
      "reward :  0.4006376391245621\n",
      "Episode 69, Total Reward: 12.053284252874677\n",
      "episode :  70\n",
      "current step :  0\n",
      "reward :  0.4526117292198828\n",
      "episode :  70\n",
      "current step :  5\n",
      "reward :  0.174210285841818\n",
      "episode :  70\n",
      "current step :  10\n",
      "reward :  0.44243869354583865\n",
      "episode :  70\n",
      "current step :  15\n",
      "reward :  0.5414711382136762\n",
      "episode :  70\n",
      "current step :  20\n",
      "reward :  0.4936085759288439\n",
      "episode :  70\n",
      "current step :  25\n",
      "reward :  0.5439923919830308\n",
      "episode :  70\n",
      "current step :  30\n",
      "reward :  0.28502485030976527\n",
      "episode :  70\n",
      "current step :  35\n",
      "reward :  0.2707350357809015\n",
      "episode :  70\n",
      "current step :  40\n",
      "reward :  0.3157557077077576\n",
      "episode :  70\n",
      "current step :  45\n",
      "reward :  0.44669187667899135\n",
      "episode :  70\n",
      "current step :  50\n",
      "reward :  0.5279888835720913\n",
      "episode :  70\n",
      "current step :  55\n",
      "reward :  0.3263203571141972\n",
      "episode :  70\n",
      "current step :  60\n",
      "reward :  0.4379040769720964\n",
      "episode :  70\n",
      "current step :  65\n",
      "reward :  0.3640228768359752\n",
      "episode :  70\n",
      "current step :  70\n",
      "reward :  0.3208544705063705\n",
      "episode :  70\n",
      "current step :  75\n",
      "reward :  0.3698445132313324\n",
      "episode :  70\n",
      "current step :  80\n",
      "reward :  0.5225761777552616\n",
      "episode :  70\n",
      "current step :  85\n",
      "reward :  0.4783787066859618\n",
      "episode :  70\n",
      "current step :  90\n",
      "reward :  0.4852764262287972\n",
      "episode :  70\n",
      "current step :  95\n",
      "reward :  0.3534366971680502\n",
      "episode :  70\n",
      "current step :  100\n",
      "reward :  0.4749590505118343\n",
      "episode :  70\n",
      "current step :  105\n",
      "reward :  0.431218597947009\n",
      "episode :  70\n",
      "current step :  110\n",
      "reward :  0.4467119034283592\n",
      "episode :  70\n",
      "current step :  115\n",
      "reward :  0.15743415676387523\n",
      "episode :  70\n",
      "current step :  120\n",
      "reward :  0.3825691538559385\n",
      "episode :  70\n",
      "current step :  125\n",
      "reward :  0.4496358578116502\n",
      "episode :  70\n",
      "current step :  130\n",
      "reward :  0.5212398029550793\n",
      "episode :  70\n",
      "current step :  135\n",
      "reward :  0.5050419576301302\n",
      "episode :  70\n",
      "current step :  140\n",
      "reward :  0.4498146543694308\n",
      "episode :  70\n",
      "current step :  145\n",
      "reward :  0.4073513865939069\n",
      "Episode 70, Total Reward: 12.379119993147853\n",
      "episode :  71\n",
      "current step :  0\n",
      "reward :  0.534662528952592\n",
      "episode :  71\n",
      "current step :  5\n",
      "reward :  0.3880192381551154\n",
      "episode :  71\n",
      "current step :  10\n",
      "reward :  0.41446155316279243\n",
      "episode :  71\n",
      "current step :  15\n",
      "reward :  0.4004204297850117\n",
      "episode :  71\n",
      "current step :  20\n",
      "reward :  0.482955485076963\n",
      "episode :  71\n",
      "current step :  25\n",
      "reward :  0.5174205979993947\n",
      "episode :  71\n",
      "current step :  30\n",
      "reward :  0.4715315852142847\n",
      "episode :  71\n",
      "current step :  35\n",
      "reward :  0.20836142295184545\n",
      "episode :  71\n",
      "current step :  40\n",
      "reward :  0.2764222109154821\n",
      "episode :  71\n",
      "current step :  45\n",
      "reward :  0.45372581899929765\n",
      "episode :  71\n",
      "current step :  50\n",
      "reward :  0.4726412714042517\n",
      "episode :  71\n",
      "current step :  55\n",
      "reward :  0.28417418860125526\n",
      "episode :  71\n",
      "current step :  60\n",
      "reward :  0.27858659749470543\n",
      "episode :  71\n",
      "current step :  65\n",
      "reward :  0.3055544234745543\n",
      "episode :  71\n",
      "current step :  70\n",
      "reward :  0.3292670104504644\n",
      "episode :  71\n",
      "current step :  75\n",
      "reward :  0.4384213256465714\n",
      "episode :  71\n",
      "current step :  80\n",
      "reward :  0.25316642241978315\n",
      "episode :  71\n",
      "current step :  85\n",
      "reward :  0.4019634728945882\n",
      "episode :  71\n",
      "current step :  90\n",
      "reward :  0.33287530897422246\n",
      "episode :  71\n",
      "current step :  95\n",
      "reward :  0.41452815267833876\n",
      "episode :  71\n",
      "current step :  100\n",
      "reward :  0.3979046706882212\n",
      "episode :  71\n",
      "current step :  105\n",
      "reward :  0.3615806187956483\n",
      "episode :  71\n",
      "current step :  110\n",
      "reward :  0.5032920009568334\n",
      "episode :  71\n",
      "current step :  115\n",
      "reward :  0.5203316082434687\n",
      "episode :  71\n",
      "current step :  120\n",
      "reward :  0.37759534128686234\n",
      "episode :  71\n",
      "current step :  125\n",
      "reward :  0.4272737113114679\n",
      "episode :  71\n",
      "current step :  130\n",
      "reward :  0.5260681011313474\n",
      "episode :  71\n",
      "current step :  135\n",
      "reward :  0.4445143429439118\n",
      "episode :  71\n",
      "current step :  140\n",
      "reward :  0.4231189950962897\n",
      "episode :  71\n",
      "current step :  145\n",
      "reward :  0.34052783295735867\n",
      "Episode 71, Total Reward: 11.981366268662923\n",
      "episode :  72\n",
      "current step :  0\n",
      "reward :  0.4229961372840137\n",
      "episode :  72\n",
      "current step :  5\n",
      "reward :  0.4063154878291993\n",
      "episode :  72\n",
      "current step :  10\n",
      "reward :  0.37315568099556023\n",
      "episode :  72\n",
      "current step :  15\n",
      "reward :  0.40027836725497173\n",
      "episode :  72\n",
      "current step :  20\n",
      "reward :  0.5120918215324741\n",
      "episode :  72\n",
      "current step :  25\n",
      "reward :  0.42146463695020975\n",
      "episode :  72\n",
      "current step :  30\n",
      "reward :  0.5083863694364125\n",
      "episode :  72\n",
      "current step :  35\n",
      "reward :  0.32445529571327103\n",
      "episode :  72\n",
      "current step :  40\n",
      "reward :  0.3406120716848036\n",
      "episode :  72\n",
      "current step :  45\n",
      "reward :  0.5161109962124619\n",
      "episode :  72\n",
      "current step :  50\n",
      "reward :  0.37559043448610363\n",
      "episode :  72\n",
      "current step :  55\n",
      "reward :  0.6242864734087415\n",
      "episode :  72\n",
      "current step :  60\n",
      "reward :  0.2401847833193712\n",
      "episode :  72\n",
      "current step :  65\n",
      "reward :  0.25578035095399027\n",
      "episode :  72\n",
      "current step :  70\n",
      "reward :  0.49972673260065237\n",
      "episode :  72\n",
      "current step :  75\n",
      "reward :  0.32642981222128814\n",
      "episode :  72\n",
      "current step :  80\n",
      "reward :  0.5936130074797947\n",
      "episode :  72\n",
      "current step :  85\n",
      "reward :  0.25750455053177046\n",
      "episode :  72\n",
      "current step :  90\n",
      "reward :  0.47321480922969034\n",
      "episode :  72\n",
      "current step :  95\n",
      "reward :  0.5208070799561525\n",
      "episode :  72\n",
      "current step :  100\n",
      "reward :  0.4417282119044297\n",
      "episode :  72\n",
      "current step :  105\n",
      "reward :  0.47265302357970784\n",
      "episode :  72\n",
      "current step :  110\n",
      "reward :  0.4714786377496366\n",
      "episode :  72\n",
      "current step :  115\n",
      "reward :  0.4249287994907209\n",
      "episode :  72\n",
      "current step :  120\n",
      "reward :  0.4158312778798123\n",
      "episode :  72\n",
      "current step :  125\n",
      "reward :  0.4677005545983391\n",
      "episode :  72\n",
      "current step :  130\n",
      "reward :  0.4269913121768024\n",
      "episode :  72\n",
      "current step :  135\n",
      "reward :  0.45647920026019617\n",
      "episode :  72\n",
      "current step :  140\n",
      "reward :  0.46655693657205205\n",
      "episode :  72\n",
      "current step :  145\n",
      "reward :  0.4521562344791872\n",
      "Episode 72, Total Reward: 12.889509087771817\n",
      "episode :  73\n",
      "current step :  0\n",
      "reward :  0.20333841892793827\n",
      "episode :  73\n",
      "current step :  5\n",
      "reward :  0.5473927041483492\n",
      "episode :  73\n",
      "current step :  10\n",
      "reward :  0.36723414905921437\n",
      "episode :  73\n",
      "current step :  15\n",
      "reward :  0.57786330788243\n",
      "episode :  73\n",
      "current step :  20\n",
      "reward :  0.23777809199713218\n",
      "episode :  73\n",
      "current step :  25\n",
      "reward :  0.5379434858765397\n",
      "episode :  73\n",
      "current step :  30\n",
      "reward :  0.46575489588547314\n",
      "episode :  73\n",
      "current step :  35\n",
      "reward :  0.20276186356397072\n",
      "episode :  73\n",
      "current step :  40\n",
      "reward :  0.31015928318580066\n",
      "episode :  73\n",
      "current step :  45\n",
      "reward :  0.5612315259513061\n",
      "episode :  73\n",
      "current step :  50\n",
      "reward :  0.22386100897781838\n",
      "episode :  73\n",
      "current step :  55\n",
      "reward :  0.4392208607184579\n",
      "episode :  73\n",
      "current step :  60\n",
      "reward :  0.20794292426925606\n",
      "episode :  73\n",
      "current step :  65\n",
      "reward :  0.1965999949495676\n",
      "episode :  73\n",
      "current step :  70\n",
      "reward :  0.41820403135525863\n",
      "episode :  73\n",
      "current step :  75\n",
      "reward :  0.5528132938247728\n",
      "episode :  73\n",
      "current step :  80\n",
      "reward :  0.48526713878955796\n",
      "episode :  73\n",
      "current step :  85\n",
      "reward :  0.2759302789786074\n",
      "episode :  73\n",
      "current step :  90\n",
      "reward :  0.42681190406671826\n",
      "episode :  73\n",
      "current step :  95\n",
      "reward :  0.3987704173895026\n",
      "episode :  73\n",
      "current step :  100\n",
      "reward :  0.18723044647550557\n",
      "episode :  73\n",
      "current step :  105\n",
      "reward :  0.48437183618669877\n",
      "episode :  73\n",
      "current step :  110\n",
      "reward :  0.2496260553576099\n",
      "episode :  73\n",
      "current step :  115\n",
      "reward :  0.3970498171578895\n",
      "episode :  73\n",
      "current step :  120\n",
      "reward :  0.4711318292356873\n",
      "episode :  73\n",
      "current step :  125\n",
      "reward :  0.4378810360864782\n",
      "episode :  73\n",
      "current step :  130\n",
      "reward :  0.5443525090197106\n",
      "episode :  73\n",
      "current step :  135\n",
      "reward :  0.4171666308986118\n",
      "episode :  73\n",
      "current step :  140\n",
      "reward :  0.47450804049829987\n",
      "episode :  73\n",
      "current step :  145\n",
      "reward :  0.44347360840714134\n",
      "Episode 73, Total Reward: 11.743671389121303\n",
      "episode :  74\n",
      "current step :  0\n",
      "reward :  0.4768173256796169\n",
      "episode :  74\n",
      "current step :  5\n",
      "reward :  0.5798223451860821\n",
      "episode :  74\n",
      "current step :  10\n",
      "reward :  0.4571751237410745\n",
      "episode :  74\n",
      "current step :  15\n",
      "reward :  0.5270002703370554\n",
      "episode :  74\n",
      "current step :  20\n",
      "reward :  0.4770209662721412\n",
      "episode :  74\n",
      "current step :  25\n",
      "reward :  0.4743387931942271\n",
      "episode :  74\n",
      "current step :  30\n",
      "reward :  0.5235976488192989\n",
      "episode :  74\n",
      "current step :  35\n",
      "reward :  0.23675201160845125\n",
      "episode :  74\n",
      "current step :  40\n",
      "reward :  0.2971124493761857\n",
      "episode :  74\n",
      "current step :  45\n",
      "reward :  0.4392875889900887\n",
      "episode :  74\n",
      "current step :  50\n",
      "reward :  0.39880180640083385\n",
      "episode :  74\n",
      "current step :  55\n",
      "reward :  0.42270477887213187\n",
      "episode :  74\n",
      "current step :  60\n",
      "reward :  0.20621654215791818\n",
      "episode :  74\n",
      "current step :  65\n",
      "reward :  0.19431529318720242\n",
      "episode :  74\n",
      "current step :  70\n",
      "reward :  0.40602414052437963\n",
      "episode :  74\n",
      "current step :  75\n",
      "reward :  0.37012781726525446\n",
      "episode :  74\n",
      "current step :  80\n",
      "reward :  0.5819221494335645\n",
      "episode :  74\n",
      "current step :  85\n",
      "reward :  0.46921979978599365\n",
      "episode :  74\n",
      "current step :  90\n",
      "reward :  0.4218282499572254\n",
      "episode :  74\n",
      "current step :  95\n",
      "reward :  0.47683731222854653\n",
      "episode :  74\n",
      "current step :  100\n",
      "reward :  0.4503396137840101\n",
      "episode :  74\n",
      "current step :  105\n",
      "reward :  0.47141184129746894\n",
      "episode :  74\n",
      "current step :  110\n",
      "reward :  0.42611288082272114\n",
      "episode :  74\n",
      "current step :  115\n",
      "reward :  0.3881964190826148\n",
      "episode :  74\n",
      "current step :  120\n",
      "reward :  0.43973500086773226\n",
      "episode :  74\n",
      "current step :  125\n",
      "reward :  0.4183017590509651\n",
      "episode :  74\n",
      "current step :  130\n",
      "reward :  0.46068184234645837\n",
      "episode :  74\n",
      "current step :  135\n",
      "reward :  0.44180311638456427\n",
      "episode :  74\n",
      "current step :  140\n",
      "reward :  0.4888349436175846\n",
      "episode :  74\n",
      "current step :  145\n",
      "reward :  0.4742964986311017\n",
      "Episode 74, Total Reward: 12.896636328902494\n",
      "episode :  75\n",
      "current step :  0\n",
      "reward :  0.4655083628506542\n",
      "episode :  75\n",
      "current step :  5\n",
      "reward :  0.4642643993289368\n",
      "episode :  75\n",
      "current step :  10\n",
      "reward :  0.42413211551917396\n",
      "episode :  75\n",
      "current step :  15\n",
      "reward :  0.4652505426820654\n",
      "episode :  75\n",
      "current step :  20\n",
      "reward :  0.49503621386192176\n",
      "episode :  75\n",
      "current step :  25\n",
      "reward :  0.4256421876658716\n",
      "episode :  75\n",
      "current step :  30\n",
      "reward :  0.41682285172765043\n",
      "episode :  75\n",
      "current step :  35\n",
      "reward :  0.41904528071196523\n",
      "episode :  75\n",
      "current step :  40\n",
      "reward :  0.34861177046163133\n",
      "episode :  75\n",
      "current step :  45\n",
      "reward :  0.41876932685131063\n",
      "episode :  75\n",
      "current step :  50\n",
      "reward :  0.28251516138657157\n",
      "episode :  75\n",
      "current step :  55\n",
      "reward :  0.24329216652877178\n",
      "episode :  75\n",
      "current step :  60\n",
      "reward :  0.25520430146156425\n",
      "episode :  75\n",
      "current step :  65\n",
      "reward :  0.29752317083339946\n",
      "episode :  75\n",
      "current step :  70\n",
      "reward :  0.47544283951027183\n",
      "episode :  75\n",
      "current step :  75\n",
      "reward :  0.38680584050466305\n",
      "episode :  75\n",
      "current step :  80\n",
      "reward :  0.43906689927404874\n",
      "episode :  75\n",
      "current step :  85\n",
      "reward :  0.47109837395076143\n",
      "episode :  75\n",
      "current step :  90\n",
      "reward :  0.46702417955927295\n",
      "episode :  75\n",
      "current step :  95\n",
      "reward :  0.19822980901206155\n",
      "episode :  75\n",
      "current step :  100\n",
      "reward :  0.3676398317890389\n",
      "episode :  75\n",
      "current step :  105\n",
      "reward :  0.445373671923633\n",
      "episode :  75\n",
      "current step :  110\n",
      "reward :  0.4753373127059981\n",
      "episode :  75\n",
      "current step :  115\n",
      "reward :  0.39754148199879613\n",
      "episode :  75\n",
      "current step :  120\n",
      "reward :  0.4086020466304564\n",
      "episode :  75\n",
      "current step :  125\n",
      "reward :  0.4600523823872611\n",
      "episode :  75\n",
      "current step :  130\n",
      "reward :  0.4962121609893757\n",
      "episode :  75\n",
      "current step :  135\n",
      "reward :  0.5270687177145608\n",
      "episode :  75\n",
      "current step :  140\n",
      "reward :  0.4491473679875928\n",
      "episode :  75\n",
      "current step :  145\n",
      "reward :  0.4569509984047858\n",
      "Episode 75, Total Reward: 12.343211766214067\n",
      "episode :  76\n",
      "current step :  0\n",
      "reward :  0.41801383967287287\n",
      "episode :  76\n",
      "current step :  5\n",
      "reward :  0.39293517320601645\n",
      "episode :  76\n",
      "current step :  10\n",
      "reward :  0.4777746078799199\n",
      "episode :  76\n",
      "current step :  15\n",
      "reward :  0.22681179941446045\n",
      "episode :  76\n",
      "current step :  20\n",
      "reward :  0.45433238569306517\n",
      "episode :  76\n",
      "current step :  25\n",
      "reward :  0.49142404012857954\n",
      "episode :  76\n",
      "current step :  30\n",
      "reward :  0.5532361705251367\n",
      "episode :  76\n",
      "current step :  35\n",
      "reward :  0.315130317316454\n",
      "episode :  76\n",
      "current step :  40\n",
      "reward :  0.30767481243153294\n",
      "episode :  76\n",
      "current step :  45\n",
      "reward :  0.4646413011828513\n",
      "episode :  76\n",
      "current step :  50\n",
      "reward :  0.42205488396482227\n",
      "episode :  76\n",
      "current step :  55\n",
      "reward :  0.23284498906598775\n",
      "episode :  76\n",
      "current step :  60\n",
      "reward :  0.21610957136829761\n",
      "episode :  76\n",
      "current step :  65\n",
      "reward :  0.36257163273564874\n",
      "episode :  76\n",
      "current step :  70\n",
      "reward :  0.2825082296216394\n",
      "episode :  76\n",
      "current step :  75\n",
      "reward :  0.4021628952202109\n",
      "episode :  76\n",
      "current step :  80\n",
      "reward :  0.5336452409811983\n",
      "episode :  76\n",
      "current step :  85\n",
      "reward :  0.4925044445472681\n",
      "episode :  76\n",
      "current step :  90\n",
      "reward :  0.3863203472458859\n",
      "episode :  76\n",
      "current step :  95\n",
      "reward :  0.4400623936873026\n",
      "episode :  76\n",
      "current step :  100\n",
      "reward :  0.5209035473019035\n",
      "episode :  76\n",
      "current step :  105\n",
      "reward :  0.5080461058435106\n",
      "episode :  76\n",
      "current step :  110\n",
      "reward :  0.424122129418234\n",
      "episode :  76\n",
      "current step :  115\n",
      "reward :  0.48961574073129005\n",
      "episode :  76\n",
      "current step :  120\n",
      "reward :  0.3810152071363515\n",
      "episode :  76\n",
      "current step :  125\n",
      "reward :  0.4955628479409533\n",
      "episode :  76\n",
      "current step :  130\n",
      "reward :  0.42422627826468473\n",
      "episode :  76\n",
      "current step :  135\n",
      "reward :  0.3879939583565648\n",
      "episode :  76\n",
      "current step :  140\n",
      "reward :  0.422761538403145\n",
      "episode :  76\n",
      "current step :  145\n",
      "reward :  0.3059362149035754\n",
      "Episode 76, Total Reward: 12.232942644189363\n",
      "episode :  77\n",
      "current step :  0\n",
      "reward :  0.5525352674740581\n",
      "episode :  77\n",
      "current step :  5\n",
      "reward :  0.28247064916245257\n",
      "episode :  77\n",
      "current step :  10\n",
      "reward :  0.43029622912720833\n",
      "episode :  77\n",
      "current step :  15\n",
      "reward :  0.4465798654240577\n",
      "episode :  77\n",
      "current step :  20\n",
      "reward :  0.4426391083178147\n",
      "episode :  77\n",
      "current step :  25\n",
      "reward :  0.32475361085956467\n",
      "episode :  77\n",
      "current step :  30\n",
      "reward :  0.48557371172887503\n",
      "episode :  77\n",
      "current step :  35\n",
      "reward :  0.3270600835440789\n",
      "episode :  77\n",
      "current step :  40\n",
      "reward :  0.48450188452081333\n",
      "episode :  77\n",
      "current step :  45\n",
      "reward :  0.6322441552890712\n",
      "episode :  77\n",
      "current step :  50\n",
      "reward :  0.5016214875801921\n",
      "episode :  77\n",
      "current step :  55\n",
      "reward :  0.319336320103494\n",
      "episode :  77\n",
      "current step :  60\n",
      "reward :  0.3259633522900133\n",
      "episode :  77\n",
      "current step :  65\n",
      "reward :  0.24369445785694882\n",
      "episode :  77\n",
      "current step :  70\n",
      "reward :  0.4068111838263277\n",
      "episode :  77\n",
      "current step :  75\n",
      "reward :  0.40873247952160535\n",
      "episode :  77\n",
      "current step :  80\n",
      "reward :  0.5164719387263408\n",
      "episode :  77\n",
      "current step :  85\n",
      "reward :  0.5499300220083624\n",
      "episode :  77\n",
      "current step :  90\n",
      "reward :  0.429256393002996\n",
      "episode :  77\n",
      "current step :  95\n",
      "reward :  0.24310480470353893\n",
      "episode :  77\n",
      "current step :  100\n",
      "reward :  0.42516157531677967\n",
      "episode :  77\n",
      "current step :  105\n",
      "reward :  0.5180653915531402\n",
      "episode :  77\n",
      "current step :  110\n",
      "reward :  0.4136408754993534\n",
      "episode :  77\n",
      "current step :  115\n",
      "reward :  0.18826421519009678\n",
      "episode :  77\n",
      "current step :  120\n",
      "reward :  0.3627024201693453\n",
      "episode :  77\n",
      "current step :  125\n",
      "reward :  0.407468813083121\n",
      "episode :  77\n",
      "current step :  130\n",
      "reward :  0.4292081652746789\n",
      "episode :  77\n",
      "current step :  135\n",
      "reward :  0.35895843196692256\n",
      "episode :  77\n",
      "current step :  140\n",
      "reward :  0.5127525498040437\n",
      "episode :  77\n",
      "current step :  145\n",
      "reward :  0.40033789855175217\n",
      "Episode 77, Total Reward: 12.370137341477045\n",
      "episode :  78\n",
      "current step :  0\n",
      "reward :  0.3716459609490992\n",
      "episode :  78\n",
      "current step :  5\n",
      "reward :  0.4810383925669857\n",
      "episode :  78\n",
      "current step :  10\n",
      "reward :  0.4665338344688905\n",
      "episode :  78\n",
      "current step :  15\n",
      "reward :  0.5357738078567639\n",
      "episode :  78\n",
      "current step :  20\n",
      "reward :  0.42450127012193445\n",
      "episode :  78\n",
      "current step :  25\n",
      "reward :  0.37948248057451406\n",
      "episode :  78\n",
      "current step :  30\n",
      "reward :  0.45514145099069236\n",
      "episode :  78\n",
      "current step :  35\n",
      "reward :  0.4034737625718379\n",
      "episode :  78\n",
      "current step :  40\n",
      "reward :  0.29508400453596206\n",
      "episode :  78\n",
      "current step :  45\n",
      "reward :  0.48091252274383206\n",
      "episode :  78\n",
      "current step :  50\n",
      "reward :  0.25447482367748403\n",
      "episode :  78\n",
      "current step :  55\n",
      "reward :  0.2851183975333253\n",
      "episode :  78\n",
      "current step :  60\n",
      "reward :  0.21904581278775453\n",
      "episode :  78\n",
      "current step :  65\n",
      "reward :  0.21510907829029546\n",
      "episode :  78\n",
      "current step :  70\n",
      "reward :  0.2857175720083394\n",
      "episode :  78\n",
      "current step :  75\n",
      "reward :  0.44423915776446143\n",
      "episode :  78\n",
      "current step :  80\n",
      "reward :  0.5122244173485822\n",
      "episode :  78\n",
      "current step :  85\n",
      "reward :  0.2808803974660001\n",
      "episode :  78\n",
      "current step :  90\n",
      "reward :  0.4176490758472716\n",
      "episode :  78\n",
      "current step :  95\n",
      "reward :  0.4582284566055528\n",
      "episode :  78\n",
      "current step :  100\n",
      "reward :  0.457900118220363\n",
      "episode :  78\n",
      "current step :  105\n",
      "reward :  0.41072731992123757\n",
      "episode :  78\n",
      "current step :  110\n",
      "reward :  0.4506056907296584\n",
      "episode :  78\n",
      "current step :  115\n",
      "reward :  0.4572820004289062\n",
      "episode :  78\n",
      "current step :  120\n",
      "reward :  0.46171835736524147\n",
      "episode :  78\n",
      "current step :  125\n",
      "reward :  0.49444382441354273\n",
      "episode :  78\n",
      "current step :  130\n",
      "reward :  0.4632775455759217\n",
      "episode :  78\n",
      "current step :  135\n",
      "reward :  0.41821981083295057\n",
      "episode :  78\n",
      "current step :  140\n",
      "reward :  0.3990820255968779\n",
      "episode :  78\n",
      "current step :  145\n",
      "reward :  0.43292812936853553\n",
      "Episode 78, Total Reward: 12.11245949916281\n",
      "episode :  79\n",
      "current step :  0\n",
      "reward :  0.5795357077390055\n",
      "episode :  79\n",
      "current step :  5\n",
      "reward :  0.4887128633690634\n",
      "episode :  79\n",
      "current step :  10\n",
      "reward :  0.4568386009575028\n",
      "episode :  79\n",
      "current step :  15\n",
      "reward :  0.40047042585543474\n",
      "episode :  79\n",
      "current step :  20\n",
      "reward :  0.534316389754564\n",
      "episode :  79\n",
      "current step :  25\n",
      "reward :  0.4518291004728229\n",
      "episode :  79\n",
      "current step :  30\n",
      "reward :  0.2648881430397503\n",
      "episode :  79\n",
      "current step :  35\n",
      "reward :  0.44785670596682436\n",
      "episode :  79\n",
      "current step :  40\n",
      "reward :  0.26454133111207107\n",
      "episode :  79\n",
      "current step :  45\n",
      "reward :  0.3780905989034446\n",
      "episode :  79\n",
      "current step :  50\n",
      "reward :  0.43476972185433016\n",
      "episode :  79\n",
      "current step :  55\n",
      "reward :  0.39453708349331895\n",
      "episode :  79\n",
      "current step :  60\n",
      "reward :  0.24935497820958813\n",
      "episode :  79\n",
      "current step :  65\n",
      "reward :  0.28351237600704565\n",
      "episode :  79\n",
      "current step :  70\n",
      "reward :  0.2815494982823671\n",
      "episode :  79\n",
      "current step :  75\n",
      "reward :  0.27602892988881866\n",
      "episode :  79\n",
      "current step :  80\n",
      "reward :  0.49746372961876784\n",
      "episode :  79\n",
      "current step :  85\n",
      "reward :  0.3081767902374596\n",
      "episode :  79\n",
      "current step :  90\n",
      "reward :  0.3993680744345076\n",
      "episode :  79\n",
      "current step :  95\n",
      "reward :  0.4316234452876883\n",
      "episode :  79\n",
      "current step :  100\n",
      "reward :  0.42490670963295274\n",
      "episode :  79\n",
      "current step :  105\n",
      "reward :  0.48692665036847527\n",
      "episode :  79\n",
      "current step :  110\n",
      "reward :  0.37915873845589065\n",
      "episode :  79\n",
      "current step :  115\n",
      "reward :  0.36424431544851354\n",
      "episode :  79\n",
      "current step :  120\n",
      "reward :  0.3667961635119745\n",
      "episode :  79\n",
      "current step :  125\n",
      "reward :  0.5394147054761391\n",
      "episode :  79\n",
      "current step :  130\n",
      "reward :  0.5659842605993678\n",
      "episode :  79\n",
      "current step :  135\n",
      "reward :  0.4767056076534486\n",
      "episode :  79\n",
      "current step :  140\n",
      "reward :  0.5546688672222126\n",
      "episode :  79\n",
      "current step :  145\n",
      "reward :  0.45309477031052975\n",
      "Episode 79, Total Reward: 12.43536528316388\n",
      "episode :  80\n",
      "current step :  0\n",
      "reward :  0.3988585101737431\n",
      "episode :  80\n",
      "current step :  5\n",
      "reward :  0.4741849614938856\n",
      "episode :  80\n",
      "current step :  10\n",
      "reward :  0.4113073779928648\n",
      "episode :  80\n",
      "current step :  15\n",
      "reward :  0.5853361110593279\n",
      "episode :  80\n",
      "current step :  20\n",
      "reward :  0.45501981790337276\n",
      "episode :  80\n",
      "current step :  25\n",
      "reward :  0.0\n",
      "episode :  80\n",
      "current step :  30\n",
      "reward :  0.23236076744816514\n",
      "episode :  80\n",
      "current step :  35\n",
      "reward :  0.5574605617077936\n",
      "episode :  80\n",
      "current step :  40\n",
      "reward :  0.33954080202015335\n",
      "episode :  80\n",
      "current step :  45\n",
      "reward :  0.30291545500017963\n",
      "episode :  80\n",
      "current step :  50\n",
      "reward :  0.27141028522260835\n",
      "episode :  80\n",
      "current step :  55\n",
      "reward :  0.2646807300806845\n",
      "episode :  80\n",
      "current step :  60\n",
      "reward :  0.20186317535882706\n",
      "episode :  80\n",
      "current step :  65\n",
      "reward :  0.4386090408347635\n",
      "episode :  80\n",
      "current step :  70\n",
      "reward :  0.30346666811418477\n",
      "episode :  80\n",
      "current step :  75\n",
      "reward :  0.44512070512862006\n",
      "episode :  80\n",
      "current step :  80\n",
      "reward :  0.5266388520775682\n",
      "episode :  80\n",
      "current step :  85\n",
      "reward :  0.28834817486568054\n",
      "episode :  80\n",
      "current step :  90\n",
      "reward :  0.35788482683181677\n",
      "episode :  80\n",
      "current step :  95\n",
      "reward :  0.403475235082253\n",
      "episode :  80\n",
      "current step :  100\n",
      "reward :  0.5257654745440122\n",
      "episode :  80\n",
      "current step :  105\n",
      "reward :  0.3837430870542555\n",
      "episode :  80\n",
      "current step :  110\n",
      "reward :  0.47987976748343303\n",
      "episode :  80\n",
      "current step :  115\n",
      "reward :  0.39699342901341633\n",
      "episode :  80\n",
      "current step :  120\n",
      "reward :  0.5170605205226687\n",
      "episode :  80\n",
      "current step :  125\n",
      "reward :  0.5384539689320342\n",
      "episode :  80\n",
      "current step :  130\n",
      "reward :  0.42578750070383065\n",
      "episode :  80\n",
      "current step :  135\n",
      "reward :  0.44259042997968584\n",
      "episode :  80\n",
      "current step :  140\n",
      "reward :  0.5321247069620509\n",
      "episode :  80\n",
      "current step :  145\n",
      "reward :  0.4639743090042112\n",
      "Episode 80, Total Reward: 11.964855252596092\n",
      "episode :  81\n",
      "current step :  0\n",
      "reward :  0.48417575742754854\n",
      "episode :  81\n",
      "current step :  5\n",
      "reward :  0.34705480953465806\n",
      "episode :  81\n",
      "current step :  10\n",
      "reward :  0.4228736345033306\n",
      "episode :  81\n",
      "current step :  15\n",
      "reward :  0.41628414890198784\n",
      "episode :  81\n",
      "current step :  20\n",
      "reward :  0.423414724381529\n",
      "episode :  81\n",
      "current step :  25\n",
      "reward :  0.4586561774683633\n",
      "episode :  81\n",
      "current step :  30\n",
      "reward :  0.528369384268436\n",
      "episode :  81\n",
      "current step :  35\n",
      "reward :  0.5347910390838732\n",
      "episode :  81\n",
      "current step :  40\n",
      "reward :  0.3235947119080082\n",
      "episode :  81\n",
      "current step :  45\n",
      "reward :  0.4360592783529349\n",
      "episode :  81\n",
      "current step :  50\n",
      "reward :  0.30332487220384635\n",
      "episode :  81\n",
      "current step :  55\n",
      "reward :  0.29845755511672345\n",
      "episode :  81\n",
      "current step :  60\n",
      "reward :  0.4599358043477348\n",
      "episode :  81\n",
      "current step :  65\n",
      "reward :  0.1913057726794409\n",
      "episode :  81\n",
      "current step :  70\n",
      "reward :  0.21356894409110622\n",
      "episode :  81\n",
      "current step :  75\n",
      "reward :  0.35780493706719746\n",
      "episode :  81\n",
      "current step :  80\n",
      "reward :  0.3198908067758894\n",
      "episode :  81\n",
      "current step :  85\n",
      "reward :  0.40876057042368186\n",
      "episode :  81\n",
      "current step :  90\n",
      "reward :  0.4003310347711461\n",
      "episode :  81\n",
      "current step :  95\n",
      "reward :  0.4454206446969581\n",
      "episode :  81\n",
      "current step :  100\n",
      "reward :  0.5239226701146342\n",
      "episode :  81\n",
      "current step :  105\n",
      "reward :  0.36527494140739486\n",
      "episode :  81\n",
      "current step :  110\n",
      "reward :  0.38436593420324733\n",
      "episode :  81\n",
      "current step :  115\n",
      "reward :  0.42932647391302986\n",
      "episode :  81\n",
      "current step :  120\n",
      "reward :  0.39586131029966876\n",
      "episode :  81\n",
      "current step :  125\n",
      "reward :  0.5373460636451952\n",
      "episode :  81\n",
      "current step :  130\n",
      "reward :  0.3269976183144802\n",
      "episode :  81\n",
      "current step :  135\n",
      "reward :  0.47029227951240576\n",
      "episode :  81\n",
      "current step :  140\n",
      "reward :  0.3912392609306489\n",
      "episode :  81\n",
      "current step :  145\n",
      "reward :  0.4294119572748529\n",
      "Episode 81, Total Reward: 12.02811311761995\n",
      "episode :  82\n",
      "current step :  0\n",
      "reward :  0.4262039185980847\n",
      "episode :  82\n",
      "current step :  5\n",
      "reward :  0.35151541685397747\n",
      "episode :  82\n",
      "current step :  10\n",
      "reward :  0.459680149482101\n",
      "episode :  82\n",
      "current step :  15\n",
      "reward :  0.4538917900559233\n",
      "episode :  82\n",
      "current step :  20\n",
      "reward :  0.38327645104730845\n",
      "episode :  82\n",
      "current step :  25\n",
      "reward :  0.4187843535372054\n",
      "episode :  82\n",
      "current step :  30\n",
      "reward :  0.4947239838300487\n",
      "episode :  82\n",
      "current step :  35\n",
      "reward :  0.44315634079889116\n",
      "episode :  82\n",
      "current step :  40\n",
      "reward :  0.3301699630010411\n",
      "episode :  82\n",
      "current step :  45\n",
      "reward :  0.39275789820082097\n",
      "episode :  82\n",
      "current step :  50\n",
      "reward :  0.46159507707487113\n",
      "episode :  82\n",
      "current step :  55\n",
      "reward :  0.5385274468042642\n",
      "episode :  82\n",
      "current step :  60\n",
      "reward :  0.20508432696037787\n",
      "episode :  82\n",
      "current step :  65\n",
      "reward :  0.23358255315764867\n",
      "episode :  82\n",
      "current step :  70\n",
      "reward :  0.4161681394053909\n",
      "episode :  82\n",
      "current step :  75\n",
      "reward :  0.3650082991201621\n",
      "episode :  82\n",
      "current step :  80\n",
      "reward :  0.3923871434885519\n",
      "episode :  82\n",
      "current step :  85\n",
      "reward :  0.5007370649512113\n",
      "episode :  82\n",
      "current step :  90\n",
      "reward :  0.2341393148667442\n",
      "episode :  82\n",
      "current step :  95\n",
      "reward :  0.4236672722404796\n",
      "episode :  82\n",
      "current step :  100\n",
      "reward :  0.5773121289401941\n",
      "episode :  82\n",
      "current step :  105\n",
      "reward :  0.4448964837552298\n",
      "episode :  82\n",
      "current step :  110\n",
      "reward :  0.19619608205481656\n",
      "episode :  82\n",
      "current step :  115\n",
      "reward :  0.40350784848715704\n",
      "episode :  82\n",
      "current step :  120\n",
      "reward :  0.16219146096280215\n",
      "episode :  82\n",
      "current step :  125\n",
      "reward :  0.5772026082994386\n",
      "episode :  82\n",
      "current step :  130\n",
      "reward :  0.5010655891020247\n",
      "episode :  82\n",
      "current step :  135\n",
      "reward :  0.47906935759512015\n",
      "episode :  82\n",
      "current step :  140\n",
      "reward :  0.5414115076692303\n",
      "episode :  82\n",
      "current step :  145\n",
      "reward :  0.43761150838254415\n",
      "Episode 82, Total Reward: 12.245521478723662\n",
      "episode :  83\n",
      "current step :  0\n",
      "reward :  0.4835411492685817\n",
      "episode :  83\n",
      "current step :  5\n",
      "reward :  0.4549818297441405\n",
      "episode :  83\n",
      "current step :  10\n",
      "reward :  0.4844572619934707\n",
      "episode :  83\n",
      "current step :  15\n",
      "reward :  0.47711506090482464\n",
      "episode :  83\n",
      "current step :  20\n",
      "reward :  0.5021085928793401\n",
      "episode :  83\n",
      "current step :  25\n",
      "reward :  0.38951555955696915\n",
      "episode :  83\n",
      "current step :  30\n",
      "reward :  0.3491904074431194\n",
      "episode :  83\n",
      "current step :  35\n",
      "reward :  0.338049837800076\n",
      "episode :  83\n",
      "current step :  40\n",
      "reward :  0.29190756100563325\n",
      "episode :  83\n",
      "current step :  45\n",
      "reward :  0.42953146392534347\n",
      "episode :  83\n",
      "current step :  50\n",
      "reward :  0.4448773026026291\n",
      "episode :  83\n",
      "current step :  55\n",
      "reward :  0.40256303209097566\n",
      "episode :  83\n",
      "current step :  60\n",
      "reward :  0.21898682659044577\n",
      "episode :  83\n",
      "current step :  65\n",
      "reward :  0.4418941820444541\n",
      "episode :  83\n",
      "current step :  70\n",
      "reward :  0.3069420995029489\n",
      "episode :  83\n",
      "current step :  75\n",
      "reward :  0.4105918734292718\n",
      "episode :  83\n",
      "current step :  80\n",
      "reward :  0.5116130651149302\n",
      "episode :  83\n",
      "current step :  85\n",
      "reward :  0.4075934750986372\n",
      "episode :  83\n",
      "current step :  90\n",
      "reward :  0.44528991571279664\n",
      "episode :  83\n",
      "current step :  95\n",
      "reward :  0.4913288440092518\n",
      "episode :  83\n",
      "current step :  100\n",
      "reward :  0.36372098881965875\n",
      "episode :  83\n",
      "current step :  105\n",
      "reward :  0.408427598361142\n",
      "episode :  83\n",
      "current step :  110\n",
      "reward :  0.5249875645782269\n",
      "episode :  83\n",
      "current step :  115\n",
      "reward :  0.5111295754251313\n",
      "episode :  83\n",
      "current step :  120\n",
      "reward :  0.2612656407441025\n",
      "episode :  83\n",
      "current step :  125\n",
      "reward :  0.4984329409925708\n",
      "episode :  83\n",
      "current step :  130\n",
      "reward :  0.39375705155508645\n",
      "episode :  83\n",
      "current step :  135\n",
      "reward :  0.527524113554191\n",
      "episode :  83\n",
      "current step :  140\n",
      "reward :  0.42759347680738136\n",
      "episode :  83\n",
      "current step :  145\n",
      "reward :  0.45150545501033335\n",
      "Episode 83, Total Reward: 12.650423746565664\n",
      "episode :  84\n",
      "current step :  0\n",
      "reward :  0.41448279581054986\n",
      "episode :  84\n",
      "current step :  5\n",
      "reward :  0.37952860062652466\n",
      "episode :  84\n",
      "current step :  10\n",
      "reward :  0.5380688065350844\n",
      "episode :  84\n",
      "current step :  15\n",
      "reward :  0.48840420822832376\n",
      "episode :  84\n",
      "current step :  20\n",
      "reward :  0.4182293950682844\n",
      "episode :  84\n",
      "current step :  25\n",
      "reward :  0.4103359825700449\n",
      "episode :  84\n",
      "current step :  30\n",
      "reward :  0.574912423004073\n",
      "episode :  84\n",
      "current step :  35\n",
      "reward :  0.30564779608385784\n",
      "episode :  84\n",
      "current step :  40\n",
      "reward :  0.2718312892931389\n",
      "episode :  84\n",
      "current step :  45\n",
      "reward :  0.40752231974837355\n",
      "episode :  84\n",
      "current step :  50\n",
      "reward :  0.5533628467710985\n",
      "episode :  84\n",
      "current step :  55\n",
      "reward :  0.271755594245983\n",
      "episode :  84\n",
      "current step :  60\n",
      "reward :  0.16862441849641918\n",
      "episode :  84\n",
      "current step :  65\n",
      "reward :  0.2465861958354579\n",
      "episode :  84\n",
      "current step :  70\n",
      "reward :  0.27283165106804963\n",
      "episode :  84\n",
      "current step :  75\n",
      "reward :  0.3611461095830331\n",
      "episode :  84\n",
      "current step :  80\n",
      "reward :  0.46358039687126684\n",
      "episode :  84\n",
      "current step :  85\n",
      "reward :  0.46154233408846035\n",
      "episode :  84\n",
      "current step :  90\n",
      "reward :  0.5116528698647076\n",
      "episode :  84\n",
      "current step :  95\n",
      "reward :  0.6082975614880363\n",
      "episode :  84\n",
      "current step :  100\n",
      "reward :  0.44898444038459284\n",
      "episode :  84\n",
      "current step :  105\n",
      "reward :  0.19881985294614388\n",
      "episode :  84\n",
      "current step :  110\n",
      "reward :  0.4047818380020377\n",
      "episode :  84\n",
      "current step :  115\n",
      "reward :  0.38661981667784634\n",
      "episode :  84\n",
      "current step :  120\n",
      "reward :  0.4078426274856738\n",
      "episode :  84\n",
      "current step :  125\n",
      "reward :  0.3843172544740245\n",
      "episode :  84\n",
      "current step :  130\n",
      "reward :  0.43399995595454477\n",
      "episode :  84\n",
      "current step :  135\n",
      "reward :  0.3910483158199879\n",
      "episode :  84\n",
      "current step :  140\n",
      "reward :  0.6146136887615805\n",
      "episode :  84\n",
      "current step :  145\n",
      "reward :  0.3960103555624668\n",
      "Episode 84, Total Reward: 12.195381741349667\n",
      "episode :  85\n",
      "current step :  0\n",
      "reward :  0.310978026027801\n",
      "episode :  85\n",
      "current step :  5\n",
      "reward :  0.4640948600538728\n",
      "episode :  85\n",
      "current step :  10\n",
      "reward :  0.45223980289565013\n",
      "episode :  85\n",
      "current step :  15\n",
      "reward :  0.474734398771996\n",
      "episode :  85\n",
      "current step :  20\n",
      "reward :  0.4500118620442892\n",
      "episode :  85\n",
      "current step :  25\n",
      "reward :  0.4032325914548814\n",
      "episode :  85\n",
      "current step :  30\n",
      "reward :  0.36892618822603307\n",
      "episode :  85\n",
      "current step :  35\n",
      "reward :  0.26435482578429104\n",
      "episode :  85\n",
      "current step :  40\n",
      "reward :  0.32366412879901124\n",
      "episode :  85\n",
      "current step :  45\n",
      "reward :  0.3950109686637924\n",
      "episode :  85\n",
      "current step :  50\n",
      "reward :  0.30640886368986603\n",
      "episode :  85\n",
      "current step :  55\n",
      "reward :  0.23734762522123978\n",
      "episode :  85\n",
      "current step :  60\n",
      "reward :  0.5108183814154347\n",
      "episode :  85\n",
      "current step :  65\n",
      "reward :  0.26461631274974207\n",
      "episode :  85\n",
      "current step :  70\n",
      "reward :  0.46162465679687037\n",
      "episode :  85\n",
      "current step :  75\n",
      "reward :  0.3550497252338135\n",
      "episode :  85\n",
      "current step :  80\n",
      "reward :  0.4350692186922863\n",
      "episode :  85\n",
      "current step :  85\n",
      "reward :  0.4695300987061023\n",
      "episode :  85\n",
      "current step :  90\n",
      "reward :  0.4324896111070015\n",
      "episode :  85\n",
      "current step :  95\n",
      "reward :  0.3756674742735643\n",
      "episode :  85\n",
      "current step :  100\n",
      "reward :  0.4597779856552122\n",
      "episode :  85\n",
      "current step :  105\n",
      "reward :  0.41228502850871857\n",
      "episode :  85\n",
      "current step :  110\n",
      "reward :  0.45976694663273543\n",
      "episode :  85\n",
      "current step :  115\n",
      "reward :  0.4329897378274093\n",
      "episode :  85\n",
      "current step :  120\n",
      "reward :  0.44344270059488095\n",
      "episode :  85\n",
      "current step :  125\n",
      "reward :  0.40549884234370137\n",
      "episode :  85\n",
      "current step :  130\n",
      "reward :  0.5385000101411324\n",
      "episode :  85\n",
      "current step :  135\n",
      "reward :  0.5205417456060749\n",
      "episode :  85\n",
      "current step :  140\n",
      "reward :  0.40601483259202154\n",
      "episode :  85\n",
      "current step :  145\n",
      "reward :  0.4086866165685561\n",
      "Episode 85, Total Reward: 12.24337406707798\n",
      "episode :  86\n",
      "current step :  0\n",
      "reward :  0.5185097660548167\n",
      "episode :  86\n",
      "current step :  5\n",
      "reward :  0.3896716576694298\n",
      "episode :  86\n",
      "current step :  10\n",
      "reward :  0.49396378942408525\n",
      "episode :  86\n",
      "current step :  15\n",
      "reward :  0.47675921351795947\n",
      "episode :  86\n",
      "current step :  20\n",
      "reward :  0.4691907342485862\n",
      "episode :  86\n",
      "current step :  25\n",
      "reward :  0.4849323697512866\n",
      "episode :  86\n",
      "current step :  30\n",
      "reward :  0.48440229932149137\n",
      "episode :  86\n",
      "current step :  35\n",
      "reward :  0.2530997761551075\n",
      "episode :  86\n",
      "current step :  40\n",
      "reward :  0.35498662326775715\n",
      "episode :  86\n",
      "current step :  45\n",
      "reward :  0.33741790238285574\n",
      "episode :  86\n",
      "current step :  50\n",
      "reward :  0.41833943042918065\n",
      "episode :  86\n",
      "current step :  55\n",
      "reward :  0.31854848364051397\n",
      "episode :  86\n",
      "current step :  60\n",
      "reward :  0.28022777066702537\n",
      "episode :  86\n",
      "current step :  65\n",
      "reward :  0.33433512416372646\n",
      "episode :  86\n",
      "current step :  70\n",
      "reward :  0.3545016824322789\n",
      "episode :  86\n",
      "current step :  75\n",
      "reward :  0.566799803899695\n",
      "episode :  86\n",
      "current step :  80\n",
      "reward :  0.2919536429531192\n",
      "episode :  86\n",
      "current step :  85\n",
      "reward :  0.4082324970871044\n",
      "episode :  86\n",
      "current step :  90\n",
      "reward :  0.5525499820848285\n",
      "episode :  86\n",
      "current step :  95\n",
      "reward :  0.39265867821465117\n",
      "episode :  86\n",
      "current step :  100\n",
      "reward :  0.39180271881630474\n",
      "episode :  86\n",
      "current step :  105\n",
      "reward :  0.4595725205965179\n",
      "episode :  86\n",
      "current step :  110\n",
      "reward :  0.3361580049069173\n",
      "episode :  86\n",
      "current step :  115\n",
      "reward :  0.16209675861131564\n",
      "episode :  86\n",
      "current step :  120\n",
      "reward :  0.3622597331674712\n",
      "episode :  86\n",
      "current step :  125\n",
      "reward :  0.47670613995610434\n",
      "episode :  86\n",
      "current step :  130\n",
      "reward :  0.4195177195322401\n",
      "episode :  86\n",
      "current step :  135\n",
      "reward :  0.3896025640714357\n",
      "episode :  86\n",
      "current step :  140\n",
      "reward :  0.5280858317921193\n",
      "episode :  86\n",
      "current step :  145\n",
      "reward :  0.5045922986827265\n",
      "Episode 86, Total Reward: 12.211475517498652\n",
      "episode :  87\n",
      "current step :  0\n",
      "reward :  0.5033400818958582\n",
      "episode :  87\n",
      "current step :  5\n",
      "reward :  0.4695099207944725\n",
      "episode :  87\n",
      "current step :  10\n",
      "reward :  0.43657745879985854\n",
      "episode :  87\n",
      "current step :  15\n",
      "reward :  0.398608575985286\n",
      "episode :  87\n",
      "current step :  20\n",
      "reward :  0.44373944947905486\n",
      "episode :  87\n",
      "current step :  25\n",
      "reward :  0.410705920402429\n",
      "episode :  87\n",
      "current step :  30\n",
      "reward :  0.4320221822950504\n",
      "episode :  87\n",
      "current step :  35\n",
      "reward :  0.3787662937978396\n",
      "episode :  87\n",
      "current step :  40\n",
      "reward :  0.39320746029842635\n",
      "episode :  87\n",
      "current step :  45\n",
      "reward :  0.3171998005393918\n",
      "episode :  87\n",
      "current step :  50\n",
      "reward :  0.20239049058898295\n",
      "episode :  87\n",
      "current step :  55\n",
      "reward :  0.3868146857088519\n",
      "episode :  87\n",
      "current step :  60\n",
      "reward :  0.23760485762297154\n",
      "episode :  87\n",
      "current step :  65\n",
      "reward :  0.223463212246642\n",
      "episode :  87\n",
      "current step :  70\n",
      "reward :  0.4607738330292245\n",
      "episode :  87\n",
      "current step :  75\n",
      "reward :  0.38281539842270607\n",
      "episode :  87\n",
      "current step :  80\n",
      "reward :  0.4887305352220209\n",
      "episode :  87\n",
      "current step :  85\n",
      "reward :  0.16224643745231834\n",
      "episode :  87\n",
      "current step :  90\n",
      "reward :  0.4421959462068744\n",
      "episode :  87\n",
      "current step :  95\n",
      "reward :  0.5520027578410353\n",
      "episode :  87\n",
      "current step :  100\n",
      "reward :  0.34582637863231114\n",
      "episode :  87\n",
      "current step :  105\n",
      "reward :  0.4287725640174347\n",
      "episode :  87\n",
      "current step :  110\n",
      "reward :  0.3438613052226694\n",
      "episode :  87\n",
      "current step :  115\n",
      "reward :  0.4504479450923208\n",
      "episode :  87\n",
      "current step :  120\n",
      "reward :  0.34886454290911023\n",
      "episode :  87\n",
      "current step :  125\n",
      "reward :  0.3323744482866487\n",
      "episode :  87\n",
      "current step :  130\n",
      "reward :  0.4408058209547986\n",
      "episode :  87\n",
      "current step :  135\n",
      "reward :  0.4326197529074474\n",
      "episode :  87\n",
      "current step :  140\n",
      "reward :  0.5218412241315901\n",
      "episode :  87\n",
      "current step :  145\n",
      "reward :  0.5595257677597885\n",
      "Episode 87, Total Reward: 11.927655048543416\n",
      "episode :  88\n",
      "current step :  0\n",
      "reward :  0.4989810692820359\n",
      "episode :  88\n",
      "current step :  5\n",
      "reward :  0.476810626463876\n",
      "episode :  88\n",
      "current step :  10\n",
      "reward :  0.4043698661386471\n",
      "episode :  88\n",
      "current step :  15\n",
      "reward :  0.4424681747519623\n",
      "episode :  88\n",
      "current step :  20\n",
      "reward :  0.41634040016513885\n",
      "episode :  88\n",
      "current step :  25\n",
      "reward :  0.4593569825486553\n",
      "episode :  88\n",
      "current step :  30\n",
      "reward :  0.24172400590936516\n",
      "episode :  88\n",
      "current step :  35\n",
      "reward :  0.3153515242484069\n",
      "episode :  88\n",
      "current step :  40\n",
      "reward :  0.26712316303008093\n",
      "episode :  88\n",
      "current step :  45\n",
      "reward :  0.547538095724342\n",
      "episode :  88\n",
      "current step :  50\n",
      "reward :  0.1687515447009946\n",
      "episode :  88\n",
      "current step :  55\n",
      "reward :  0.4405227534796302\n",
      "episode :  88\n",
      "current step :  60\n",
      "reward :  0.21058475166654908\n",
      "episode :  88\n",
      "current step :  65\n",
      "reward :  0.4623153967253156\n",
      "episode :  88\n",
      "current step :  70\n",
      "reward :  0.3102942100104712\n",
      "episode :  88\n",
      "current step :  75\n",
      "reward :  0.4392639342441588\n",
      "episode :  88\n",
      "current step :  80\n",
      "reward :  0.31183222117790727\n",
      "episode :  88\n",
      "current step :  85\n",
      "reward :  0.5159005109912604\n",
      "episode :  88\n",
      "current step :  90\n",
      "reward :  0.5007971995113752\n",
      "episode :  88\n",
      "current step :  95\n",
      "reward :  0.4591497482906557\n",
      "episode :  88\n",
      "current step :  100\n",
      "reward :  0.30100015335608205\n",
      "episode :  88\n",
      "current step :  105\n",
      "reward :  0.3820419165170185\n",
      "episode :  88\n",
      "current step :  110\n",
      "reward :  0.4150420022054232\n",
      "episode :  88\n",
      "current step :  115\n",
      "reward :  0.42113659412594884\n",
      "episode :  88\n",
      "current step :  120\n",
      "reward :  0.39213025724074296\n",
      "episode :  88\n",
      "current step :  125\n",
      "reward :  0.3806394085355578\n",
      "episode :  88\n",
      "current step :  130\n",
      "reward :  0.45534637028628067\n",
      "episode :  88\n",
      "current step :  135\n",
      "reward :  0.34067837635855414\n",
      "episode :  88\n",
      "current step :  140\n",
      "reward :  0.3871704655521815\n",
      "episode :  88\n",
      "current step :  145\n",
      "reward :  0.45094920611022105\n",
      "Episode 88, Total Reward: 11.815610929348837\n",
      "episode :  89\n",
      "current step :  0\n",
      "reward :  0.5115285785212921\n",
      "episode :  89\n",
      "current step :  5\n",
      "reward :  0.43619974544493495\n",
      "episode :  89\n",
      "current step :  10\n",
      "reward :  0.455078536978775\n",
      "episode :  89\n",
      "current step :  15\n",
      "reward :  0.5156995860741322\n",
      "episode :  89\n",
      "current step :  20\n",
      "reward :  0.46532440494910937\n",
      "episode :  89\n",
      "current step :  25\n",
      "reward :  0.19430540161247783\n",
      "episode :  89\n",
      "current step :  30\n",
      "reward :  0.5167325087460192\n",
      "episode :  89\n",
      "current step :  35\n",
      "reward :  0.23314301281257718\n",
      "episode :  89\n",
      "current step :  40\n",
      "reward :  0.509698736000433\n",
      "episode :  89\n",
      "current step :  45\n",
      "reward :  0.31514661763693175\n",
      "episode :  89\n",
      "current step :  50\n",
      "reward :  0.42564614319252986\n",
      "episode :  89\n",
      "current step :  55\n",
      "reward :  0.28915537953324416\n",
      "episode :  89\n",
      "current step :  60\n",
      "reward :  0.18226882339534126\n",
      "episode :  89\n",
      "current step :  65\n",
      "reward :  0.22082544997244868\n",
      "episode :  89\n",
      "current step :  70\n",
      "reward :  0.42396106976285197\n",
      "episode :  89\n",
      "current step :  75\n",
      "reward :  0.3093509706203124\n",
      "episode :  89\n",
      "current step :  80\n",
      "reward :  0.36483668504967237\n",
      "episode :  89\n",
      "current step :  85\n",
      "reward :  0.5036187303243991\n",
      "episode :  89\n",
      "current step :  90\n",
      "reward :  0.3806214091802822\n",
      "episode :  89\n",
      "current step :  95\n",
      "reward :  0.4442233560629799\n",
      "episode :  89\n",
      "current step :  100\n",
      "reward :  0.39967475881531384\n",
      "episode :  89\n",
      "current step :  105\n",
      "reward :  0.422942673948162\n",
      "episode :  89\n",
      "current step :  110\n",
      "reward :  0.4795960909313992\n",
      "episode :  89\n",
      "current step :  115\n",
      "reward :  0.3843080364838592\n",
      "episode :  89\n",
      "current step :  120\n",
      "reward :  0.4904205172254019\n",
      "episode :  89\n",
      "current step :  125\n",
      "reward :  0.4065356944175339\n",
      "episode :  89\n",
      "current step :  130\n",
      "reward :  0.45292013417724764\n",
      "episode :  89\n",
      "current step :  135\n",
      "reward :  0.439001930180339\n",
      "episode :  89\n",
      "current step :  140\n",
      "reward :  0.5197344331370036\n",
      "episode :  89\n",
      "current step :  145\n",
      "reward :  0.45342711265014124\n",
      "Episode 89, Total Reward: 12.145926527837144\n",
      "episode :  90\n",
      "current step :  0\n",
      "reward :  0.4555691353385354\n",
      "episode :  90\n",
      "current step :  5\n",
      "reward :  0.5576627209062939\n",
      "episode :  90\n",
      "current step :  10\n",
      "reward :  0.265072634270937\n",
      "episode :  90\n",
      "current step :  15\n",
      "reward :  0.4917522449332609\n",
      "episode :  90\n",
      "current step :  20\n",
      "reward :  0.4351894207424579\n",
      "episode :  90\n",
      "current step :  25\n",
      "reward :  0.43435625332212136\n",
      "episode :  90\n",
      "current step :  30\n",
      "reward :  0.33496283268963667\n",
      "episode :  90\n",
      "current step :  35\n",
      "reward :  0.4036948324232955\n",
      "episode :  90\n",
      "current step :  40\n",
      "reward :  0.3935627150264818\n",
      "episode :  90\n",
      "current step :  45\n",
      "reward :  0.4876120931525444\n",
      "episode :  90\n",
      "current step :  50\n",
      "reward :  0.2777431557215861\n",
      "episode :  90\n",
      "current step :  55\n",
      "reward :  0.27671161524390014\n",
      "episode :  90\n",
      "current step :  60\n",
      "reward :  0.20255294956562617\n",
      "episode :  90\n",
      "current step :  65\n",
      "reward :  0.3453518054413612\n",
      "episode :  90\n",
      "current step :  70\n",
      "reward :  0.2912545260524839\n",
      "episode :  90\n",
      "current step :  75\n",
      "reward :  0.37120125590385467\n",
      "episode :  90\n",
      "current step :  80\n",
      "reward :  0.5175958730653988\n",
      "episode :  90\n",
      "current step :  85\n",
      "reward :  0.5203181742829008\n",
      "episode :  90\n",
      "current step :  90\n",
      "reward :  0.47425371724144183\n",
      "episode :  90\n",
      "current step :  95\n",
      "reward :  0.39534467298918546\n",
      "episode :  90\n",
      "current step :  100\n",
      "reward :  0.5677540993328887\n",
      "episode :  90\n",
      "current step :  105\n",
      "reward :  0.37129632754638936\n",
      "episode :  90\n",
      "current step :  110\n",
      "reward :  0.42528097092407957\n",
      "episode :  90\n",
      "current step :  115\n",
      "reward :  0.3675014274116276\n",
      "episode :  90\n",
      "current step :  120\n",
      "reward :  0.5406128789964112\n",
      "episode :  90\n",
      "current step :  125\n",
      "reward :  0.44125931165699406\n",
      "episode :  90\n",
      "current step :  130\n",
      "reward :  0.46946124776978043\n",
      "episode :  90\n",
      "current step :  135\n",
      "reward :  0.47419517110932374\n",
      "episode :  90\n",
      "current step :  140\n",
      "reward :  0.4973559472395726\n",
      "episode :  90\n",
      "current step :  145\n",
      "reward :  0.41522465011304976\n",
      "Episode 90, Total Reward: 12.501704660413422\n",
      "episode :  91\n",
      "current step :  0\n",
      "reward :  0.6011079905311222\n",
      "episode :  91\n",
      "current step :  5\n",
      "reward :  0.4320677934668774\n",
      "episode :  91\n",
      "current step :  10\n",
      "reward :  0.5163659968955845\n",
      "episode :  91\n",
      "current step :  15\n",
      "reward :  0.3959930122471118\n",
      "episode :  91\n",
      "current step :  20\n",
      "reward :  0.47436608703193195\n",
      "episode :  91\n",
      "current step :  25\n",
      "reward :  0.4529585773759177\n",
      "episode :  91\n",
      "current step :  30\n",
      "reward :  0.34420326513745586\n",
      "episode :  91\n",
      "current step :  35\n",
      "reward :  0.4176435624723704\n",
      "episode :  91\n",
      "current step :  40\n",
      "reward :  0.27565348900200287\n",
      "episode :  91\n",
      "current step :  45\n",
      "reward :  0.3315762401511419\n",
      "episode :  91\n",
      "current step :  50\n",
      "reward :  0.25094586507144634\n",
      "episode :  91\n",
      "current step :  55\n",
      "reward :  0.3387755643878808\n",
      "episode :  91\n",
      "current step :  60\n",
      "reward :  0.37553897544941617\n",
      "episode :  91\n",
      "current step :  65\n",
      "reward :  0.2616434509995391\n",
      "episode :  91\n",
      "current step :  70\n",
      "reward :  0.29739697608660715\n",
      "episode :  91\n",
      "current step :  75\n",
      "reward :  0.3344719653568788\n",
      "episode :  91\n",
      "current step :  80\n",
      "reward :  0.3008601257571466\n",
      "episode :  91\n",
      "current step :  85\n",
      "reward :  0.568283931785047\n",
      "episode :  91\n",
      "current step :  90\n",
      "reward :  0.44335232080335985\n",
      "episode :  91\n",
      "current step :  95\n",
      "reward :  0.20730542461280196\n",
      "episode :  91\n",
      "current step :  100\n",
      "reward :  0.5066374619574102\n",
      "episode :  91\n",
      "current step :  105\n",
      "reward :  0.35920730465196515\n",
      "episode :  91\n",
      "current step :  110\n",
      "reward :  0.44327316347490026\n",
      "episode :  91\n",
      "current step :  115\n",
      "reward :  0.2975992733391743\n",
      "episode :  91\n",
      "current step :  120\n",
      "reward :  0.4820261878807611\n",
      "episode :  91\n",
      "current step :  125\n",
      "reward :  0.3778170171679669\n",
      "episode :  91\n",
      "current step :  130\n",
      "reward :  0.5360811080437395\n",
      "episode :  91\n",
      "current step :  135\n",
      "reward :  0.4661149326924408\n",
      "episode :  91\n",
      "current step :  140\n",
      "reward :  0.4131991325347568\n",
      "episode :  91\n",
      "current step :  145\n",
      "reward :  0.48205271957130885\n",
      "Episode 91, Total Reward: 11.984518915936066\n",
      "episode :  92\n",
      "current step :  0\n",
      "reward :  0.49490178984089805\n",
      "episode :  92\n",
      "current step :  5\n",
      "reward :  0.4036632767935272\n",
      "episode :  92\n",
      "current step :  10\n",
      "reward :  0.46941373974384837\n",
      "episode :  92\n",
      "current step :  15\n",
      "reward :  0.4934398047873716\n",
      "episode :  92\n",
      "current step :  20\n",
      "reward :  0.4941708324246187\n",
      "episode :  92\n",
      "current step :  25\n",
      "reward :  0.5082706524319007\n",
      "episode :  92\n",
      "current step :  30\n",
      "reward :  0.5128104983263659\n",
      "episode :  92\n",
      "current step :  35\n",
      "reward :  0.4827142697622362\n",
      "episode :  92\n",
      "current step :  40\n",
      "reward :  0.2147320148420054\n",
      "episode :  92\n",
      "current step :  45\n",
      "reward :  0.466710997314873\n",
      "episode :  92\n",
      "current step :  50\n",
      "reward :  0.4808458096855631\n",
      "episode :  92\n",
      "current step :  55\n",
      "reward :  0.23403971422439612\n",
      "episode :  92\n",
      "current step :  60\n",
      "reward :  0.18491558650159934\n",
      "episode :  92\n",
      "current step :  65\n",
      "reward :  0.2561295641229159\n",
      "episode :  92\n",
      "current step :  70\n",
      "reward :  0.21523496625865474\n",
      "episode :  92\n",
      "current step :  75\n",
      "reward :  0.44776235752474297\n",
      "episode :  92\n",
      "current step :  80\n",
      "reward :  0.6198296641109082\n",
      "episode :  92\n",
      "current step :  85\n",
      "reward :  0.4513601129873801\n",
      "episode :  92\n",
      "current step :  90\n",
      "reward :  0.40825109890843747\n",
      "episode :  92\n",
      "current step :  95\n",
      "reward :  0.47409037961280365\n",
      "episode :  92\n",
      "current step :  100\n",
      "reward :  0.267755545503438\n",
      "episode :  92\n",
      "current step :  105\n",
      "reward :  0.3726444515159511\n",
      "episode :  92\n",
      "current step :  110\n",
      "reward :  0.33836259877246444\n",
      "episode :  92\n",
      "current step :  115\n",
      "reward :  0.3687119036110518\n",
      "episode :  92\n",
      "current step :  120\n",
      "reward :  0.4274583514273704\n",
      "episode :  92\n",
      "current step :  125\n",
      "reward :  0.4677303050215974\n",
      "episode :  92\n",
      "current step :  130\n",
      "reward :  0.4401845106987655\n",
      "episode :  92\n",
      "current step :  135\n",
      "reward :  0.41518500970798555\n",
      "episode :  92\n",
      "current step :  140\n",
      "reward :  0.48101514201437934\n",
      "episode :  92\n",
      "current step :  145\n",
      "reward :  0.4620029672479377\n",
      "Episode 92, Total Reward: 12.354337915725987\n",
      "episode :  93\n",
      "current step :  0\n",
      "reward :  0.5291467376131737\n",
      "episode :  93\n",
      "current step :  5\n",
      "reward :  0.36114093414783754\n",
      "episode :  93\n",
      "current step :  10\n",
      "reward :  0.45810448924122327\n",
      "episode :  93\n",
      "current step :  15\n",
      "reward :  0.5235470480335929\n",
      "episode :  93\n",
      "current step :  20\n",
      "reward :  0.39575744321780904\n",
      "episode :  93\n",
      "current step :  25\n",
      "reward :  0.3433177103796709\n",
      "episode :  93\n",
      "current step :  30\n",
      "reward :  0.18627617582416836\n",
      "episode :  93\n",
      "current step :  35\n",
      "reward :  0.5306368082520729\n",
      "episode :  93\n",
      "current step :  40\n",
      "reward :  0.34479371969600986\n",
      "episode :  93\n",
      "current step :  45\n",
      "reward :  0.4457880659754395\n",
      "episode :  93\n",
      "current step :  50\n",
      "reward :  0.2584798013267513\n",
      "episode :  93\n",
      "current step :  55\n",
      "reward :  0.3820357253657718\n",
      "episode :  93\n",
      "current step :  60\n",
      "reward :  0.167872436640592\n",
      "episode :  93\n",
      "current step :  65\n",
      "reward :  0.20141109250843284\n",
      "episode :  93\n",
      "current step :  70\n",
      "reward :  0.22136338128199545\n",
      "episode :  93\n",
      "current step :  75\n",
      "reward :  0.32524444174946765\n",
      "episode :  93\n",
      "current step :  80\n",
      "reward :  0.5197254131448427\n",
      "episode :  93\n",
      "current step :  85\n",
      "reward :  0.4482909608183635\n",
      "episode :  93\n",
      "current step :  90\n",
      "reward :  0.4175130433880026\n",
      "episode :  93\n",
      "current step :  95\n",
      "reward :  0.42772768082064727\n",
      "episode :  93\n",
      "current step :  100\n",
      "reward :  0.4362640843793256\n",
      "episode :  93\n",
      "current step :  105\n",
      "reward :  0.3875973338872474\n",
      "episode :  93\n",
      "current step :  110\n",
      "reward :  0.3796631800928428\n",
      "episode :  93\n",
      "current step :  115\n",
      "reward :  0.3430209586014976\n",
      "episode :  93\n",
      "current step :  120\n",
      "reward :  0.3292556155833154\n",
      "episode :  93\n",
      "current step :  125\n",
      "reward :  0.38452388060088194\n",
      "episode :  93\n",
      "current step :  130\n",
      "reward :  0.4500279588401101\n",
      "episode :  93\n",
      "current step :  135\n",
      "reward :  0.5010358993179234\n",
      "episode :  93\n",
      "current step :  140\n",
      "reward :  0.453078344321421\n",
      "episode :  93\n",
      "current step :  145\n",
      "reward :  0.3727737628285281\n",
      "Episode 93, Total Reward: 11.52541412787896\n",
      "episode :  94\n",
      "current step :  0\n",
      "reward :  0.5433056658529475\n",
      "episode :  94\n",
      "current step :  5\n",
      "reward :  0.45576821884326624\n",
      "episode :  94\n",
      "current step :  10\n",
      "reward :  0.4714644782500705\n",
      "episode :  94\n",
      "current step :  15\n",
      "reward :  0.4906021162573889\n",
      "episode :  94\n",
      "current step :  20\n",
      "reward :  0.48731420858519897\n",
      "episode :  94\n",
      "current step :  25\n",
      "reward :  0.40184552089762204\n",
      "episode :  94\n",
      "current step :  30\n",
      "reward :  0.49154379450969166\n",
      "episode :  94\n",
      "current step :  35\n",
      "reward :  0.3260163130470576\n",
      "episode :  94\n",
      "current step :  40\n",
      "reward :  0.24145208230617785\n",
      "episode :  94\n",
      "current step :  45\n",
      "reward :  0.393973869585333\n",
      "episode :  94\n",
      "current step :  50\n",
      "reward :  0.522110949392687\n",
      "episode :  94\n",
      "current step :  55\n",
      "reward :  0.22548759117341213\n",
      "episode :  94\n",
      "current step :  60\n",
      "reward :  0.2754435686725846\n",
      "episode :  94\n",
      "current step :  65\n",
      "reward :  0.22616581664811533\n",
      "episode :  94\n",
      "current step :  70\n",
      "reward :  0.2965289210307616\n",
      "episode :  94\n",
      "current step :  75\n",
      "reward :  0.306426056891995\n",
      "episode :  94\n",
      "current step :  80\n",
      "reward :  0.5146802947167497\n",
      "episode :  94\n",
      "current step :  85\n",
      "reward :  0.46997726290513925\n",
      "episode :  94\n",
      "current step :  90\n",
      "reward :  0.49579638403948917\n",
      "episode :  94\n",
      "current step :  95\n",
      "reward :  0.3487713387871371\n",
      "episode :  94\n",
      "current step :  100\n",
      "reward :  0.5284468431851073\n",
      "episode :  94\n",
      "current step :  105\n",
      "reward :  0.423586947789071\n",
      "episode :  94\n",
      "current step :  110\n",
      "reward :  0.4402356363386808\n",
      "episode :  94\n",
      "current step :  115\n",
      "reward :  0.4742419716292562\n",
      "episode :  94\n",
      "current step :  120\n",
      "reward :  0.36925752368343917\n",
      "episode :  94\n",
      "current step :  125\n",
      "reward :  0.4078205547171605\n",
      "episode :  94\n",
      "current step :  130\n",
      "reward :  0.5223114756627507\n",
      "episode :  94\n",
      "current step :  135\n",
      "reward :  0.442758550787001\n",
      "episode :  94\n",
      "current step :  140\n",
      "reward :  0.5031048967266842\n",
      "episode :  94\n",
      "current step :  145\n",
      "reward :  0.5035709914939871\n",
      "Episode 94, Total Reward: 12.600009844405962\n",
      "episode :  95\n",
      "current step :  0\n",
      "reward :  0.5084261091173763\n",
      "episode :  95\n",
      "current step :  5\n",
      "reward :  0.40944823182028645\n",
      "episode :  95\n",
      "current step :  10\n",
      "reward :  0.3964815494791979\n",
      "episode :  95\n",
      "current step :  15\n",
      "reward :  0.4923509508236571\n",
      "episode :  95\n",
      "current step :  20\n",
      "reward :  0.604971416458189\n",
      "episode :  95\n",
      "current step :  25\n",
      "reward :  0.4733529187571579\n",
      "episode :  95\n",
      "current step :  30\n",
      "reward :  0.45249638813696\n",
      "episode :  95\n",
      "current step :  35\n",
      "reward :  0.4541483932814713\n",
      "episode :  95\n",
      "current step :  40\n",
      "reward :  0.2856668384571487\n",
      "episode :  95\n",
      "current step :  45\n",
      "reward :  0.428572010633412\n",
      "episode :  95\n",
      "current step :  50\n",
      "reward :  0.4577615203790423\n",
      "episode :  95\n",
      "current step :  55\n",
      "reward :  0.24036411345809805\n",
      "episode :  95\n",
      "current step :  60\n",
      "reward :  0.18685859209005334\n",
      "episode :  95\n",
      "current step :  65\n",
      "reward :  0.19728922377707536\n",
      "episode :  95\n",
      "current step :  70\n",
      "reward :  0.521029647553116\n",
      "episode :  95\n",
      "current step :  75\n",
      "reward :  0.3818600342318862\n",
      "episode :  95\n",
      "current step :  80\n",
      "reward :  0.4352746998381647\n",
      "episode :  95\n",
      "current step :  85\n",
      "reward :  0.48972150145107335\n",
      "episode :  95\n",
      "current step :  90\n",
      "reward :  0.20094083116108674\n",
      "episode :  95\n",
      "current step :  95\n",
      "reward :  0.4405217324378458\n",
      "episode :  95\n",
      "current step :  100\n",
      "reward :  0.520219383537699\n",
      "episode :  95\n",
      "current step :  105\n",
      "reward :  0.47773361972195844\n",
      "episode :  95\n",
      "current step :  110\n",
      "reward :  0.39654597222936905\n",
      "episode :  95\n",
      "current step :  115\n",
      "reward :  0.1599323507041391\n",
      "episode :  95\n",
      "current step :  120\n",
      "reward :  0.49066815601112607\n",
      "episode :  95\n",
      "current step :  125\n",
      "reward :  0.4546382654672886\n",
      "episode :  95\n",
      "current step :  130\n",
      "reward :  0.4491104268210816\n",
      "episode :  95\n",
      "current step :  135\n",
      "reward :  0.5627486396968269\n",
      "episode :  95\n",
      "current step :  140\n",
      "reward :  0.4087873342350483\n",
      "episode :  95\n",
      "current step :  145\n",
      "reward :  0.23423096768643203\n",
      "Episode 95, Total Reward: 12.212151819453265\n",
      "episode :  96\n",
      "current step :  0\n",
      "reward :  0.48098960116151745\n",
      "episode :  96\n",
      "current step :  5\n",
      "reward :  0.40862957354630125\n",
      "episode :  96\n",
      "current step :  10\n",
      "reward :  0.4563055682764627\n",
      "episode :  96\n",
      "current step :  15\n",
      "reward :  0.4504927780315291\n",
      "episode :  96\n",
      "current step :  20\n",
      "reward :  0.4725767250382685\n",
      "episode :  96\n",
      "current step :  25\n",
      "reward :  0.509133370493434\n",
      "episode :  96\n",
      "current step :  30\n",
      "reward :  0.2633111628076788\n",
      "episode :  96\n",
      "current step :  35\n",
      "reward :  0.4425939808113144\n",
      "episode :  96\n",
      "current step :  40\n",
      "reward :  0.453013922745839\n",
      "episode :  96\n",
      "current step :  45\n",
      "reward :  0.4363031697145645\n",
      "episode :  96\n",
      "current step :  50\n",
      "reward :  0.28065820410219083\n",
      "episode :  96\n",
      "current step :  55\n",
      "reward :  0.29355029141030176\n",
      "episode :  96\n",
      "current step :  60\n",
      "reward :  0.22611884322623602\n",
      "episode :  96\n",
      "current step :  65\n",
      "reward :  0.16369025952667543\n",
      "episode :  96\n",
      "current step :  70\n",
      "reward :  0.2120300202991162\n",
      "episode :  96\n",
      "current step :  75\n",
      "reward :  0.4104690254227936\n",
      "episode :  96\n",
      "current step :  80\n",
      "reward :  0.3560988595148063\n",
      "episode :  96\n",
      "current step :  85\n",
      "reward :  0.5560401938145207\n",
      "episode :  96\n",
      "current step :  90\n",
      "reward :  0.44883723451896596\n",
      "episode :  96\n",
      "current step :  95\n",
      "reward :  0.41784681460781203\n",
      "episode :  96\n",
      "current step :  100\n",
      "reward :  0.46813329800935455\n",
      "episode :  96\n",
      "current step :  105\n",
      "reward :  0.23765799687227626\n",
      "episode :  96\n",
      "current step :  110\n",
      "reward :  0.45457035784434524\n",
      "episode :  96\n",
      "current step :  115\n",
      "reward :  0.39783903313194247\n",
      "episode :  96\n",
      "current step :  120\n",
      "reward :  0.31141583529334527\n",
      "episode :  96\n",
      "current step :  125\n",
      "reward :  0.4459463904823612\n",
      "episode :  96\n",
      "current step :  130\n",
      "reward :  0.47622405726991335\n",
      "episode :  96\n",
      "current step :  135\n",
      "reward :  0.16271012622638936\n",
      "episode :  96\n",
      "current step :  140\n",
      "reward :  0.507271963683816\n",
      "episode :  96\n",
      "current step :  145\n",
      "reward :  0.3100395232115639\n",
      "Episode 96, Total Reward: 11.510498181095638\n",
      "episode :  97\n",
      "current step :  0\n",
      "reward :  0.5012302909257275\n",
      "episode :  97\n",
      "current step :  5\n",
      "reward :  0.4250488417445777\n",
      "episode :  97\n",
      "current step :  10\n",
      "reward :  0.5048459382966788\n",
      "episode :  97\n",
      "current step :  15\n",
      "reward :  0.4614171647456546\n",
      "episode :  97\n",
      "current step :  20\n",
      "reward :  0.5364323937818556\n",
      "episode :  97\n",
      "current step :  25\n",
      "reward :  0.5307578685627768\n",
      "episode :  97\n",
      "current step :  30\n",
      "reward :  0.31746062969677546\n",
      "episode :  97\n",
      "current step :  35\n",
      "reward :  0.2943862088443349\n",
      "episode :  97\n",
      "current step :  40\n",
      "reward :  0.33595119278062907\n",
      "episode :  97\n",
      "current step :  45\n",
      "reward :  0.4076814380352734\n",
      "episode :  97\n",
      "current step :  50\n",
      "reward :  0.18494597133625995\n",
      "episode :  97\n",
      "current step :  55\n",
      "reward :  0.46239374469340977\n",
      "episode :  97\n",
      "current step :  60\n",
      "reward :  0.22401523379372498\n",
      "episode :  97\n",
      "current step :  65\n",
      "reward :  0.48202642710129406\n",
      "episode :  97\n",
      "current step :  70\n",
      "reward :  0.24156863700955894\n",
      "episode :  97\n",
      "current step :  75\n",
      "reward :  0.5131819093473191\n",
      "episode :  97\n",
      "current step :  80\n",
      "reward :  0.4428945044941086\n",
      "episode :  97\n",
      "current step :  85\n",
      "reward :  0.4969520944501621\n",
      "episode :  97\n",
      "current step :  90\n",
      "reward :  0.3322936066684038\n",
      "episode :  97\n",
      "current step :  95\n",
      "reward :  0.3945836402809283\n",
      "episode :  97\n",
      "current step :  100\n",
      "reward :  0.39540519836974053\n",
      "episode :  97\n",
      "current step :  105\n",
      "reward :  0.5313649459955169\n",
      "episode :  97\n",
      "current step :  110\n",
      "reward :  0.526527034569089\n",
      "episode :  97\n",
      "current step :  115\n",
      "reward :  0.32761262022281246\n",
      "episode :  97\n",
      "current step :  120\n",
      "reward :  0.21089002331210993\n",
      "episode :  97\n",
      "current step :  125\n",
      "reward :  0.5162953228317759\n",
      "episode :  97\n",
      "current step :  130\n",
      "reward :  0.513246840072449\n",
      "episode :  97\n",
      "current step :  135\n",
      "reward :  0.40675441684257035\n",
      "episode :  97\n",
      "current step :  140\n",
      "reward :  0.4330894998499336\n",
      "episode :  97\n",
      "current step :  145\n",
      "reward :  0.4544010958770045\n",
      "Episode 97, Total Reward: 12.405654734532453\n",
      "episode :  98\n",
      "current step :  0\n",
      "reward :  0.4126670678526279\n",
      "episode :  98\n",
      "current step :  5\n",
      "reward :  0.4225224783281527\n",
      "episode :  98\n",
      "current step :  10\n",
      "reward :  0.4287535675659196\n",
      "episode :  98\n",
      "current step :  15\n",
      "reward :  0.45790878939621404\n",
      "episode :  98\n",
      "current step :  20\n",
      "reward :  0.42618098046695185\n",
      "episode :  98\n",
      "current step :  25\n",
      "reward :  0.4482335180286312\n",
      "episode :  98\n",
      "current step :  30\n",
      "reward :  0.46074391632250733\n",
      "episode :  98\n",
      "current step :  35\n",
      "reward :  0.4928449519528604\n",
      "episode :  98\n",
      "current step :  40\n",
      "reward :  0.3647264501367387\n",
      "episode :  98\n",
      "current step :  45\n",
      "reward :  0.447110620525185\n",
      "episode :  98\n",
      "current step :  50\n",
      "reward :  0.2460003240415179\n",
      "episode :  98\n",
      "current step :  55\n",
      "reward :  0.34076242599987105\n",
      "episode :  98\n",
      "current step :  60\n",
      "reward :  0.22575697527683797\n",
      "episode :  98\n",
      "current step :  65\n",
      "reward :  0.394015188772474\n",
      "episode :  98\n",
      "current step :  70\n",
      "reward :  0.3769554491319899\n",
      "episode :  98\n",
      "current step :  75\n",
      "reward :  0.5464050893292698\n",
      "episode :  98\n",
      "current step :  80\n",
      "reward :  0.4643997276712898\n",
      "episode :  98\n",
      "current step :  85\n",
      "reward :  0.3054668192109001\n",
      "episode :  98\n",
      "current step :  90\n",
      "reward :  0.3692405196065699\n",
      "episode :  98\n",
      "current step :  95\n",
      "reward :  0.3731061151901665\n",
      "episode :  98\n",
      "current step :  100\n",
      "reward :  0.5699517378048716\n",
      "episode :  98\n",
      "current step :  105\n",
      "reward :  0.4351946730870263\n",
      "episode :  98\n",
      "current step :  110\n",
      "reward :  0.37613112766853235\n",
      "episode :  98\n",
      "current step :  115\n",
      "reward :  0.440841680498813\n",
      "episode :  98\n",
      "current step :  120\n",
      "reward :  0.41526770194725915\n",
      "episode :  98\n",
      "current step :  125\n",
      "reward :  0.26446854328283714\n",
      "episode :  98\n",
      "current step :  130\n",
      "reward :  0.4566635502641323\n",
      "episode :  98\n",
      "current step :  135\n",
      "reward :  0.40400175966412244\n",
      "episode :  98\n",
      "current step :  140\n",
      "reward :  0.4162392947500023\n",
      "episode :  98\n",
      "current step :  145\n",
      "reward :  0.38409325827012997\n",
      "Episode 98, Total Reward: 12.166654302044401\n",
      "episode :  99\n",
      "current step :  0\n",
      "reward :  0.35490713636639387\n",
      "episode :  99\n",
      "current step :  5\n",
      "reward :  0.4597683641459116\n",
      "episode :  99\n",
      "current step :  10\n",
      "reward :  0.5311860277176186\n",
      "episode :  99\n",
      "current step :  15\n",
      "reward :  0.5613325913069648\n",
      "episode :  99\n",
      "current step :  20\n",
      "reward :  0.220973087631223\n",
      "episode :  99\n",
      "current step :  25\n",
      "reward :  0.4305582481925061\n",
      "episode :  99\n",
      "current step :  30\n",
      "reward :  0.44191598315595837\n",
      "episode :  99\n",
      "current step :  35\n",
      "reward :  0.549124151862765\n",
      "episode :  99\n",
      "current step :  40\n",
      "reward :  0.25589899424926354\n",
      "episode :  99\n",
      "current step :  45\n",
      "reward :  0.4701388785152603\n",
      "episode :  99\n",
      "current step :  50\n",
      "reward :  0.4668871225732517\n",
      "episode :  99\n",
      "current step :  55\n",
      "reward :  0.3148844780405752\n",
      "episode :  99\n",
      "current step :  60\n",
      "reward :  0.458507427374812\n",
      "episode :  99\n",
      "current step :  65\n",
      "reward :  0.3529883771388009\n",
      "episode :  99\n",
      "current step :  70\n",
      "reward :  0.557486652400053\n",
      "episode :  99\n",
      "current step :  75\n",
      "reward :  0.5058502868869668\n",
      "episode :  99\n",
      "current step :  80\n",
      "reward :  0.4486908268644024\n",
      "episode :  99\n",
      "current step :  85\n",
      "reward :  0.5026417876202619\n",
      "episode :  99\n",
      "current step :  90\n",
      "reward :  0.41884784908748096\n",
      "episode :  99\n",
      "current step :  95\n",
      "reward :  0.5494831216794893\n",
      "episode :  99\n",
      "current step :  100\n",
      "reward :  0.4830366720313033\n",
      "episode :  99\n",
      "current step :  105\n",
      "reward :  0.3463988774701944\n",
      "episode :  99\n",
      "current step :  110\n",
      "reward :  0.4100739799117542\n",
      "episode :  99\n",
      "current step :  115\n",
      "reward :  0.2744429716332126\n",
      "episode :  99\n",
      "current step :  120\n",
      "reward :  0.18214174139926015\n",
      "episode :  99\n",
      "current step :  125\n",
      "reward :  0.49548006726884797\n",
      "episode :  99\n",
      "current step :  130\n",
      "reward :  0.4108527563056641\n",
      "episode :  99\n",
      "current step :  135\n",
      "reward :  0.4095003452280875\n",
      "episode :  99\n",
      "current step :  140\n",
      "reward :  0.5056985954373592\n",
      "episode :  99\n",
      "current step :  145\n",
      "reward :  0.2560546906494982\n",
      "Episode 99, Total Reward: 12.62575209014514\n",
      "episode :  100\n",
      "current step :  0\n",
      "reward :  0.49444514675311124\n",
      "episode :  100\n",
      "current step :  5\n",
      "reward :  0.4355681749126531\n",
      "episode :  100\n",
      "current step :  10\n",
      "reward :  0.3803951810387689\n",
      "episode :  100\n",
      "current step :  15\n",
      "reward :  0.45756470979019526\n",
      "episode :  100\n",
      "current step :  20\n",
      "reward :  0.4486700122973386\n",
      "episode :  100\n",
      "current step :  25\n",
      "reward :  0.42675310343708617\n",
      "episode :  100\n",
      "current step :  30\n",
      "reward :  0.3278430089035366\n",
      "episode :  100\n",
      "current step :  35\n",
      "reward :  0.39835812067554355\n",
      "episode :  100\n",
      "current step :  40\n",
      "reward :  0.31813408671261895\n",
      "episode :  100\n",
      "current step :  45\n",
      "reward :  0.48978839671364616\n",
      "episode :  100\n",
      "current step :  50\n",
      "reward :  0.5484966815350772\n",
      "episode :  100\n",
      "current step :  55\n",
      "reward :  0.28647582782286773\n",
      "episode :  100\n",
      "current step :  60\n",
      "reward :  0.45475662962910707\n",
      "episode :  100\n",
      "current step :  65\n",
      "reward :  0.2173444316540123\n",
      "episode :  100\n",
      "current step :  70\n",
      "reward :  0.47764792716775023\n",
      "episode :  100\n",
      "current step :  75\n",
      "reward :  0.42512278964985273\n",
      "episode :  100\n",
      "current step :  80\n",
      "reward :  0.39892547680094337\n",
      "episode :  100\n",
      "current step :  85\n",
      "reward :  0.3663430423926307\n",
      "episode :  100\n",
      "current step :  90\n",
      "reward :  0.3916950265723529\n",
      "episode :  100\n",
      "current step :  95\n",
      "reward :  0.4348134384608838\n",
      "episode :  100\n",
      "current step :  100\n",
      "reward :  0.5901396682720208\n",
      "episode :  100\n",
      "current step :  105\n",
      "reward :  0.4733576054651877\n",
      "episode :  100\n",
      "current step :  110\n",
      "reward :  0.4124066193177137\n",
      "episode :  100\n",
      "current step :  115\n",
      "reward :  0.4687820452777781\n",
      "episode :  100\n",
      "current step :  120\n",
      "reward :  0.3416994438293456\n",
      "episode :  100\n",
      "current step :  125\n",
      "reward :  0.4626287434546573\n",
      "episode :  100\n",
      "current step :  130\n",
      "reward :  0.4571790607487099\n",
      "episode :  100\n",
      "current step :  135\n",
      "reward :  0.46344093943390147\n",
      "episode :  100\n",
      "current step :  140\n",
      "reward :  0.42480554610869203\n",
      "episode :  100\n",
      "current step :  145\n",
      "reward :  0.49962888586245224\n",
      "Episode 100, Total Reward: 12.773209770690436\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Routine d'entraînement\n",
    "num_episodes = 100\n",
    "# for episode in range(num_episodes):\n",
    "#     state = env.reset()\n",
    "#     done = False\n",
    "#     total_reward = 0\n",
    "#     while not done:\n",
    "#         action = agent.act(state)\n",
    "#         next_state, reward, done, _ = env.step(action)\n",
    "#         agent.step(state, action, reward, next_state, done)\n",
    "#         state = next_state\n",
    "#         total_reward += reward\n",
    "#     print(f'Episode {episode + 1}, Total Reward: {total_reward}')\n",
    "\n",
    "def add_noise(action, std_dev=0.25):\n",
    "    noise = np.random.normal(0, std_dev, size=action.shape)\n",
    "    return action + noise\n",
    "    \n",
    "rew = []\n",
    "for episode in range(num_episodes):\n",
    "    state, info = env.reset()  # Assurez-vous que reset retourne également 'info'\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        assert state.shape == (6,), f\"Invalid state shape: {state.shape}\"\n",
    "        action = agent.act(state)\n",
    "        action = add_noise(action) \n",
    "        next_state, reward, done, truncated, _ = env.step(action)\n",
    "        agent.step(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "    rew.append(total_reward)\n",
    "    print(f'Episode {episode + 1}, Total Reward: {total_reward}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3578452d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tracer l'évolution de la récompense moyenne\n",
    "plt.plot(rew)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Mean Reward')\n",
    "plt.title('Evolution of Mean Reward')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90227845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Après l'entraînement\n",
    "torch.save(agent.actor.state_dict(), 'actor_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e6a3ec3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Actor(\n",
       "  (network): Sequential(\n",
       "    (0): Linear(in_features=6, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=256, out_features=13, bias=True)\n",
       "    (5): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Avant la démonstration\n",
    "agent.actor.load_state_dict(torch.load('actor_model_ed.pth'))\n",
    "agent.actor.eval()  # Passer le modèle en mode évaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c5638d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading targets from :  anaele_bent_arms_0_poppy_skeletons.pt\n",
      "[ 0.12243557 -0.17011382  0.04156335 -0.13337028 -0.19293001  0.0522413 ]\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.get_target()\n",
    "print(np.r_[env.targets[0][13], env.targets[0][16]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d9a745b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "episode :  101\n",
      "current step :  0\n",
      "reward :  0.4594191100097319\n",
      "2\n",
      "episode :  101\n",
      "current step :  5\n",
      "reward :  0.4516971030854802\n",
      "3\n",
      "episode :  101\n",
      "current step :  10\n",
      "reward :  0.45902974132642704\n",
      "4\n",
      "episode :  101\n",
      "current step :  15\n",
      "reward :  0.475193786848172\n",
      "5\n",
      "episode :  101\n",
      "current step :  20\n",
      "reward :  0.46916108110952703\n",
      "6\n",
      "episode :  101\n",
      "current step :  25\n",
      "reward :  0.44488341703932455\n",
      "7\n",
      "episode :  101\n",
      "current step :  30\n",
      "reward :  0.47691179172869774\n",
      "8\n",
      "episode :  101\n",
      "current step :  35\n",
      "reward :  0.3103341183966122\n",
      "9\n",
      "episode :  101\n",
      "current step :  40\n",
      "reward :  0.3530342034474124\n",
      "10\n",
      "episode :  101\n",
      "current step :  45\n",
      "reward :  0.4608865469370711\n",
      "11\n",
      "episode :  101\n",
      "current step :  50\n",
      "reward :  0.427676444292153\n",
      "12\n",
      "episode :  101\n",
      "current step :  55\n",
      "reward :  0.2582629916786323\n",
      "13\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(i)\n\u001b[0;32m      7\u001b[0m action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mact(state)\n\u001b[1;32m----> 8\u001b[0m next_state, reward, done, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[0;32m     10\u001b[0m total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "File \u001b[1;32mc:\\Users\\edoua\\_Travail\\IA705\\Projet2023M\\Poppy\\Poppy_Env_edouard.py:325\u001b[0m, in \u001b[0;36mPoppyEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''entrainement d'un step avec calcul error (distance) et reward\u001b[39;00m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;124;03mInput : action dictionary in the format {\"joint_name\": angle}\u001b[39;00m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;124;03mOutput :\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;124;03m    info : dictionary with episode, step and reward values\u001b[39;00m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;66;03m# move the robot and get observations\u001b[39;00m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;66;03m# we will use only end effectors position for now,\u001b[39;00m\n\u001b[0;32m    324\u001b[0m \u001b[38;5;66;03m# obs, _, _ = self.poppy_goto(action)\u001b[39;00m\n\u001b[1;32m--> 325\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoppy_goto\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    326\u001b[0m obs, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_observation()\n\u001b[0;32m    328\u001b[0m \u001b[38;5;66;03m# calculate the reward (based on similarity of end effectors positions with targets)\u001b[39;00m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;66;03m# Note: reward function could penalise moovements that Poppy cannot do using l_joints_obs and r_joints_obs....\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\edoua\\_Travail\\IA705\\Projet2023M\\Poppy\\Poppy_Env_edouard.py:172\u001b[0m, in \u001b[0;36mPoppyEnv.poppy_goto\u001b[1;34m(self, joints_to_move, wait_for)\u001b[0m\n\u001b[0;32m    169\u001b[0m         m\u001b[38;5;241m.\u001b[39mgoto_position(action, \u001b[38;5;241m3\u001b[39m, wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# wait for moovements to be executed\u001b[39;00m\n\u001b[1;32m--> 172\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait_for\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total_reward = 0\n",
    "i=0\n",
    "done = False\n",
    "while not done:\n",
    "    i += 1\n",
    "    print(i)\n",
    "    action = agent.act(state)\n",
    "    next_state, reward, done, truncated, info = env.step(action)\n",
    "    state = next_state\n",
    "    total_reward += reward\n",
    "print(\"Total Reward during demonstration:\", total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7ed6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from utils.skeleton import *\n",
    "from utils.quaternion import *\n",
    "from utils.blazepose import blazepose_skeletons\n",
    "\n",
    "from pypot.creatures import PoppyTorso\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "import gymnasium as gym\n",
    "\n",
    "from Poppy_Env_edouard import PoppyEnv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec19ef6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_env.close()\n",
    "from pypot import vrep\n",
    "vrep.close_all_connections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa52cc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create the environment\n",
    "# env = make_vec_env(lambda: PoppyEnv(), n_envs=1)\n",
    "\n",
    "# # Initialize the agent\n",
    "# model = SAC(\"MlpPolicy\", env, verbose=1, tensorboard_log=\"./sac_poppy_tensorboard/\")\n",
    "\n",
    "# # Train the agent\n",
    "# total_timesteps = 3  # Set this to a higher number for better results\n",
    "# model.learn(total_timesteps=total_timesteps)\n",
    "\n",
    "# # Save the model\n",
    "# model.save(\"sac_poppy_model\")\n",
    "\n",
    "# # Optionally evaluate the policy\n",
    "# mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)\n",
    "# print(f\"Mean reward: {mean_reward} +/- {std_reward}\")\n",
    "\n",
    "# # Close the environment\n",
    "# env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243295ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
